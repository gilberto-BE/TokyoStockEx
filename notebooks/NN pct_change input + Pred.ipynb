{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tokyo\\lib\\site-packages\\torch\\__init__.py:721\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautocast_mode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast\n\u001b[0;32m    720\u001b[0m \u001b[38;5;66;03m# Shared memory manager needs to know the exact location of manager executable\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initExtension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanager_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m manager_path\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# Appease the type checker: it can't deal with direct setting of globals().\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# Note that we will see \"too many\" functions when reexporting this way; there\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# so that this import is good enough\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tokyo\\lib\\site-packages\\torch\\cuda\\__init__.py:759\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m profiler\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nvtx\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tokyo\\lib\\site-packages\\torch\\cuda\\amp\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautocast_mode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast, custom_fwd, custom_bwd  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrad_scaler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tokyo\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     HAS_NUMPY \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tokyo\\lib\\site-packages\\numpy\\__init__.py:155\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fft\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polynomial\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ctypeslib\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ma\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tokyo\\lib\\site-packages\\numpy\\random\\__init__.py:180\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    126\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinomial\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzipf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    177\u001b[0m ]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pickle\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _common\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _bounded_integers\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tokyo\\lib\\site-packages\\numpy\\random\\_pickle.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmtrand\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomState\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_philox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Philox\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pcg64\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCG64, PCG64DXSM\n",
      "File \u001b[1;32mmtrand.pyx:1\u001b[0m, in \u001b[0;36minit numpy.random.mtrand\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mbit_generator.pyx:40\u001b[0m, in \u001b[0;36minit numpy.random.bit_generator\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:969\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1091\u001b[0m, in \u001b[0;36mpath_stats\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics as TM\n",
    "# pl.utilities.seed.seed_everything(seed=42)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(source_path)\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'preprocessing')\n",
    "sys.path.append(source_path)\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'metrics')\n",
    "sys.path.append(source_path)\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'predict')\n",
    "sys.path.append(source_path)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dl import NeuralNetwork, Trainer\n",
    "from preprocess import (\n",
    "    show_df, \n",
    "    date_features, \n",
    "    preprocess, \n",
    "    ToTorch, \n",
    "    get_loader, \n",
    "    ts_split,\n",
    "    cont_cat_split,\n",
    "    dataloader_by_stock,\n",
    "    get_data,\n",
    "    dataloader_test_by_stock\n",
    ")\n",
    "# from metrics import calc_spread_return_sharpe\n",
    "print(torch.__version__)\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dl import plot_loss\n",
    "\n",
    "\n",
    "from predict import run_pred_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda), torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get Data and train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the unique security codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_data()\n",
    "# print('Unique adjustment factor:', train_df['AdjustmentFactor'].unique())\n",
    "print(train_df['AdjustmentFactor'])\n",
    "display(train_df.head())\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args and constants\n",
    "* Adding financials wes cumbersome maybe not worth now to add that data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "SUBTRACT:\n",
    "*) 3 FROM CONTINUOUS COLS BECAUSE OF POOLING\n",
    "*) 1 FROM CAT_FEATURES TO MAKE MATRICES MATCH AFTER TORCH.CAT\n",
    "\"\"\"\n",
    "CONT_COLS=['Close', 'Open', 'High', 'Low', 'MarketCapitalization',         \n",
    "           # 'NetSales', 'EquityToAssetRatio', 'TotalAssets', 'Profit', \n",
    "           # 'OperatingProfit', 'EarningsPerShare', 'Equity', \n",
    "           # 'BookValuePerShare', 'ResultDividendPerShare1stQuarter', \n",
    "           # 'ResultDividendPerShare2ndQuarter', 'ResultDividendPerShare3rdQuarter',\n",
    "           # 'ResultDividendPerShareFiscalYearEnd', 'ResultDividendPerShareAnnual'\n",
    "          ]\n",
    "TS_IN_FEATURES = len(CONT_COLS)\n",
    "print(TS_IN_FEATURES)\n",
    "CAT_FEATURES = 4 + 4 - 1 # TEXT_COLS = ['Section/Products', '33SectorName', '17SectorName', 'Universe0']\n",
    "print('CAT_FEATURES:', CAT_FEATURES)\n",
    "EMBEDDING_DIM = 300\n",
    "NO_EMBEDDING = 2000 #2 * len(df_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    model = None\n",
    "    model = NeuralNetwork(\n",
    "        in_features=TS_IN_FEATURES - 3, \n",
    "        units=128,\n",
    "        out_features=1, \n",
    "        categorical_dim=CAT_FEATURES,\n",
    "        no_embedding=NO_EMBEDDING, \n",
    "        emb_dim=EMBEDDING_DIM,\n",
    "        n_blocks=4,\n",
    "        n_stacks=1,\n",
    "        dropout=0.3,\n",
    "        pooling_sizes=3\n",
    "    )\n",
    "\n",
    "    print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop throug each stock\n",
    "Create Trainer only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = 'example_test_files'\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "df = get_data(folder=FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stocks = train_df['SecuritiesCode'].unique()\n",
    "count = 0\n",
    "BATCH_SIZE = 512\n",
    "weight_decay = 0.1\n",
    "EPOCHS = 5\n",
    "\n",
    "scaler_dict = {}\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer_name='rmsprop', \n",
    "    lr=1.3333e-5, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_mae_list = []\n",
    "valid_loss_list = []\n",
    "valid_mae_list = []\n",
    "\n",
    "for no_stock, stock in enumerate(stocks[: 5]):\n",
    "    # try:\n",
    "    train_loader, val_dataloader = None, None\n",
    "\n",
    "    print(f'Stock-iteratation: {no_stock}')\n",
    "    print(f'Start training for stock: {stock}')\n",
    "\n",
    "    train_dataloader, val_dataloader = dataloader_by_stock(\n",
    "        train_df, \n",
    "        stock, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        continous_cols=CONT_COLS,\n",
    "        return_scaler=False\n",
    "    )\n",
    "    # if count > 1:\n",
    "    #     EPOCHS = 15\n",
    "    train_loss, train_mae, val_loss, val_mae = trainer.fit_epochs(\n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        use_cyclic_lr=True, \n",
    "        x_cat=True, \n",
    "        epochs=EPOCHS\n",
    "    )\n",
    "    plot_loss(train_loss, val_loss, f'Train-loss: {stock}', f'Valid-loss: {stock}')\n",
    "    plot_loss(train_mae, val_mae, f'Train-mae: {stock}', f'Valid-mae: {stock}')\n",
    "    # train_loss_list.extend(train_loss)\n",
    "    # train_mae_list.extend(train_mae)\n",
    "    # valid_loss_list.extend(val_loss)\n",
    "    # valid_mae_list.extend(val_mae)\n",
    "    # scaler_dict[stock] = scaler\n",
    "    print('#' * 20)\n",
    "    print()\n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "    # except Exception as e:\n",
    "    #     print(f'Training loop: {e}')\n",
    "    \n",
    "torch.save(model.state_dict(), './trained_model.pt')\n",
    "with open('scaler_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dl import plot_loss\n",
    "# plt.plot(train_loss_list)\n",
    "# plt.title('Train loss');\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(valid_loss_list)\n",
    "# plt.title('Valid loss')\n",
    "# plt.show()\n",
    "\n",
    "# plot_loss(train_loss_list, valid_loss_list, 'Train-loss', 'Valid-loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = 'example_test_files'\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "df = get_data(folder=FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for stock in stocks:\n",
    "    # teast_dataloader = None\n",
    "    # if count > 500:\n",
    "    #     break\n",
    "    # try:\n",
    "    print(f'Run prediction for stock: {stock}')\n",
    "    # transformer = scaler_dict[stock]\n",
    "    test_dataloader = dataloader_test_by_stock(\n",
    "        df, \n",
    "        stock, \n",
    "        # transformer=transformer,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        continous_cols=CONT_COLS,\n",
    "        target_col=None\n",
    "    )\n",
    "    for batch, data in enumerate(test_dataloader):\n",
    "        model.to('cpu')\n",
    "        x_con_test = data['num_features'].to(\"cpu\")\n",
    "        x_cat_test = data['cat_features'].to('cpu')\n",
    "        print('x.shape:', x_con_test.shape)\n",
    "        print('x_cat.shape:', x_cat_test.t().shape)\n",
    "        pred = model(x_con_test, x_cat_test)\n",
    "        print('pred:', pred)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "\n",
    "for stock in stocks:\n",
    "    # teast_dataloader = None\n",
    "    # if count > 500:\n",
    "    #     break\n",
    "    # try:\n",
    "    print(f'Run prediction for stock: {stock}')\n",
    "    # transformer = scaler_dict[stock]\n",
    "    test_dataloader = dataloader_test_by_stock(\n",
    "        df, \n",
    "        stock, \n",
    "        # transformer=transformer,\n",
    "        batch_size=BATCH_SIZE, \n",
    "        continous_cols=CONT_COLS,\n",
    "        target_col=None\n",
    "    )\n",
    "    pred_list = run_pred_step(test_dataloader, model, x_cat=True, target=False)\n",
    "    df_pred = pd.DataFrame(pred_list, index=[stock])\n",
    "    pred_df = pd.concat([pred_df, df_pred])#.pivot()\n",
    "    print('pred_df')\n",
    "    print(pred_df[:5])\n",
    "    # pred_df[stock] = pred_list\n",
    "    print()\n",
    "    count += 1\n",
    "#     except Exception as e:\n",
    "#         print(f'Exception {e}')\n",
    "        \n",
    "\n",
    "# pred_df = pd.DataFrame.from_dict(pred_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
