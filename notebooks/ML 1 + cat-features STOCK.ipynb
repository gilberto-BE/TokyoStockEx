{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics as TM\n",
    "pl.utilities.seed.seed_everything(seed=42)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(source_path)\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'preprocessing')\n",
    "sys.path.append(source_path)\n",
    "\n",
    "from models import NeuralNetwork, Trainer\n",
    "from preprocess import (\n",
    "    show_df, \n",
    "    date_features, \n",
    "    preprocess, \n",
    "    ToTorch, \n",
    "    get_loader, \n",
    "    ts_split,\n",
    "    cont_cat_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get Data and train a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_rows: 901\n",
      "no_rows: 301\n",
      "xtrain.shape: (900, 1)\n",
      "[[-0.00145879]\n",
      " [ 0.00073046]\n",
      " [ 0.00291971]\n",
      " [-0.0010917 ]\n",
      " [-0.00510018]]\n",
      "\n",
      "ytrain.shape: (901, 1)\n",
      "df_train_cat.shape: (901, 3)\n",
      "            day_of_year  month  day_of_week\n",
      "Date                                       \n",
      "2017-01-04            4      1            4\n",
      "2017-01-05            5      1            5\n",
      "2017-01-06            6      1            6\n",
      "2017-01-10           10      1           10\n",
      "2017-01-11           11      1           11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' xtrain and df_train_cat have different shapes!!!!!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH = 'c:/Users/gilbe/Documents/TokyoData'\n",
    "\n",
    "\n",
    "'/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/financials.csv'\n",
    "'/train_files/trades.csv'\n",
    "\n",
    "train_df = pd.read_csv(f'{ROOT_PATH}/train_files/stock_prices.csv')\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date']) \n",
    "train_df.set_index('Date', inplace=True)\n",
    "# train_df = date_features(train_df)\n",
    "\n",
    "train_options = pd.read_csv(f'{ROOT_PATH}/train_files/options.csv', low_memory=False)\n",
    "train_financials = pd.read_csv(f'{ROOT_PATH}/train_files/financials.csv', low_memory=False)\n",
    "train_trades = pd.read_csv(f'{ROOT_PATH}/train_files/trades.csv', low_memory=False)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_1301 = train_df[train_df['SecuritiesCode'] == 1301].drop(['SecuritiesCode', 'Volume'], axis=1)\n",
    "\n",
    "df_1301 = date_features(df_1301)\n",
    "\n",
    "cont, cat = cont_cat_split(df_1301, 'int64')\n",
    "df_train_cat, df_val_cat = ts_split(cat)\n",
    "df_train, df_val = ts_split(cont)\n",
    "\n",
    "\n",
    "xtrain, ytrain = preprocess(df_train, 'Target', 1, continous_cols=['Close'])\n",
    "xval, yval = preprocess(df_val, 'Target', 1, continous_cols=['Close'])\n",
    "\n",
    "\n",
    "print('xtrain.shape:', xtrain.shape)\n",
    "print(xtrain[:5])\n",
    "print()\n",
    "print('ytrain.shape:', ytrain.shape)\n",
    "print('df_train_cat.shape:', df_train_cat.shape)\n",
    "print(df_train_cat.head())\n",
    "\n",
    "\"\"\" xtrain and df_train_cat have different shapes!!!!!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_features: 7\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (embedding): Embedding(901, 2)\n",
      "  (emb_input): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (emb_output): Linear(in_features=5, out_features=1, bias=True)\n",
      "  (cont_input): Linear(in_features=1, out_features=5, bias=True)\n",
      "  (hidden_layer): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "Using cpu-device\n",
      "Train-Loss: 0.07647451013326645 [0/225]\n",
      "Train-Loss: 0.07831727713346481 [1/225]\n",
      "Train-Loss: 0.07330185174942017 [2/225]\n",
      "Train-Loss: 0.06964460760354996 [3/225]\n",
      "Train-Loss: 0.0657733604311943 [4/225]\n",
      "Train-Loss: 0.05720509588718414 [5/225]\n",
      "Train-Loss: 0.04262036457657814 [6/225]\n",
      "Train-Loss: 0.03131498396396637 [7/225]\n",
      "Train-Loss: 0.019496385008096695 [8/225]\n",
      "Train-Loss: 0.008636076003313065 [9/225]\n",
      "Train-Loss: 0.0018690002616494894 [10/225]\n",
      "Train-Loss: 0.0005088847829028964 [11/225]\n",
      "Train-Loss: 0.005654815584421158 [12/225]\n",
      "Train-Loss: 0.018020890653133392 [13/225]\n",
      "Train-Loss: 0.031462375074625015 [14/225]\n",
      "Train-Loss: 0.05174451693892479 [15/225]\n",
      "Train-Loss: 0.04744305461645126 [16/225]\n",
      "Train-Loss: 0.0347200445830822 [17/225]\n",
      "Train-Loss: 0.02334223873913288 [18/225]\n",
      "Train-Loss: 0.011237112805247307 [19/225]\n",
      "Train-Loss: 0.004514460451900959 [20/225]\n",
      "Train-Loss: 0.00041368481470271945 [21/225]\n",
      "Train-Loss: 0.00018433907825965434 [22/225]\n",
      "Train-Loss: 0.0030467163305729628 [23/225]\n",
      "Train-Loss: 0.005890230648219585 [24/225]\n",
      "Train-Loss: 0.006788700819015503 [25/225]\n",
      "Train-Loss: 0.008063431829214096 [26/225]\n",
      "Train-Loss: 0.009760484099388123 [27/225]\n",
      "Train-Loss: 0.010843158699572086 [28/225]\n",
      "Train-Loss: 0.011179995723068714 [29/225]\n",
      "Train-Loss: 0.008016590029001236 [30/225]\n",
      "Train-Loss: 0.007816563360393047 [31/225]\n",
      "Train-Loss: 0.006792615167796612 [32/225]\n",
      "Train-Loss: 0.002990550361573696 [33/225]\n",
      "Train-Loss: 0.0019419583259150386 [34/225]\n",
      "Train-Loss: 0.0005982907023280859 [35/225]\n",
      "Train-Loss: 0.0007844145293347538 [36/225]\n",
      "Train-Loss: 0.00118915899656713 [37/225]\n",
      "Train-Loss: 0.0008693606359884143 [38/225]\n",
      "Train-Loss: 0.001799269113689661 [39/225]\n",
      "Train-Loss: 0.0029701145831495523 [40/225]\n",
      "Train-Loss: 0.0015456380788236856 [41/225]\n",
      "Train-Loss: 0.0014922723639756441 [42/225]\n",
      "Train-Loss: 0.002225500764325261 [43/225]\n",
      "Train-Loss: 0.000946172745898366 [44/225]\n",
      "Train-Loss: 0.0015519231092184782 [45/225]\n",
      "Train-Loss: 0.000948058906942606 [46/225]\n",
      "Train-Loss: 0.00021761166863143444 [47/225]\n",
      "Train-Loss: 0.00017154452507384121 [48/225]\n",
      "Train-Loss: 0.0004192485648673028 [49/225]\n",
      "Train-Loss: 0.0002844335394911468 [50/225]\n",
      "Train-Loss: 0.0013482554350048304 [51/225]\n",
      "Train-Loss: 0.001107563148252666 [52/225]\n",
      "Train-Loss: 0.0008523506112396717 [53/225]\n",
      "Train-Loss: 0.0008216726128011942 [54/225]\n",
      "Train-Loss: 0.0004507842822931707 [55/225]\n",
      "Train-Loss: 0.0001588378509040922 [56/225]\n",
      "Train-Loss: 3.596695023588836e-05 [57/225]\n",
      "Train-Loss: 0.00026554474607110023 [58/225]\n",
      "Train-Loss: 0.00021219186601229012 [59/225]\n",
      "Train-Loss: 0.0003329564642626792 [60/225]\n",
      "Train-Loss: 5.7120181736536324e-05 [61/225]\n",
      "Train-Loss: 0.0004909018753096461 [62/225]\n",
      "Train-Loss: 0.00019065664673689753 [63/225]\n",
      "Train-Loss: 0.0003645589458756149 [64/225]\n",
      "Train-Loss: 0.0001380239409627393 [65/225]\n",
      "Train-Loss: 0.0003758785896934569 [66/225]\n",
      "Train-Loss: 0.00019797086133621633 [67/225]\n",
      "Train-Loss: 0.0004456525784917176 [68/225]\n",
      "Train-Loss: 0.00020572228822857141 [69/225]\n",
      "Train-Loss: 0.00010890868725255132 [70/225]\n",
      "Train-Loss: 0.00014098247629590333 [71/225]\n",
      "Train-Loss: 9.388002217747271e-05 [72/225]\n",
      "Train-Loss: 0.0001069443896994926 [73/225]\n",
      "Train-Loss: 0.00019273390353191644 [74/225]\n",
      "Train-Loss: 0.00045209802919998765 [75/225]\n",
      "Train-Loss: 0.0001325552730122581 [76/225]\n",
      "Train-Loss: 0.0005414516781456769 [77/225]\n",
      "Train-Loss: 0.00019005194189958274 [78/225]\n",
      "Train-Loss: 0.0002045908331638202 [79/225]\n",
      "Train-Loss: 1.816024087020196e-05 [80/225]\n",
      "Train-Loss: 0.0002543229784350842 [81/225]\n",
      "Train-Loss: 0.0002031159383477643 [82/225]\n",
      "Train-Loss: 0.0002960739948321134 [83/225]\n",
      "Train-Loss: 6.904132897034287e-05 [84/225]\n",
      "Train-Loss: 0.00046393717639148235 [85/225]\n",
      "Train-Loss: 7.358771108556539e-05 [86/225]\n",
      "Train-Loss: 0.0001685128954704851 [87/225]\n",
      "Train-Loss: 3.9679045585216954e-05 [88/225]\n",
      "Train-Loss: 0.0002448925224598497 [89/225]\n",
      "Train-Loss: 0.00027109874645248055 [90/225]\n",
      "Train-Loss: 0.0003907090285792947 [91/225]\n",
      "Train-Loss: 0.00040362487197853625 [92/225]\n",
      "Train-Loss: 0.00025081081548705697 [93/225]\n",
      "Train-Loss: 6.854880484752357e-05 [94/225]\n",
      "Train-Loss: 9.949278319254518e-05 [95/225]\n",
      "Train-Loss: 9.197852341458201e-05 [96/225]\n",
      "Train-Loss: 0.0010589455487206578 [97/225]\n",
      "Train-Loss: 0.00010630996257532388 [98/225]\n",
      "Train-Loss: 0.00012831825006287545 [99/225]\n",
      "Train-Loss: 0.0001169235838460736 [100/225]\n",
      "Train-Loss: 2.592824603198096e-05 [101/225]\n",
      "Train-Loss: 0.00027873972430825233 [102/225]\n",
      "Train-Loss: 0.00030433290521614254 [103/225]\n",
      "Train-Loss: 0.00010065791138913482 [104/225]\n",
      "Train-Loss: 0.00021243833180051297 [105/225]\n",
      "Train-Loss: 0.0004338118596933782 [106/225]\n",
      "Train-Loss: 0.00010956679761875421 [107/225]\n",
      "Train-Loss: 0.0005899306852370501 [108/225]\n",
      "Train-Loss: 0.00010304044553777203 [109/225]\n",
      "Train-Loss: 7.794432167429477e-05 [110/225]\n",
      "Train-Loss: 0.0006031254306435585 [111/225]\n",
      "Train-Loss: 0.0004617049125954509 [112/225]\n",
      "Train-Loss: 0.00035297777503728867 [113/225]\n",
      "Train-Loss: 0.0007156362407840788 [114/225]\n",
      "Train-Loss: 0.00018440729763824493 [115/225]\n",
      "Train-Loss: 0.0001163412380265072 [116/225]\n",
      "Train-Loss: 0.0002632404211908579 [117/225]\n",
      "Train-Loss: 0.000555034726858139 [118/225]\n",
      "Train-Loss: 0.0010058210464194417 [119/225]\n",
      "Train-Loss: 0.0004286673211026937 [120/225]\n",
      "Train-Loss: 0.0019563317764550447 [121/225]\n",
      "Train-Loss: 0.0015203064540401101 [122/225]\n",
      "Train-Loss: 0.0003710862365551293 [123/225]\n",
      "Train-Loss: 0.0002276867162436247 [124/225]\n",
      "Train-Loss: 0.00015699716459494084 [125/225]\n",
      "Train-Loss: 2.9393009754130617e-05 [126/225]\n",
      "Train-Loss: 0.0002622266474645585 [127/225]\n",
      "Train-Loss: 0.0003003023157361895 [128/225]\n",
      "Train-Loss: 1.4924497008905746e-05 [129/225]\n",
      "Train-Loss: 0.00014002039097249508 [130/225]\n",
      "Train-Loss: 0.0002548126212786883 [131/225]\n",
      "Train-Loss: 0.00017022868269123137 [132/225]\n",
      "Train-Loss: 0.00027419536490924656 [133/225]\n",
      "Train-Loss: 8.45676040626131e-05 [134/225]\n",
      "Train-Loss: 0.00013007564120925963 [135/225]\n",
      "Train-Loss: 0.0006716581992805004 [136/225]\n",
      "Train-Loss: 0.00012744989362545311 [137/225]\n",
      "Train-Loss: 0.00042193836998194456 [138/225]\n",
      "Train-Loss: 0.00035650498466566205 [139/225]\n",
      "Train-Loss: 6.492843385785818e-05 [140/225]\n",
      "Train-Loss: 4.576463106786832e-05 [141/225]\n",
      "Train-Loss: 6.23215310042724e-05 [142/225]\n",
      "Train-Loss: 0.0004774969129357487 [143/225]\n",
      "Train-Loss: 0.00016791440430097282 [144/225]\n",
      "Train-Loss: 9.214771853294224e-05 [145/225]\n",
      "Train-Loss: 0.000533372163772583 [146/225]\n",
      "Train-Loss: 9.367825259687379e-05 [147/225]\n",
      "Train-Loss: 0.00013368800864554942 [148/225]\n",
      "Train-Loss: 0.0002049479226116091 [149/225]\n",
      "Train-Loss: 0.0005625711055472493 [150/225]\n",
      "Train-Loss: 0.0002987004700116813 [151/225]\n",
      "Train-Loss: 1.0993095202138647e-05 [152/225]\n",
      "Train-Loss: 0.00024163888883776963 [153/225]\n",
      "Train-Loss: 0.0006930372328497469 [154/225]\n",
      "Train-Loss: 9.878792479867116e-05 [155/225]\n",
      "Train-Loss: 0.00012644992966670543 [156/225]\n",
      "Train-Loss: 0.00017672193644102663 [157/225]\n",
      "Train-Loss: 0.0002503471332602203 [158/225]\n",
      "Train-Loss: 2.3767021048115566e-05 [159/225]\n",
      "Train-Loss: 0.000302490167086944 [160/225]\n",
      "Train-Loss: 0.00019039625476580113 [161/225]\n",
      "Train-Loss: 0.0005347701371647418 [162/225]\n",
      "Train-Loss: 0.00013635552022606134 [163/225]\n",
      "Train-Loss: 2.815317020576913e-05 [164/225]\n",
      "Train-Loss: 0.00042089566704817116 [165/225]\n",
      "Train-Loss: 0.00045042193960398436 [166/225]\n",
      "Train-Loss: 0.00020420964574441314 [167/225]\n",
      "Train-Loss: 0.00033268320839852095 [168/225]\n",
      "Train-Loss: 0.00029384662047959864 [169/225]\n",
      "Train-Loss: 7.606690633110702e-05 [170/225]\n",
      "Train-Loss: 9.801915439311415e-05 [171/225]\n",
      "Train-Loss: 3.916906280210242e-05 [172/225]\n",
      "Train-Loss: 0.00038128424785099924 [173/225]\n",
      "Train-Loss: 0.00029764455393888056 [174/225]\n",
      "Train-Loss: 0.00014637710410170257 [175/225]\n",
      "Train-Loss: 1.3653198038809933e-05 [176/225]\n",
      "Train-Loss: 4.931799412588589e-05 [177/225]\n",
      "Train-Loss: 9.888695785775781e-05 [178/225]\n",
      "Train-Loss: 8.989334310172126e-05 [179/225]\n",
      "Train-Loss: 3.788358662859537e-05 [180/225]\n",
      "Train-Loss: 1.2032047379761934e-05 [181/225]\n",
      "Train-Loss: 2.174033215851523e-05 [182/225]\n",
      "Train-Loss: 3.1183037208393216e-05 [183/225]\n",
      "Train-Loss: 0.00012223210069350898 [184/225]\n",
      "Train-Loss: 0.00014354668383020908 [185/225]\n",
      "Train-Loss: 0.00010389839007984847 [186/225]\n",
      "Train-Loss: 9.256332123186439e-05 [187/225]\n",
      "Train-Loss: 0.0002069758193101734 [188/225]\n",
      "Train-Loss: 5.055264045950025e-05 [189/225]\n",
      "Train-Loss: 3.171796925016679e-05 [190/225]\n",
      "Train-Loss: 0.0004395161522552371 [191/225]\n",
      "Train-Loss: 0.0004372113908175379 [192/225]\n",
      "Train-Loss: 0.0002314092853339389 [193/225]\n",
      "Train-Loss: 0.0024084504693746567 [194/225]\n",
      "Train-Loss: 0.0009634995949454606 [195/225]\n",
      "Train-Loss: 0.0010977264028042555 [196/225]\n",
      "Train-Loss: 0.0008034377824515104 [197/225]\n",
      "Train-Loss: 0.0002953608927782625 [198/225]\n",
      "Train-Loss: 0.00011948429892072454 [199/225]\n",
      "Train-Loss: 0.0002981778234243393 [200/225]\n",
      "Train-Loss: 8.701818296685815e-05 [201/225]\n",
      "Train-Loss: 0.00026003533275797963 [202/225]\n",
      "Train-Loss: 0.0009601629571989179 [203/225]\n",
      "Train-Loss: 0.0002014330675592646 [204/225]\n",
      "Train-Loss: 0.0003773678035940975 [205/225]\n",
      "Train-Loss: 0.0004476127796806395 [206/225]\n",
      "Train-Loss: 0.0001324708282481879 [207/225]\n",
      "Train-Loss: 8.195690315915272e-05 [208/225]\n",
      "Train-Loss: 0.00021540900343097746 [209/225]\n",
      "Train-Loss: 0.00016988084826152772 [210/225]\n",
      "Train-Loss: 0.0005158546264283359 [211/225]\n",
      "Train-Loss: 0.0001643715368118137 [212/225]\n",
      "Train-Loss: 3.108173405053094e-05 [213/225]\n",
      "Train-Loss: 0.0004961157101206481 [214/225]\n",
      "Train-Loss: 0.00016616247012279928 [215/225]\n",
      "Train-Loss: 0.00034522873465903103 [216/225]\n",
      "Train-Loss: 0.0002581828448455781 [217/225]\n",
      "Train-Loss: 0.0002583770838100463 [218/225]\n",
      "Train-Loss: 0.00032990833278745413 [219/225]\n",
      "Train-Loss: 0.0001489142159698531 [220/225]\n",
      "Train-Loss: 0.00016818914446048439 [221/225]\n",
      "Train-Loss: 0.00011188117059646174 [222/225]\n",
      "Train-Loss: 5.544345185626298e-05 [223/225]\n",
      "Train-Loss: 0.00015756308857817203 [224/225]\n",
      "Val-Loss: 2.709630280151032e-05 [1/75]\n",
      "Val-Loss: 0.00017346622189506888 [2/75]\n",
      "Val-Loss: 0.00034604259417392313 [3/75]\n",
      "Val-Loss: 0.00025710678892210126 [4/75]\n",
      "Val-Loss: 7.291855581570417e-05 [5/75]\n",
      "Val-Loss: 0.00018105882918462157 [6/75]\n",
      "Val-Loss: 9.60100514930673e-05 [7/75]\n",
      "Val-Loss: 7.349991938099265e-05 [8/75]\n",
      "Val-Loss: 0.0006530469981953502 [9/75]\n",
      "Val-Loss: 0.0001552607282064855 [10/75]\n",
      "Val-Loss: 6.967961235204712e-05 [11/75]\n",
      "Val-Loss: 8.09567718533799e-05 [12/75]\n",
      "Val-Loss: 0.00013168853183742613 [13/75]\n",
      "Val-Loss: 8.363069355254993e-05 [14/75]\n",
      "Val-Loss: 8.657839498482645e-05 [15/75]\n",
      "Val-Loss: 9.355014481116086e-05 [16/75]\n",
      "Val-Loss: 0.00017161559662781656 [17/75]\n",
      "Val-Loss: 6.037585626472719e-05 [18/75]\n",
      "Val-Loss: 9.949090599548072e-05 [19/75]\n",
      "Val-Loss: 0.00041773502016440034 [20/75]\n",
      "Val-Loss: 0.00023668141511734575 [21/75]\n",
      "Val-Loss: 0.0001680452551227063 [22/75]\n",
      "Val-Loss: 0.0003210926952306181 [23/75]\n",
      "Val-Loss: 3.350089900777675e-05 [24/75]\n",
      "Val-Loss: 5.345037061488256e-05 [25/75]\n",
      "Val-Loss: 0.00015504185284953564 [26/75]\n",
      "Val-Loss: 0.00011384618119336665 [27/75]\n",
      "Val-Loss: 0.0001111765086534433 [28/75]\n",
      "Val-Loss: 1.661013811826706e-05 [29/75]\n",
      "Val-Loss: 0.00015494583931285888 [30/75]\n",
      "Val-Loss: 6.042786844773218e-05 [31/75]\n",
      "Val-Loss: 0.0002718520991038531 [32/75]\n",
      "Val-Loss: 0.0006487526698037982 [33/75]\n",
      "Val-Loss: 0.00026527175214141607 [34/75]\n",
      "Val-Loss: 0.00025466957595199347 [35/75]\n",
      "Val-Loss: 5.7144461607094854e-05 [36/75]\n",
      "Val-Loss: 0.00031449925154447556 [37/75]\n",
      "Val-Loss: 0.00019048404647037387 [38/75]\n",
      "Val-Loss: 0.00014832957822363824 [39/75]\n",
      "Val-Loss: 0.0001401627087034285 [40/75]\n",
      "Val-Loss: 9.613929432816803e-05 [41/75]\n",
      "Val-Loss: 0.00016030138067435473 [42/75]\n",
      "Val-Loss: 0.0001592934859218076 [43/75]\n",
      "Val-Loss: 0.00026841697399504483 [44/75]\n",
      "Val-Loss: 3.9756625483278185e-05 [45/75]\n",
      "Val-Loss: 4.479580820770934e-05 [46/75]\n",
      "Val-Loss: 7.725752220721915e-05 [47/75]\n",
      "Val-Loss: 1.2044294635416009e-05 [48/75]\n",
      "Val-Loss: 5.5440159485442564e-05 [49/75]\n",
      "Val-Loss: 7.530712900916114e-05 [50/75]\n",
      "Val-Loss: 5.412229074863717e-05 [51/75]\n",
      "Val-Loss: 0.00010259206464979798 [52/75]\n",
      "Val-Loss: 3.304099664092064e-05 [53/75]\n",
      "Val-Loss: 5.596144910668954e-05 [54/75]\n",
      "Val-Loss: 0.000287089089397341 [55/75]\n",
      "Val-Loss: 0.00011325968080200255 [56/75]\n",
      "Val-Loss: 0.0001274061796721071 [57/75]\n",
      "Val-Loss: 9.844143642112613e-05 [58/75]\n",
      "Val-Loss: 0.00016480011981911957 [59/75]\n",
      "Val-Loss: 1.174425233330112e-05 [60/75]\n",
      "Val-Loss: 4.777353387908079e-05 [61/75]\n",
      "Val-Loss: 9.824851440498605e-05 [62/75]\n",
      "Val-Loss: 0.0003959887253586203 [63/75]\n",
      "Val-Loss: 0.0001734989637043327 [64/75]\n",
      "Val-Loss: 0.00013895354641135782 [65/75]\n",
      "Val-Loss: 0.0001871385466074571 [66/75]\n",
      "Val-Loss: 9.673528256826103e-05 [67/75]\n",
      "Val-Loss: 0.00018671226280275732 [68/75]\n",
      "Val-Loss: 8.030483877519146e-05 [69/75]\n",
      "Val-Loss: 0.0011954527581110597 [70/75]\n",
      "Val-Loss: 0.0002654428535606712 [71/75]\n",
      "Val-Loss: 0.00024614142603240907 [72/75]\n",
      "Val-Loss: 0.0001560645323479548 [73/75]\n",
      "Val-Loss: 0.00030373616027645767 [74/75]\n",
      "Val-Loss: 7.776430720696226e-05 [75/75]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "batch_size = 4\n",
    "train_dataloader = get_loader(x=xtrain, y=ytrain, batch_size=batch_size, x_cat=df_train_cat.to_numpy())\n",
    "val_dataloader = get_loader(x=xval, y=yval, batch_size=batch_size, x_cat=df_val_cat.to_numpy())\n",
    "\n",
    "\n",
    "embedding_dim = 2\n",
    "cat_features = 3 \n",
    "cat_features = cat_features * embedding_dim\n",
    "print('in_features:', xtrain.shape[1] + cat_features)\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    in_features=xtrain.shape[1], \n",
    "    units=5 ,\n",
    "    out_features=1, \n",
    "    no_embedding=len(df_train_cat), \n",
    "    emb_dim=embedding_dim\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "trainer = Trainer(model, lr=3e-6)\n",
    "trainer.fit_one_epoch(train_dataloader, val_dataloader, use_cyclic_lr=True, x_cat=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
