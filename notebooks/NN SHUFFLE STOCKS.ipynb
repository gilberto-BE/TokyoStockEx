{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gilbe\\anaconda3\\envs\\tokyo\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics as TM\n",
    "# pl.utilities.seed.seed_everything(seed=42)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(source_path)\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'preprocessing')\n",
    "sys.path.append(source_path)\n",
    "source_path = os.path.join(os.getcwd(), os.pardir, 'metrics')\n",
    "sys.path.append(source_path)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dl import NeuralNetwork, Trainer\n",
    "from preprocess import (\n",
    "    show_df, \n",
    "    date_features, \n",
    "    preprocess, \n",
    "    ToTorch, \n",
    "    get_loader, \n",
    "    ts_split,\n",
    "    cont_cat_split,\n",
    "    dataloader_by_stock,\n",
    "    get_data\n",
    ")\n",
    "from metrics.metrics import calc_spread_return_sharpe\n",
    "print(torch.__version__)\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dl import plot_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, True, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.version.cuda), torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get Data and train a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the unique security codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.head(10):\n",
      "            Section/Products  33SectorName  17SectorName  Universe0  \\\n",
      "Date                                                                  \n",
      "2017-01-04                 0             6             8          0   \n",
      "2017-01-05                 0             6             8          0   \n",
      "2017-01-06                 0             6             8          0   \n",
      "2017-01-10                 0             6             8          0   \n",
      "2017-01-11                 0             6             8          0   \n",
      "2017-01-12                 0             6             8          0   \n",
      "2017-01-13                 0             6             8          0   \n",
      "2017-01-16                 0             6             8          0   \n",
      "2017-01-17                 0             6             8          0   \n",
      "2017-01-18                 0             6             8          0   \n",
      "\n",
      "            MarketCapitalization  SecuritiesCode          RowId    Open  \\\n",
      "Date                                                                      \n",
      "2017-01-04          3.365911e+10            1301  20170104_1301  2734.0   \n",
      "2017-01-05          3.365911e+10            1301  20170105_1301  2743.0   \n",
      "2017-01-06          3.365911e+10            1301  20170106_1301  2734.0   \n",
      "2017-01-10          3.365911e+10            1301  20170110_1301  2745.0   \n",
      "2017-01-11          3.365911e+10            1301  20170111_1301  2748.0   \n",
      "2017-01-12          3.365911e+10            1301  20170112_1301  2745.0   \n",
      "2017-01-13          3.365911e+10            1301  20170113_1301  2707.0   \n",
      "2017-01-16          3.365911e+10            1301  20170116_1301  2725.0   \n",
      "2017-01-17          3.365911e+10            1301  20170117_1301  2702.0   \n",
      "2017-01-18          3.365911e+10            1301  20170118_1301  2689.0   \n",
      "\n",
      "              High     Low   Close  Volume  AdjustmentFactor  \\\n",
      "Date                                                           \n",
      "2017-01-04  2755.0  2730.0  2742.0   31400               1.0   \n",
      "2017-01-05  2747.0  2735.0  2738.0   17900               1.0   \n",
      "2017-01-06  2744.0  2720.0  2740.0   19900               1.0   \n",
      "2017-01-10  2754.0  2735.0  2748.0   24200               1.0   \n",
      "2017-01-11  2752.0  2737.0  2745.0    9300               1.0   \n",
      "2017-01-12  2747.0  2703.0  2731.0   28700               1.0   \n",
      "2017-01-13  2730.0  2707.0  2722.0   19400               1.0   \n",
      "2017-01-16  2725.0  2696.0  2704.0   20100               1.0   \n",
      "2017-01-17  2704.0  2682.0  2686.0   18400               1.0   \n",
      "2017-01-18  2695.0  2681.0  2694.0   12100               1.0   \n",
      "\n",
      "            ExpectedDividend  SupervisionFlag    Target  \n",
      "Date                                                     \n",
      "2017-01-04               NaN            False  0.000730  \n",
      "2017-01-05               NaN            False  0.002920  \n",
      "2017-01-06               NaN            False -0.001092  \n",
      "2017-01-10               NaN            False -0.005100  \n",
      "2017-01-11               NaN            False -0.003295  \n",
      "2017-01-12               NaN            False -0.006613  \n",
      "2017-01-13               NaN            False -0.006657  \n",
      "2017-01-16               NaN            False  0.002978  \n",
      "2017-01-17               NaN            False  0.001856  \n",
      "2017-01-18               NaN            False  0.014079  \n",
      "Date\n",
      "2017-01-04    1.0\n",
      "2017-01-05    1.0\n",
      "2017-01-06    1.0\n",
      "2017-01-10    1.0\n",
      "2017-01-11    1.0\n",
      "             ... \n",
      "2021-11-29    1.0\n",
      "2021-11-30    1.0\n",
      "2021-12-01    1.0\n",
      "2021-12-02    1.0\n",
      "2021-12-03    1.0\n",
      "Name: AdjustmentFactor, Length: 2332531, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section/Products</th>\n",
       "      <th>33SectorName</th>\n",
       "      <th>17SectorName</th>\n",
       "      <th>Universe0</th>\n",
       "      <th>MarketCapitalization</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>RowId</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjustmentFactor</th>\n",
       "      <th>ExpectedDividend</th>\n",
       "      <th>SupervisionFlag</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.365911e+10</td>\n",
       "      <td>1301</td>\n",
       "      <td>20170104_1301</td>\n",
       "      <td>2734.0</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>2742.0</td>\n",
       "      <td>31400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.365911e+10</td>\n",
       "      <td>1301</td>\n",
       "      <td>20170105_1301</td>\n",
       "      <td>2743.0</td>\n",
       "      <td>2747.0</td>\n",
       "      <td>2735.0</td>\n",
       "      <td>2738.0</td>\n",
       "      <td>17900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.002920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.365911e+10</td>\n",
       "      <td>1301</td>\n",
       "      <td>20170106_1301</td>\n",
       "      <td>2734.0</td>\n",
       "      <td>2744.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>2740.0</td>\n",
       "      <td>19900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.001092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-10</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.365911e+10</td>\n",
       "      <td>1301</td>\n",
       "      <td>20170110_1301</td>\n",
       "      <td>2745.0</td>\n",
       "      <td>2754.0</td>\n",
       "      <td>2735.0</td>\n",
       "      <td>2748.0</td>\n",
       "      <td>24200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-11</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3.365911e+10</td>\n",
       "      <td>1301</td>\n",
       "      <td>20170111_1301</td>\n",
       "      <td>2748.0</td>\n",
       "      <td>2752.0</td>\n",
       "      <td>2737.0</td>\n",
       "      <td>2745.0</td>\n",
       "      <td>9300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.003295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Section/Products  33SectorName  17SectorName  Universe0  \\\n",
       "Date                                                                  \n",
       "2017-01-04                 0             6             8          0   \n",
       "2017-01-05                 0             6             8          0   \n",
       "2017-01-06                 0             6             8          0   \n",
       "2017-01-10                 0             6             8          0   \n",
       "2017-01-11                 0             6             8          0   \n",
       "\n",
       "            MarketCapitalization  SecuritiesCode          RowId    Open  \\\n",
       "Date                                                                      \n",
       "2017-01-04          3.365911e+10            1301  20170104_1301  2734.0   \n",
       "2017-01-05          3.365911e+10            1301  20170105_1301  2743.0   \n",
       "2017-01-06          3.365911e+10            1301  20170106_1301  2734.0   \n",
       "2017-01-10          3.365911e+10            1301  20170110_1301  2745.0   \n",
       "2017-01-11          3.365911e+10            1301  20170111_1301  2748.0   \n",
       "\n",
       "              High     Low   Close  Volume  AdjustmentFactor  \\\n",
       "Date                                                           \n",
       "2017-01-04  2755.0  2730.0  2742.0   31400               1.0   \n",
       "2017-01-05  2747.0  2735.0  2738.0   17900               1.0   \n",
       "2017-01-06  2744.0  2720.0  2740.0   19900               1.0   \n",
       "2017-01-10  2754.0  2735.0  2748.0   24200               1.0   \n",
       "2017-01-11  2752.0  2737.0  2745.0    9300               1.0   \n",
       "\n",
       "            ExpectedDividend  SupervisionFlag    Target  \n",
       "Date                                                     \n",
       "2017-01-04               NaN            False  0.000730  \n",
       "2017-01-05               NaN            False  0.002920  \n",
       "2017-01-06               NaN            False -0.001092  \n",
       "2017-01-10               NaN            False -0.005100  \n",
       "2017-01-11               NaN            False -0.003295  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2332531 entries, 2017-01-04 to 2021-12-03\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Dtype  \n",
      "---  ------                -----  \n",
      " 0   Section/Products      int64  \n",
      " 1   33SectorName          int64  \n",
      " 2   17SectorName          int64  \n",
      " 3   Universe0             int64  \n",
      " 4   MarketCapitalization  float64\n",
      " 5   SecuritiesCode        int64  \n",
      " 6   RowId                 object \n",
      " 7   Open                  float64\n",
      " 8   High                  float64\n",
      " 9   Low                   float64\n",
      " 10  Close                 float64\n",
      " 11  Volume                int64  \n",
      " 12  AdjustmentFactor      float64\n",
      " 13  ExpectedDividend      float64\n",
      " 14  SupervisionFlag       bool   \n",
      " 15  Target                float64\n",
      "dtypes: bool(1), float64(8), int64(6), object(1)\n",
      "memory usage: 287.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df = get_data()\n",
    "# print('Unique adjustment factor:', train_df['AdjustmentFactor'].unique())\n",
    "print(train_df['AdjustmentFactor'])\n",
    "display(train_df.head())\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args and constants\n",
    "* Adding financials wes cumbersome maybe not worth now to add that data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAT_FEATURES: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "SUBTRACT:\n",
    "*) 3 FROM CONTINUOUS COLS BECAUSE OF POOLING\n",
    "*) 1 FROM CAT_FEATURES TO MAKE MATRICES MATCH AFTER TORCH.CAT\n",
    "\"\"\"\n",
    "CONT_COLS=['Close', 'Open', 'High', 'Low', 'MarketCapitalization',         \n",
    "           # 'NetSales', 'EquityToAssetRatio', 'TotalAssets', 'Profit', \n",
    "           # 'OperatingProfit', 'EarningsPerShare', 'Equity', \n",
    "           # 'BookValuePerShare', 'ResultDividendPerShare1stQuarter', \n",
    "           # 'ResultDividendPerShare2ndQuarter', 'ResultDividendPerShare3rdQuarter',\n",
    "           # 'ResultDividendPerShareFiscalYearEnd', 'ResultDividendPerShareAnnual'\n",
    "          ]\n",
    "TS_IN_FEATURES = len(CONT_COLS)\n",
    "CAT_FEATURES = 4 + 4 - 1 # TEXT_COLS = ['Section/Products', '33SectorName', '17SectorName', 'Universe0']\n",
    "print('CAT_FEATURES:', CAT_FEATURES)\n",
    "EMBEDDING_DIM = 300\n",
    "NO_EMBEDDING = 2000 #2 * len(df_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    model = None\n",
    "    model = NeuralNetwork(\n",
    "        in_features=TS_IN_FEATURES - 3, \n",
    "        units=128,\n",
    "        out_features=1, \n",
    "        categorical_dim=CAT_FEATURES,\n",
    "        no_embedding=NO_EMBEDDING, \n",
    "        emb_dim=EMBEDDING_DIM,\n",
    "        n_blocks=4,\n",
    "        n_stacks=4,\n",
    "        dropout=0.1,\n",
    "        pooling_sizes=3\n",
    "    )\n",
    "\n",
    "    print(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop throug each stock\n",
    "Create Trainer only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (embedding_layer): Embedding(2000, 300)\n",
      "  (embedding_to_hidden): Linear(in_features=300, out_features=128, bias=True)\n",
      "  (embedding_output): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (cont_input): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (pooling_layer): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=True)\n",
      "  (stacks): ModuleList(\n",
      "    (0): NeuralStack(\n",
      "      (blocks): ModuleList(\n",
      "        (0): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (1): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (2): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (3): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (output_layers): ModuleList(\n",
      "        (0): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (1): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (2): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (3): Linear(in_features=135, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): NeuralStack(\n",
      "      (blocks): ModuleList(\n",
      "        (0): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (1): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (2): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (3): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (output_layers): ModuleList(\n",
      "        (0): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (1): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (2): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (3): Linear(in_features=135, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): NeuralStack(\n",
      "      (blocks): ModuleList(\n",
      "        (0): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (1): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (2): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (3): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (output_layers): ModuleList(\n",
      "        (0): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (1): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (2): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (3): Linear(in_features=135, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): NeuralStack(\n",
      "      (blocks): ModuleList(\n",
      "        (0): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (1): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (2): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "        (3): NeuralBlock(\n",
      "          (resnet_block): ModuleList(\n",
      "            (0): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): FFResNetBlock(\n",
      "              (layers): ModuleList(\n",
      "                (0): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (1): Linear(in_features=135, out_features=135, bias=True)\n",
      "                (2): Linear(in_features=135, out_features=135, bias=True)\n",
      "              )\n",
      "              (layer_norm): LayerNorm((135,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (fwr_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (output): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (backcast_layer): Linear(in_features=135, out_features=135, bias=True)\n",
      "          (back_cast): Linear(in_features=135, out_features=135, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (output_layers): ModuleList(\n",
      "        (0): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (1): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (2): Linear(in_features=135, out_features=1, bias=True)\n",
      "        (3): Linear(in_features=135, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Using cuda-device\n",
      "[3457 6779 6586 ... 8355 4100 4550]\n",
      "Stock-iteratation: 0\n",
      "Start training for stock: 3457\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.002099018692970276 | \n",
      "                    Train-Mae: 0.2739367187023163 |\n",
      "\n",
      "                    Average val loss: 0.029031630605459213|\n",
      "                    Val-Mae: 0.1611734926700592\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 1\n",
      "Start training for stock: 6779\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.0007113485038280487 | \n",
      "                    Train-Mae: 0.187022402882576 |\n",
      "\n",
      "                    Average val loss: 0.03459090366959572|\n",
      "                    Val-Mae: 0.18348188698291779\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 2\n",
      "Start training for stock: 6586\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00035366296768188474 | \n",
      "                    Train-Mae: 0.1420554667711258 |\n",
      "\n",
      "                    Average val loss: 0.022402312606573105|\n",
      "                    Val-Mae: 0.14842340350151062\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 3\n",
      "Start training for stock: 4676\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.0001983572170138359 | \n",
      "                    Train-Mae: 0.10526032000780106 |\n",
      "\n",
      "                    Average val loss: 0.006623364519327879|\n",
      "                    Val-Mae: 0.06437487155199051\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 4\n",
      "Start training for stock: 8909\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.0001166369952261448 | \n",
      "                    Train-Mae: 0.07451312988996506 |\n",
      "\n",
      "                    Average val loss: 0.005759601481258869|\n",
      "                    Val-Mae: 0.06697020679712296\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 5\n",
      "Start training for stock: 6745\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.0001937803067266941 | \n",
      "                    Train-Mae: 0.10719133913516998 |\n",
      "\n",
      "                    Average val loss: 0.00404586736112833|\n",
      "                    Val-Mae: 0.062429606914520264\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 6\n",
      "Start training for stock: 8016\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00012822562828660011 | \n",
      "                    Train-Mae: 0.08804488182067871 |\n",
      "\n",
      "                    Average val loss: 0.0037927981466054916|\n",
      "                    Val-Mae: 0.04704771563410759\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 7\n",
      "Start training for stock: 7181\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00014946185052394867 | \n",
      "                    Train-Mae: 0.09523431211709976 |\n",
      "\n",
      "                    Average val loss: 0.0008139043347910047|\n",
      "                    Val-Mae: 0.02801058627665043\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 8\n",
      "Start training for stock: 9434\n",
      "continuos shape: (717, 11)  categorical shape: (717, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([201, 7])\n",
      "x_cat after embedding: torch.Size([201, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([201, 7, 1])\n",
      "x.shape after fft2: torch.Size([201, 5])\n",
      "x.shape after pooling: torch.Size([201, 2])\n",
      "x.shape after first cont layer: torch.Size([201, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([201, 7])\n",
      "x_cat.squeeze().shape: torch.Size([201, 7])\n",
      "x.shape after torch.cat: torch.Size([201, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.491420350968838e-05 | \n",
      "                    Train-Mae: 0.07587496191263199 |\n",
      "\n",
      "                    Average val loss: 0.004287585616111755|\n",
      "                    Val-Mae: 0.05945590138435364\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 9\n",
      "Start training for stock: 1605\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.229110553860665e-05 | \n",
      "                    Train-Mae: 0.07186256349086761 |\n",
      "\n",
      "                    Average val loss: 0.0033390733879059553|\n",
      "                    Val-Mae: 0.05664104223251343\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 10\n",
      "Start training for stock: 6994\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00028036855161190033 | \n",
      "                    Train-Mae: 0.13085736334323883 |\n",
      "\n",
      "                    Average val loss: 0.022900588810443878|\n",
      "                    Val-Mae: 0.12744241952896118\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 11\n",
      "Start training for stock: 8043\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.23636446148157e-05 | \n",
      "                    Train-Mae: 0.073952317237854 |\n",
      "\n",
      "                    Average val loss: 0.017390012741088867|\n",
      "                    Val-Mae: 0.12577463686466217\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 12\n",
      "Start training for stock: 2742\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00016332689672708513 | \n",
      "                    Train-Mae: 0.10533216595649719 |\n",
      "\n",
      "                    Average val loss: 0.02152462862432003|\n",
      "                    Val-Mae: 0.12191613763570786\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 13\n",
      "Start training for stock: 2432\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.326591156423091e-05 | \n",
      "                    Train-Mae: 0.0732024535536766 |\n",
      "\n",
      "                    Average val loss: 0.0010702551808208227|\n",
      "                    Val-Mae: 0.03048151731491089\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 14\n",
      "Start training for stock: 4641\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.299095720052719e-05 | \n",
      "                    Train-Mae: 0.07288292795419693 |\n",
      "\n",
      "                    Average val loss: 0.0008755242452025414|\n",
      "                    Val-Mae: 0.026400143280625343\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 15\n",
      "Start training for stock: 9612\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.924644440412521e-05 | \n",
      "                    Train-Mae: 0.07023704797029495 |\n",
      "\n",
      "                    Average val loss: 0.00064466631738469|\n",
      "                    Val-Mae: 0.020761124789714813\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 16\n",
      "Start training for stock: 3677\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.227218106389046e-05 | \n",
      "                    Train-Mae: 0.06220020726323128 |\n",
      "\n",
      "                    Average val loss: 0.005610153079032898|\n",
      "                    Val-Mae: 0.053467750549316406\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 17\n",
      "Start training for stock: 9788\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.883928880095482e-05 | \n",
      "                    Train-Mae: 0.07364858686923981 |\n",
      "\n",
      "                    Average val loss: 0.0007285441970452666|\n",
      "                    Val-Mae: 0.02203229069709778\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 18\n",
      "Start training for stock: 8050\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.0006051205098628997 | \n",
      "                    Train-Mae: 0.07322198152542114 |\n",
      "\n",
      "                    Average val loss: 0.018584996461868286|\n",
      "                    Val-Mae: 0.10009288042783737\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 19\n",
      "Start training for stock: 6820\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.435308769345284e-05 | \n",
      "                    Train-Mae: 0.07110835611820221 |\n",
      "\n",
      "                    Average val loss: 0.0015462142182514071|\n",
      "                    Val-Mae: 0.03300058841705322\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 20\n",
      "Start training for stock: 9631\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00034642290323972703 | \n",
      "                    Train-Mae: 0.12959682941436768 |\n",
      "\n",
      "                    Average val loss: 0.012825462967157364|\n",
      "                    Val-Mae: 0.10449427366256714\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 21\n",
      "Start training for stock: 6914\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.660091854631901e-05 | \n",
      "                    Train-Mae: 0.06692713499069214 |\n",
      "\n",
      "                    Average val loss: 0.0011037762742489576|\n",
      "                    Val-Mae: 0.029769301414489746\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 22\n",
      "Start training for stock: 4933\n",
      "continuos shape: (293, 11)  categorical shape: (293, 7)\n",
      "x_cat.shape before embedding: torch.Size([289, 7])\n",
      "x_cat after embedding: torch.Size([289, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([289, 7, 1])\n",
      "x.shape after fft2: torch.Size([289, 5])\n",
      "x.shape after pooling: torch.Size([289, 2])\n",
      "x.shape after first cont layer: torch.Size([289, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([289, 7])\n",
      "x_cat.squeeze().shape: torch.Size([289, 7])\n",
      "x.shape after torch.cat: torch.Size([289, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00010945180431008339 | \n",
      "                    Train-Mae: 0.08511172980070114 |\n",
      "\n",
      "                    Average val loss: 0.025241825729608536|\n",
      "                    Val-Mae: 0.1361669898033142\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 23\n",
      "Start training for stock: 8158\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.33879953622818e-05 | \n",
      "                    Train-Mae: 0.07070793956518173 |\n",
      "\n",
      "                    Average val loss: 0.014341648668050766|\n",
      "                    Val-Mae: 0.11197078973054886\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 24\n",
      "Start training for stock: 6463\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.134083658456802e-05 | \n",
      "                    Train-Mae: 0.060157425701618195 |\n",
      "\n",
      "                    Average val loss: 0.005633260123431683|\n",
      "                    Val-Mae: 0.06516250967979431\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 25\n",
      "Start training for stock: 7327\n",
      "continuos shape: (772, 11)  categorical shape: (772, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([256, 7])\n",
      "x_cat after embedding: torch.Size([256, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([256, 7, 1])\n",
      "x.shape after fft2: torch.Size([256, 5])\n",
      "x.shape after pooling: torch.Size([256, 2])\n",
      "x.shape after first cont layer: torch.Size([256, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([256, 7])\n",
      "x_cat.squeeze().shape: torch.Size([256, 7])\n",
      "x.shape after torch.cat: torch.Size([256, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00010004423558712005 | \n",
      "                    Train-Mae: 0.07595454156398773 |\n",
      "\n",
      "                    Average val loss: 0.009820292703807354|\n",
      "                    Val-Mae: 0.07934369891881943\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 26\n",
      "Start training for stock: 3835\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.463384583592414e-05 | \n",
      "                    Train-Mae: 0.062405455857515335 |\n",
      "\n",
      "                    Average val loss: 0.0022114887833595276|\n",
      "                    Val-Mae: 0.03137149289250374\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 27\n",
      "Start training for stock: 6632\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.356934551149607e-05 | \n",
      "                    Train-Mae: 0.06488826125860214 |\n",
      "\n",
      "                    Average val loss: 0.0014925277791917324|\n",
      "                    Val-Mae: 0.03220353648066521\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 28\n",
      "Start training for stock: 3941\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00012995732016861437 | \n",
      "                    Train-Mae: 0.09066411852836609 |\n",
      "\n",
      "                    Average val loss: 0.010395902208983898|\n",
      "                    Val-Mae: 0.08551045507192612\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 29\n",
      "Start training for stock: 9733\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00012281308881938456 | \n",
      "                    Train-Mae: 0.06418467313051224 |\n",
      "\n",
      "                    Average val loss: 0.0025512780994176865|\n",
      "                    Val-Mae: 0.04321026802062988\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 30\n",
      "Start training for stock: 6963\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.22575543820858e-05 | \n",
      "                    Train-Mae: 0.06264059990644455 |\n",
      "\n",
      "                    Average val loss: 0.00075690564699471|\n",
      "                    Val-Mae: 0.02043834514915943\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 31\n",
      "Start training for stock: 6486\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.625895388424396e-05 | \n",
      "                    Train-Mae: 0.05836477503180504 |\n",
      "\n",
      "                    Average val loss: 0.005987570621073246|\n",
      "                    Val-Mae: 0.06440173834562302\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 32\n",
      "Start training for stock: 9746\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.2534354403615e-05 | \n",
      "                    Train-Mae: 0.0647769346833229 |\n",
      "\n",
      "                    Average val loss: 0.0016967657720670104|\n",
      "                    Val-Mae: 0.03621658310294151\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 33\n",
      "Start training for stock: 8890\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.877998054027557e-05 | \n",
      "                    Train-Mae: 0.05983725190162659 |\n",
      "\n",
      "                    Average val loss: 0.0063529773615300655|\n",
      "                    Val-Mae: 0.07610327750444412\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 34\n",
      "Start training for stock: 6371\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00031998530030250547 | \n",
      "                    Train-Mae: 0.06530582159757614 |\n",
      "\n",
      "                    Average val loss: 0.007989967241883278|\n",
      "                    Val-Mae: 0.08002524822950363\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 35\n",
      "Start training for stock: 3569\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.790272589772939e-05 | \n",
      "                    Train-Mae: 0.063395656645298 |\n",
      "\n",
      "                    Average val loss: 0.0008597854757681489|\n",
      "                    Val-Mae: 0.026377201080322266\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 36\n",
      "Start training for stock: 6287\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.486042402684689e-05 | \n",
      "                    Train-Mae: 0.05701274797320366 |\n",
      "\n",
      "                    Average val loss: 0.006165578495711088|\n",
      "                    Val-Mae: 0.06627358496189117\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 37\n",
      "Start training for stock: 2282\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.527243345975876e-05 | \n",
      "                    Train-Mae: 0.06908171623945236 |\n",
      "\n",
      "                    Average val loss: 0.003249536268413067|\n",
      "                    Val-Mae: 0.04726812615990639\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 38\n",
      "Start training for stock: 3880\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.498685598373413e-05 | \n",
      "                    Train-Mae: 0.07794284075498581 |\n",
      "\n",
      "                    Average val loss: 0.009493639692664146|\n",
      "                    Val-Mae: 0.07170852273702621\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 39\n",
      "Start training for stock: 6194\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00010638394393026829 | \n",
      "                    Train-Mae: 0.06985507160425186 |\n",
      "\n",
      "                    Average val loss: 0.0013990210136398673|\n",
      "                    Val-Mae: 0.03711538389325142\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 40\n",
      "Start training for stock: 1726\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.698081735521555e-05 | \n",
      "                    Train-Mae: 0.054984427988529205 |\n",
      "\n",
      "                    Average val loss: 0.000148845007061027|\n",
      "                    Val-Mae: 0.011619987897574902\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 41\n",
      "Start training for stock: 7239\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.073223263025284e-05 | \n",
      "                    Train-Mae: 0.058579180389642715 |\n",
      "\n",
      "                    Average val loss: 0.003131189150735736|\n",
      "                    Val-Mae: 0.0465785376727581\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 42\n",
      "Start training for stock: 6923\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.424041301012039e-05 | \n",
      "                    Train-Mae: 0.05699222907423973 |\n",
      "\n",
      "                    Average val loss: 0.0008805009420029819|\n",
      "                    Val-Mae: 0.02960166335105896\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 43\n",
      "Start training for stock: 1518\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.392717361450195e-05 | \n",
      "                    Train-Mae: 0.058029625564813614 |\n",
      "\n",
      "                    Average val loss: 0.0021555006969720125|\n",
      "                    Val-Mae: 0.04266566038131714\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 44\n",
      "Start training for stock: 8253\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.3979549556970595e-05 | \n",
      "                    Train-Mae: 0.05661502853035927 |\n",
      "\n",
      "                    Average val loss: 0.003401020308956504|\n",
      "                    Val-Mae: 0.05752931162714958\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 45\n",
      "Start training for stock: 3978\n",
      "continuos shape: (1149, 11)  categorical shape: (1149, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([121, 7])\n",
      "x_cat after embedding: torch.Size([121, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([121, 7, 1])\n",
      "x.shape after fft2: torch.Size([121, 5])\n",
      "x.shape after pooling: torch.Size([121, 2])\n",
      "x.shape after first cont layer: torch.Size([121, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([121, 7])\n",
      "x_cat.squeeze().shape: torch.Size([121, 7])\n",
      "x.shape after torch.cat: torch.Size([121, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.714270915836096e-05 | \n",
      "                    Train-Mae: 0.06361524760723114 |\n",
      "\n",
      "                    Average val loss: 0.007670680992305279|\n",
      "                    Val-Mae: 0.07247252017259598\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 46\n",
      "Start training for stock: 3222\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.681493811309338e-05 | \n",
      "                    Train-Mae: 0.07482287287712097 |\n",
      "\n",
      "                    Average val loss: 0.013930635526776314|\n",
      "                    Val-Mae: 0.09332249313592911\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 47\n",
      "Start training for stock: 2915\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.1722894422709945e-05 | \n",
      "                    Train-Mae: 0.051896631717681885 |\n",
      "\n",
      "                    Average val loss: 0.0031310939230024815|\n",
      "                    Val-Mae: 0.034019459038972855\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 48\n",
      "Start training for stock: 9048\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00036423929035663607 | \n",
      "                    Train-Mae: 0.06285244226455688 |\n",
      "\n",
      "                    Average val loss: 0.008642191998660564|\n",
      "                    Val-Mae: 0.0620294027030468\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 49\n",
      "Start training for stock: 6592\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.118957720696926e-05 | \n",
      "                    Train-Mae: 0.05301729217171669 |\n",
      "\n",
      "                    Average val loss: 0.0018308678409084678|\n",
      "                    Val-Mae: 0.042668938636779785\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 50\n",
      "Start training for stock: 8057\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00014778022654354572 | \n",
      "                    Train-Mae: 0.058691129088401794 |\n",
      "\n",
      "                    Average val loss: 0.010761158540844917|\n",
      "                    Val-Mae: 0.08766674995422363\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 51\n",
      "Start training for stock: 7240\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.694531206041574e-05 | \n",
      "                    Train-Mae: 0.05648508295416832 |\n",
      "\n",
      "                    Average val loss: 0.004107230808585882|\n",
      "                    Val-Mae: 0.057087332010269165\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 52\n",
      "Start training for stock: 1934\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.496613681316376e-05 | \n",
      "                    Train-Mae: 0.054023053497076035 |\n",
      "\n",
      "                    Average val loss: 0.0002088086330331862|\n",
      "                    Val-Mae: 0.014350849203765392\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 53\n",
      "Start training for stock: 4202\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.760584190487862e-05 | \n",
      "                    Train-Mae: 0.05612846836447716 |\n",
      "\n",
      "                    Average val loss: 0.00031496977317146957|\n",
      "                    Val-Mae: 0.01476320531219244\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 54\n",
      "Start training for stock: 2874\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.385572556406259e-05 | \n",
      "                    Train-Mae: 0.054052066057920456 |\n",
      "\n",
      "                    Average val loss: 0.008995717391371727|\n",
      "                    Val-Mae: 0.08668821305036545\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 55\n",
      "Start training for stock: 5959\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.494066514074802e-05 | \n",
      "                    Train-Mae: 0.05206778272986412 |\n",
      "\n",
      "                    Average val loss: 0.0015881191939115524|\n",
      "                    Val-Mae: 0.029538961127400398\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 56\n",
      "Start training for stock: 3800\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.479735814034939e-05 | \n",
      "                    Train-Mae: 0.054387032985687256 |\n",
      "\n",
      "                    Average val loss: 0.003403262933716178|\n",
      "                    Val-Mae: 0.050168026238679886\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 57\n",
      "Start training for stock: 9042\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.981347523629666e-05 | \n",
      "                    Train-Mae: 0.06497397273778915 |\n",
      "\n",
      "                    Average val loss: 0.008086539804935455|\n",
      "                    Val-Mae: 0.056647252291440964\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 58\n",
      "Start training for stock: 6282\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.8493160866200926e-05 | \n",
      "                    Train-Mae: 0.04671531543135643 |\n",
      "\n",
      "                    Average val loss: 0.00409168703481555|\n",
      "                    Val-Mae: 0.05070984363555908\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 59\n",
      "Start training for stock: 8068\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.4922169074416164e-05 | \n",
      "                    Train-Mae: 0.05411457642912865 |\n",
      "\n",
      "                    Average val loss: 0.01241049449890852|\n",
      "                    Val-Mae: 0.10032486915588379\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 60\n",
      "Start training for stock: 5192\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.855015642940998e-05 | \n",
      "                    Train-Mae: 0.052736006677150726 |\n",
      "\n",
      "                    Average val loss: 0.005055077373981476|\n",
      "                    Val-Mae: 0.05906733497977257\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 61\n",
      "Start training for stock: 7989\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.12588319927454e-05 | \n",
      "                    Train-Mae: 0.056613679975271225 |\n",
      "\n",
      "                    Average val loss: 0.0022127171978354454|\n",
      "                    Val-Mae: 0.037879619747400284\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 62\n",
      "Start training for stock: 6501\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00020526153966784477 | \n",
      "                    Train-Mae: 0.060026463121175766 |\n",
      "\n",
      "                    Average val loss: 0.0015517513966187835|\n",
      "                    Val-Mae: 0.033089861273765564\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 63\n",
      "Start training for stock: 7421\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.045666851103306e-05 | \n",
      "                    Train-Mae: 0.07147306948900223 |\n",
      "\n",
      "                    Average val loss: 0.01350359432399273|\n",
      "                    Val-Mae: 0.09387115389108658\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 64\n",
      "Start training for stock: 1821\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00010186514817178249 | \n",
      "                    Train-Mae: 0.05040577054023743 |\n",
      "\n",
      "                    Average val loss: 0.0002656966680660844|\n",
      "                    Val-Mae: 0.015302835963666439\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 65\n",
      "Start training for stock: 4922\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.456076633185148e-05 | \n",
      "                    Train-Mae: 0.05576096475124359 |\n",
      "\n",
      "                    Average val loss: 0.001718998420983553|\n",
      "                    Val-Mae: 0.03559425100684166\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 66\n",
      "Start training for stock: 5901\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.8518497496843335e-05 | \n",
      "                    Train-Mae: 0.05198126286268234 |\n",
      "\n",
      "                    Average val loss: 0.0016859964234754443|\n",
      "                    Val-Mae: 0.029194630682468414\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 67\n",
      "Start training for stock: 6861\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.6055493876338004e-05 | \n",
      "                    Train-Mae: 0.05059831961989403 |\n",
      "\n",
      "                    Average val loss: 0.0006164528313092887|\n",
      "                    Val-Mae: 0.02437126636505127\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 68\n",
      "Start training for stock: 3926\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.322948589920997e-05 | \n",
      "                    Train-Mae: 0.05905981734395027 |\n",
      "\n",
      "                    Average val loss: 0.01132206991314888|\n",
      "                    Val-Mae: 0.0870826318860054\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 69\n",
      "Start training for stock: 3191\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.09560264274478e-05 | \n",
      "                    Train-Mae: 0.05554505065083504 |\n",
      "\n",
      "                    Average val loss: 0.009960455819964409|\n",
      "                    Val-Mae: 0.07230672240257263\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 70\n",
      "Start training for stock: 2830\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00010430186986923218 | \n",
      "                    Train-Mae: 0.07941823452711105 |\n",
      "\n",
      "                    Average val loss: 0.00820845179259777|\n",
      "                    Val-Mae: 0.07916445285081863\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 71\n",
      "Start training for stock: 5449\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.199066199362278e-05 | \n",
      "                    Train-Mae: 0.05143994837999344 |\n",
      "\n",
      "                    Average val loss: 0.003957837820053101|\n",
      "                    Val-Mae: 0.04849264398217201\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 72\n",
      "Start training for stock: 9319\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.542130347341299e-05 | \n",
      "                    Train-Mae: 0.04748518764972687 |\n",
      "\n",
      "                    Average val loss: 0.006255946587771177|\n",
      "                    Val-Mae: 0.05401793122291565\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 73\n",
      "Start training for stock: 8600\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.898569080978632e-05 | \n",
      "                    Train-Mae: 0.05057254806160927 |\n",
      "\n",
      "                    Average val loss: 0.003656742163002491|\n",
      "                    Val-Mae: 0.04483604431152344\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 74\n",
      "Start training for stock: 3134\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.5924692638218404e-05 | \n",
      "                    Train-Mae: 0.0464007705450058 |\n",
      "\n",
      "                    Average val loss: 0.012508708983659744|\n",
      "                    Val-Mae: 0.06850901991128922\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 75\n",
      "Start training for stock: 8233\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.5326707400381566e-05 | \n",
      "                    Train-Mae: 0.04344595968723297 |\n",
      "\n",
      "                    Average val loss: 0.011220013722777367|\n",
      "                    Val-Mae: 0.07067939639091492\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 76\n",
      "Start training for stock: 2201\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.42877465300262e-05 | \n",
      "                    Train-Mae: 0.04048304259777069 |\n",
      "\n",
      "                    Average val loss: 0.0013012273702770472|\n",
      "                    Val-Mae: 0.0296785905957222\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 77\n",
      "Start training for stock: 2384\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.244940355420112e-05 | \n",
      "                    Train-Mae: 0.04796710982918739 |\n",
      "\n",
      "                    Average val loss: 0.00465756468474865|\n",
      "                    Val-Mae: 0.05175388976931572\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 78\n",
      "Start training for stock: 8928\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.4150729663670064e-05 | \n",
      "                    Train-Mae: 0.04700363799929619 |\n",
      "\n",
      "                    Average val loss: 0.004778596572577953|\n",
      "                    Val-Mae: 0.06414783746004105\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 79\n",
      "Start training for stock: 7943\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.083522293716669e-05 | \n",
      "                    Train-Mae: 0.04614581912755966 |\n",
      "\n",
      "                    Average val loss: 0.0013742231531068683|\n",
      "                    Val-Mae: 0.030735215172171593\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 80\n",
      "Start training for stock: 5946\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.7639227230101824e-05 | \n",
      "                    Train-Mae: 0.04628225415945053 |\n",
      "\n",
      "                    Average val loss: 0.0016895266016945243|\n",
      "                    Val-Mae: 0.026975907385349274\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 81\n",
      "Start training for stock: 2009\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.432515542954207e-05 | \n",
      "                    Train-Mae: 0.04410592466592789 |\n",
      "\n",
      "                    Average val loss: 0.0037598595954477787|\n",
      "                    Val-Mae: 0.04702627658843994\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 82\n",
      "Start training for stock: 3391\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.901153638958931e-05 | \n",
      "                    Train-Mae: 0.042340219020843506 |\n",
      "\n",
      "                    Average val loss: 0.009172281250357628|\n",
      "                    Val-Mae: 0.06893470138311386\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 83\n",
      "Start training for stock: 6706\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00023262230679392814 | \n",
      "                    Train-Mae: 0.04756886884570122 |\n",
      "\n",
      "                    Average val loss: 0.001983924303203821|\n",
      "                    Val-Mae: 0.04161982238292694\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 84\n",
      "Start training for stock: 8585\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.6122037563472986e-05 | \n",
      "                    Train-Mae: 0.04560525715351105 |\n",
      "\n",
      "                    Average val loss: 0.002129870466887951|\n",
      "                    Val-Mae: 0.03943774476647377\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 85\n",
      "Start training for stock: 2790\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.8068770188838243e-05 | \n",
      "                    Train-Mae: 0.04418547824025154 |\n",
      "\n",
      "                    Average val loss: 0.010746842250227928|\n",
      "                    Val-Mae: 0.08230612426996231\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 86\n",
      "Start training for stock: 8802\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.486413672566414e-05 | \n",
      "                    Train-Mae: 0.0475628562271595 |\n",
      "\n",
      "                    Average val loss: 0.004586591850966215|\n",
      "                    Val-Mae: 0.06602992117404938\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 87\n",
      "Start training for stock: 2782\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.334421198815107e-05 | \n",
      "                    Train-Mae: 0.03957686573266983 |\n",
      "\n",
      "                    Average val loss: 0.012107916176319122|\n",
      "                    Val-Mae: 0.08881136029958725\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 88\n",
      "Start training for stock: 3857\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.891081437468528e-05 | \n",
      "                    Train-Mae: 0.05216663330793381 |\n",
      "\n",
      "                    Average val loss: 0.0023537930101156235|\n",
      "                    Val-Mae: 0.04777047038078308\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 89\n",
      "Start training for stock: 3003\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.983155988156796e-05 | \n",
      "                    Train-Mae: 0.051958419382572174 |\n",
      "\n",
      "                    Average val loss: 0.004396810662001371|\n",
      "                    Val-Mae: 0.06445872783660889\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 90\n",
      "Start training for stock: 1861\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.0006103092804551124 | \n",
      "                    Train-Mae: 0.051730331033468246 |\n",
      "\n",
      "                    Average val loss: 0.0009376159287057817|\n",
      "                    Val-Mae: 0.02860056795179844\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 91\n",
      "Start training for stock: 6266\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.105359341949224e-05 | \n",
      "                    Train-Mae: 0.04965454339981079 |\n",
      "\n",
      "                    Average val loss: 0.0057004289701581|\n",
      "                    Val-Mae: 0.06104451045393944\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 92\n",
      "Start training for stock: 2229\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.417579621076584e-05 | \n",
      "                    Train-Mae: 0.04649963229894638 |\n",
      "\n",
      "                    Average val loss: 0.0028078609611839056|\n",
      "                    Val-Mae: 0.041669439524412155\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 93\n",
      "Start training for stock: 7729\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.1384226642549037e-05 | \n",
      "                    Train-Mae: 0.045713797211647034 |\n",
      "\n",
      "                    Average val loss: 0.009244311600923538|\n",
      "                    Val-Mae: 0.06234164163470268\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 94\n",
      "Start training for stock: 8130\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.819757606834173e-05 | \n",
      "                    Train-Mae: 0.04613298550248146 |\n",
      "\n",
      "                    Average val loss: 0.006976533215492964|\n",
      "                    Val-Mae: 0.07286161929368973\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 95\n",
      "Start training for stock: 9997\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.96883487701416e-05 | \n",
      "                    Train-Mae: 0.04874332621693611 |\n",
      "\n",
      "                    Average val loss: 0.007744567468762398|\n",
      "                    Val-Mae: 0.058877091854810715\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 96\n",
      "Start training for stock: 9722\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.690272621810436e-05 | \n",
      "                    Train-Mae: 0.053798288106918335 |\n",
      "\n",
      "                    Average val loss: 0.004645015578716993|\n",
      "                    Val-Mae: 0.06522241979837418\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 97\n",
      "Start training for stock: 1942\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.799702739343047e-05 | \n",
      "                    Train-Mae: 0.04249339923262596 |\n",
      "\n",
      "                    Average val loss: 0.00044614088255912066|\n",
      "                    Val-Mae: 0.013269404880702496\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 98\n",
      "Start training for stock: 9424\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.647557951509953e-05 | \n",
      "                    Train-Mae: 0.048640962690114975 |\n",
      "\n",
      "                    Average val loss: 0.008611051365733147|\n",
      "                    Val-Mae: 0.06396356225013733\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 99\n",
      "Start training for stock: 2792\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.7071575168520212e-05 | \n",
      "                    Train-Mae: 0.04253195598721504 |\n",
      "\n",
      "                    Average val loss: 0.00821775197982788|\n",
      "                    Val-Mae: 0.05997229740023613\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 100\n",
      "Start training for stock: 9936\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.0497054792940615e-05 | \n",
      "                    Train-Mae: 0.03531716763973236 |\n",
      "\n",
      "                    Average val loss: 0.005760402884334326|\n",
      "                    Val-Mae: 0.04844322428107262\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 101\n",
      "Start training for stock: 7011\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.980679601430893e-05 | \n",
      "                    Train-Mae: 0.03898081183433533 |\n",
      "\n",
      "                    Average val loss: 0.003016593400388956|\n",
      "                    Val-Mae: 0.033836621791124344\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 102\n",
      "Start training for stock: 4045\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.620833322405815e-05 | \n",
      "                    Train-Mae: 0.045515723526477814 |\n",
      "\n",
      "                    Average val loss: 0.0004050728166475892|\n",
      "                    Val-Mae: 0.018003927543759346\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 103\n",
      "Start training for stock: 8008\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.433642977848649e-05 | \n",
      "                    Train-Mae: 0.039199259132146835 |\n",
      "\n",
      "                    Average val loss: 0.006032008212059736|\n",
      "                    Val-Mae: 0.05626251921057701\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 104\n",
      "Start training for stock: 1803\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.4700853284448385e-05 | \n",
      "                    Train-Mae: 0.04088756814599037 |\n",
      "\n",
      "                    Average val loss: 0.00034686888102442026|\n",
      "                    Val-Mae: 0.013630345463752747\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 105\n",
      "Start training for stock: 5698\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.8148326352238656e-05 | \n",
      "                    Train-Mae: 0.05748429894447327 |\n",
      "\n",
      "                    Average val loss: 0.008082924410700798|\n",
      "                    Val-Mae: 0.07408332079648972\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 106\n",
      "Start training for stock: 6572\n",
      "continuos shape: (900, 11)  categorical shape: (900, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([383, 7])\n",
      "x_cat after embedding: torch.Size([383, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([383, 7, 1])\n",
      "x.shape after fft2: torch.Size([383, 5])\n",
      "x.shape after pooling: torch.Size([383, 2])\n",
      "x.shape after first cont layer: torch.Size([383, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([383, 7])\n",
      "x_cat.squeeze().shape: torch.Size([383, 7])\n",
      "x.shape after torch.cat: torch.Size([383, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00011075001209974289 | \n",
      "                    Train-Mae: 0.05843088775873184 |\n",
      "\n",
      "                    Average val loss: 0.005367638077586889|\n",
      "                    Val-Mae: 0.07114659994840622\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 107\n",
      "Start training for stock: 8237\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.3054229095578194e-05 | \n",
      "                    Train-Mae: 0.03982255235314369 |\n",
      "\n",
      "                    Average val loss: 0.009842452593147755|\n",
      "                    Val-Mae: 0.072151780128479\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 108\n",
      "Start training for stock: 7030\n",
      "continuos shape: (835, 11)  categorical shape: (835, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([319, 7])\n",
      "x_cat after embedding: torch.Size([319, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([319, 7, 1])\n",
      "x.shape after fft2: torch.Size([319, 5])\n",
      "x.shape after pooling: torch.Size([319, 2])\n",
      "x.shape after first cont layer: torch.Size([319, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([319, 7])\n",
      "x_cat.squeeze().shape: torch.Size([319, 7])\n",
      "x.shape after torch.cat: torch.Size([319, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.174824502319097e-05 | \n",
      "                    Train-Mae: 0.056980036199092865 |\n",
      "\n",
      "                    Average val loss: 0.007552058435976505|\n",
      "                    Val-Mae: 0.08222108334302902\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 109\n",
      "Start training for stock: 5423\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.225655760616064e-05 | \n",
      "                    Train-Mae: 0.04972253367304802 |\n",
      "\n",
      "                    Average val loss: 0.003357915673404932|\n",
      "                    Val-Mae: 0.05104783549904823\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 110\n",
      "Start training for stock: 3377\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.757060669362545e-05 | \n",
      "                    Train-Mae: 0.0633593201637268 |\n",
      "\n",
      "                    Average val loss: 0.01723559759557247|\n",
      "                    Val-Mae: 0.11063077300786972\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 111\n",
      "Start training for stock: 9233\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.676328368484974e-05 | \n",
      "                    Train-Mae: 0.056285127997398376 |\n",
      "\n",
      "                    Average val loss: 0.010346658527851105|\n",
      "                    Val-Mae: 0.08590596914291382\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 112\n",
      "Start training for stock: 6125\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.348776027560234e-05 | \n",
      "                    Train-Mae: 0.06088263541460037 |\n",
      "\n",
      "                    Average val loss: 0.012874981388449669|\n",
      "                    Val-Mae: 0.0786852017045021\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 113\n",
      "Start training for stock: 6418\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.8063433021306994e-05 | \n",
      "                    Train-Mae: 0.04343041032552719 |\n",
      "\n",
      "                    Average val loss: 0.00438897218555212|\n",
      "                    Val-Mae: 0.05733171105384827\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 114\n",
      "Start training for stock: 8041\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.709775723516941e-05 | \n",
      "                    Train-Mae: 0.041889600455760956 |\n",
      "\n",
      "                    Average val loss: 0.004710033535957336|\n",
      "                    Val-Mae: 0.05998707935214043\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 115\n",
      "Start training for stock: 4475\n",
      "continuos shape: (526, 11)  categorical shape: (526, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([10, 7])\n",
      "x_cat after embedding: torch.Size([10, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([10, 7, 1])\n",
      "x.shape after fft2: torch.Size([10, 5])\n",
      "x.shape after pooling: torch.Size([10, 2])\n",
      "x.shape after first cont layer: torch.Size([10, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([10, 7])\n",
      "x_cat.squeeze().shape: torch.Size([10, 7])\n",
      "x.shape after torch.cat: torch.Size([10, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.118892088532448e-05 | \n",
      "                    Train-Mae: 0.0947432890534401 |\n",
      "\n",
      "                    Average val loss: 0.012418799102306366|\n",
      "                    Val-Mae: 0.10715591907501221\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 116\n",
      "Start training for stock: 9708\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.229088477790356e-05 | \n",
      "                    Train-Mae: 0.06020626053214073 |\n",
      "\n",
      "                    Average val loss: 0.0030134348198771477|\n",
      "                    Val-Mae: 0.04833091422915459\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 117\n",
      "Start training for stock: 1938\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.473992925137281e-05 | \n",
      "                    Train-Mae: 0.04617924988269806 |\n",
      "\n",
      "                    Average val loss: 0.0005100801936350763|\n",
      "                    Val-Mae: 0.02151092328131199\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 118\n",
      "Start training for stock: 4559\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.203658852726221e-05 | \n",
      "                    Train-Mae: 0.037691906094551086 |\n",
      "\n",
      "                    Average val loss: 0.00034615499316714704|\n",
      "                    Val-Mae: 0.01694486476480961\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 119\n",
      "Start training for stock: 1413\n",
      "continuos shape: (1149, 11)  categorical shape: (1149, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([121, 7])\n",
      "x_cat after embedding: torch.Size([121, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([121, 7, 1])\n",
      "x.shape after fft2: torch.Size([121, 5])\n",
      "x.shape after pooling: torch.Size([121, 2])\n",
      "x.shape after first cont layer: torch.Size([121, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([121, 7])\n",
      "x_cat.squeeze().shape: torch.Size([121, 7])\n",
      "x.shape after torch.cat: torch.Size([121, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.8227038457989694e-05 | \n",
      "                    Train-Mae: 0.04236363247036934 |\n",
      "\n",
      "                    Average val loss: 0.00032266502967104316|\n",
      "                    Val-Mae: 0.015073333866894245\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 120\n",
      "Start training for stock: 5310\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.902683965861797e-05 | \n",
      "                    Train-Mae: 0.03851358965039253 |\n",
      "\n",
      "                    Average val loss: 0.0007522464147768915|\n",
      "                    Val-Mae: 0.019760001450777054\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 121\n",
      "Start training for stock: 1379\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.108845558017492e-05 | \n",
      "                    Train-Mae: 0.040966298431158066 |\n",
      "\n",
      "                    Average val loss: 0.0032296995632350445|\n",
      "                    Val-Mae: 0.03626273199915886\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 122\n",
      "Start training for stock: 6448\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.680435474961996e-05 | \n",
      "                    Train-Mae: 0.041590988636016846 |\n",
      "\n",
      "                    Average val loss: 0.0016933351289480925|\n",
      "                    Val-Mae: 0.0410793162882328\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 123\n",
      "Start training for stock: 2301\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.8005879614502194e-05 | \n",
      "                    Train-Mae: 0.04670153930783272 |\n",
      "\n",
      "                    Average val loss: 0.0014748366083949804|\n",
      "                    Val-Mae: 0.03643285855650902\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 124\n",
      "Start training for stock: 8168\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.8924924079328774e-05 | \n",
      "                    Train-Mae: 0.03533250465989113 |\n",
      "\n",
      "                    Average val loss: 0.005571315065026283|\n",
      "                    Val-Mae: 0.051311176270246506\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 125\n",
      "Start training for stock: 6503\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.180984873324633e-05 | \n",
      "                    Train-Mae: 0.04059741646051407 |\n",
      "\n",
      "                    Average val loss: 0.0016021359479054809|\n",
      "                    Val-Mae: 0.03948742151260376\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 126\n",
      "Start training for stock: 6073\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.57607752084732e-05 | \n",
      "                    Train-Mae: 0.04364176094532013 |\n",
      "\n",
      "                    Average val loss: 0.0012301616370677948|\n",
      "                    Val-Mae: 0.031876370310783386\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 127\n",
      "Start training for stock: 7482\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.110155928879976e-05 | \n",
      "                    Train-Mae: 0.04017709940671921 |\n",
      "\n",
      "                    Average val loss: 0.005160720087587833|\n",
      "                    Val-Mae: 0.06333570927381516\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 128\n",
      "Start training for stock: 4825\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.26104648411274e-05 | \n",
      "                    Train-Mae: 0.040850769728422165 |\n",
      "\n",
      "                    Average val loss: 0.002855055732652545|\n",
      "                    Val-Mae: 0.04292859509587288\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 129\n",
      "Start training for stock: 6740\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.647308029234409e-05 | \n",
      "                    Train-Mae: 0.04341300576925278 |\n",
      "\n",
      "                    Average val loss: 0.0004444991936907172|\n",
      "                    Val-Mae: 0.01284684706479311\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 130\n",
      "Start training for stock: 5011\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.0282391235232354e-05 | \n",
      "                    Train-Mae: 0.03503704443573952 |\n",
      "\n",
      "                    Average val loss: 0.0009769115131348372|\n",
      "                    Val-Mae: 0.029027990996837616\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 131\n",
      "Start training for stock: 2221\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.226013224571943e-05 | \n",
      "                    Train-Mae: 0.031211331486701965 |\n",
      "\n",
      "                    Average val loss: 0.0008572009392082691|\n",
      "                    Val-Mae: 0.025867590680718422\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 132\n",
      "Start training for stock: 2702\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.7056086799129845e-05 | \n",
      "                    Train-Mae: 0.03332649543881416 |\n",
      "\n",
      "                    Average val loss: 0.0070479027926921844|\n",
      "                    Val-Mae: 0.06225934624671936\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 133\n",
      "Start training for stock: 2579\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.8464659117162227e-05 | \n",
      "                    Train-Mae: 0.04092017561197281 |\n",
      "\n",
      "                    Average val loss: 0.0038304608315229416|\n",
      "                    Val-Mae: 0.045049916952848434\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 134\n",
      "Start training for stock: 9543\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.540526144206524e-05 | \n",
      "                    Train-Mae: 0.03646368905901909 |\n",
      "\n",
      "                    Average val loss: 0.0014272582484409213|\n",
      "                    Val-Mae: 0.033627890050411224\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 135\n",
      "Start training for stock: 3763\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.335193429142237e-05 | \n",
      "                    Train-Mae: 0.03844231739640236 |\n",
      "\n",
      "                    Average val loss: 0.003041220363229513|\n",
      "                    Val-Mae: 0.03933544456958771\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 136\n",
      "Start training for stock: 6149\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.8316740430891515e-05 | \n",
      "                    Train-Mae: 0.046701595187187195 |\n",
      "\n",
      "                    Average val loss: 0.0048101069405674934|\n",
      "                    Val-Mae: 0.05113644897937775\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 137\n",
      "Start training for stock: 6920\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.111082293093205e-05 | \n",
      "                    Train-Mae: 0.0475752167403698 |\n",
      "\n",
      "                    Average val loss: 0.0010395348072052002|\n",
      "                    Val-Mae: 0.022376537322998047\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 138\n",
      "Start training for stock: 4767\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.100539721548557e-05 | \n",
      "                    Train-Mae: 0.04396241530776024 |\n",
      "\n",
      "                    Average val loss: 0.0017119345720857382|\n",
      "                    Val-Mae: 0.03742665797472\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 139\n",
      "Start training for stock: 2752\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.435754518955946e-05 | \n",
      "                    Train-Mae: 0.03461204469203949 |\n",
      "\n",
      "                    Average val loss: 0.005798127502202988|\n",
      "                    Val-Mae: 0.04550083354115486\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 140\n",
      "Start training for stock: 8285\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.7505543548613788e-05 | \n",
      "                    Train-Mae: 0.03777039423584938 |\n",
      "\n",
      "                    Average val loss: 0.004982118494808674|\n",
      "                    Val-Mae: 0.057038962841033936\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 141\n",
      "Start training for stock: 4847\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.483829718083143e-05 | \n",
      "                    Train-Mae: 0.0387318953871727 |\n",
      "\n",
      "                    Average val loss: 0.0026271305978298187|\n",
      "                    Val-Mae: 0.037231527268886566\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 142\n",
      "Start training for stock: 7741\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.7030555065721275e-05 | \n",
      "                    Train-Mae: 0.0382472388446331 |\n",
      "\n",
      "                    Average val loss: 0.009861285798251629|\n",
      "                    Val-Mae: 0.06790850311517715\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 143\n",
      "Start training for stock: 4182\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.7116024177521464e-05 | \n",
      "                    Train-Mae: 0.04046115279197693 |\n",
      "\n",
      "                    Average val loss: 0.0005257910815998912|\n",
      "                    Val-Mae: 0.020031308755278587\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 144\n",
      "Start training for stock: 1950\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.663725521415472e-05 | \n",
      "                    Train-Mae: 0.03905084356665611 |\n",
      "\n",
      "                    Average val loss: 0.0003477739810477942|\n",
      "                    Val-Mae: 0.015753913670778275\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 145\n",
      "Start training for stock: 4433\n",
      "continuos shape: (672, 11)  categorical shape: (672, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([156, 7])\n",
      "x_cat after embedding: torch.Size([156, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([156, 7, 1])\n",
      "x.shape after fft2: torch.Size([156, 5])\n",
      "x.shape after pooling: torch.Size([156, 2])\n",
      "x.shape after first cont layer: torch.Size([156, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([156, 7])\n",
      "x_cat.squeeze().shape: torch.Size([156, 7])\n",
      "x.shape after torch.cat: torch.Size([156, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.596398677676916e-05 | \n",
      "                    Train-Mae: 0.047179095447063446 |\n",
      "\n",
      "                    Average val loss: 0.001583604607731104|\n",
      "                    Val-Mae: 0.03472738713026047\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 146\n",
      "Start training for stock: 3086\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.8551332177594305e-05 | \n",
      "                    Train-Mae: 0.03782259300351143 |\n",
      "\n",
      "                    Average val loss: 0.00837769079953432|\n",
      "                    Val-Mae: 0.06180094555020332\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 147\n",
      "Start training for stock: 2130\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.1103027760982514e-05 | \n",
      "                    Train-Mae: 0.048413731157779694 |\n",
      "\n",
      "                    Average val loss: 0.002578744199126959|\n",
      "                    Val-Mae: 0.041679974645376205\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 148\n",
      "Start training for stock: 9039\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.485839417204261e-05 | \n",
      "                    Train-Mae: 0.04090575501322746 |\n",
      "\n",
      "                    Average val loss: 0.0029965308494865894|\n",
      "                    Val-Mae: 0.04425157979130745\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 149\n",
      "Start training for stock: 9509\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.1693789642304184e-05 | \n",
      "                    Train-Mae: 0.0355304554104805 |\n",
      "\n",
      "                    Average val loss: 0.0014036436332389712|\n",
      "                    Val-Mae: 0.03216850385069847\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 150\n",
      "Start training for stock: 3659\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.63803980872035e-05 | \n",
      "                    Train-Mae: 0.04222569614648819 |\n",
      "\n",
      "                    Average val loss: 0.0010684030130505562|\n",
      "                    Val-Mae: 0.0212706271559\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 151\n",
      "Start training for stock: 9621\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.059589304029941e-05 | \n",
      "                    Train-Mae: 0.041814714670181274 |\n",
      "\n",
      "                    Average val loss: 0.000874421326443553|\n",
      "                    Val-Mae: 0.02450026012957096\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 152\n",
      "Start training for stock: 3964\n",
      "continuos shape: (1144, 11)  categorical shape: (1144, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([116, 7])\n",
      "x_cat after embedding: torch.Size([116, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([116, 7, 1])\n",
      "x.shape after fft2: torch.Size([116, 5])\n",
      "x.shape after pooling: torch.Size([116, 2])\n",
      "x.shape after first cont layer: torch.Size([116, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([116, 7])\n",
      "x_cat.squeeze().shape: torch.Size([116, 7])\n",
      "x.shape after torch.cat: torch.Size([116, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.254594979807735e-05 | \n",
      "                    Train-Mae: 0.04680640250444412 |\n",
      "\n",
      "                    Average val loss: 0.002685227431356907|\n",
      "                    Val-Mae: 0.045562196522951126\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 153\n",
      "Start training for stock: 8338\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.472505370154977e-05 | \n",
      "                    Train-Mae: 0.04044973850250244 |\n",
      "\n",
      "                    Average val loss: 0.002026683185249567|\n",
      "                    Val-Mae: 0.0348660834133625\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 154\n",
      "Start training for stock: 1775\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.72446933761239e-05 | \n",
      "                    Train-Mae: 0.046430423855781555 |\n",
      "\n",
      "                    Average val loss: 0.0022734273225069046|\n",
      "                    Val-Mae: 0.04533904418349266\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 155\n",
      "Start training for stock: 4620\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.2366858795285226e-05 | \n",
      "                    Train-Mae: 0.03685277700424194 |\n",
      "\n",
      "                    Average val loss: 0.0004471270367503166|\n",
      "                    Val-Mae: 0.020263755694031715\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 156\n",
      "Start training for stock: 2760\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.5916220620274545e-05 | \n",
      "                    Train-Mae: 0.042347609996795654 |\n",
      "\n",
      "                    Average val loss: 0.00553955277428031|\n",
      "                    Val-Mae: 0.06112566590309143\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 157\n",
      "Start training for stock: 5707\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00020512711256742477 | \n",
      "                    Train-Mae: 0.04494818300008774 |\n",
      "\n",
      "                    Average val loss: 0.005880173295736313|\n",
      "                    Val-Mae: 0.06895042955875397\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 158\n",
      "Start training for stock: 4216\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.463217549026013e-05 | \n",
      "                    Train-Mae: 0.03839295357465744 |\n",
      "\n",
      "                    Average val loss: 0.0002924435830209404|\n",
      "                    Val-Mae: 0.015931906178593636\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 159\n",
      "Start training for stock: 7211\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.3205201141536236e-05 | \n",
      "                    Train-Mae: 0.03846580162644386 |\n",
      "\n",
      "                    Average val loss: 0.004362509120255709|\n",
      "                    Val-Mae: 0.05696525797247887\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 160\n",
      "Start training for stock: 9537\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.113960709422827e-05 | \n",
      "                    Train-Mae: 0.05239369347691536 |\n",
      "\n",
      "                    Average val loss: 0.0033228180836886168|\n",
      "                    Val-Mae: 0.049847155809402466\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 161\n",
      "Start training for stock: 9107\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.353155545890331e-05 | \n",
      "                    Train-Mae: 0.0449741929769516 |\n",
      "\n",
      "                    Average val loss: 0.005248371511697769|\n",
      "                    Val-Mae: 0.06285835057497025\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 162\n",
      "Start training for stock: 6539\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.515510078519583e-05 | \n",
      "                    Train-Mae: 0.04049931839108467 |\n",
      "\n",
      "                    Average val loss: 0.0030091721564531326|\n",
      "                    Val-Mae: 0.045109111815690994\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 163\n",
      "Start training for stock: 7254\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.7706632390618325e-05 | \n",
      "                    Train-Mae: 0.046366460621356964 |\n",
      "\n",
      "                    Average val loss: 0.00419040210545063|\n",
      "                    Val-Mae: 0.04437451437115669\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 164\n",
      "Start training for stock: 9692\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.51555971428752e-05 | \n",
      "                    Train-Mae: 0.03391426429152489 |\n",
      "\n",
      "                    Average val loss: 0.00242356164380908|\n",
      "                    Val-Mae: 0.03802550956606865\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 165\n",
      "Start training for stock: 5727\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.2888839486986397e-05 | \n",
      "                    Train-Mae: 0.0376691035926342 |\n",
      "\n",
      "                    Average val loss: 0.005516937933862209|\n",
      "                    Val-Mae: 0.07159054279327393\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 166\n",
      "Start training for stock: 8291\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4340546913444996e-05 | \n",
      "                    Train-Mae: 0.031075045466423035 |\n",
      "\n",
      "                    Average val loss: 0.004054977558553219|\n",
      "                    Val-Mae: 0.04145653918385506\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 167\n",
      "Start training for stock: 9435\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.6394024025648833e-05 | \n",
      "                    Train-Mae: 0.03284670040011406 |\n",
      "\n",
      "                    Average val loss: 0.0021100479643791914|\n",
      "                    Val-Mae: 0.04029236361384392\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 168\n",
      "Start training for stock: 4927\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.7073233872652054e-05 | \n",
      "                    Train-Mae: 0.037965547293424606 |\n",
      "\n",
      "                    Average val loss: 0.0007613496854901314|\n",
      "                    Val-Mae: 0.018450649455189705\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 169\n",
      "Start training for stock: 6420\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.056090161204338e-05 | \n",
      "                    Train-Mae: 0.032133929431438446 |\n",
      "\n",
      "                    Average val loss: 0.0008088094182312489|\n",
      "                    Val-Mae: 0.021609246730804443\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 170\n",
      "Start training for stock: 3990\n",
      "continuos shape: (1039, 11)  categorical shape: (1039, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([10, 7])\n",
      "x_cat after embedding: torch.Size([10, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([10, 7, 1])\n",
      "x.shape after fft2: torch.Size([10, 5])\n",
      "x.shape after pooling: torch.Size([10, 2])\n",
      "x.shape after first cont layer: torch.Size([10, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([10, 7])\n",
      "x_cat.squeeze().shape: torch.Size([10, 7])\n",
      "x.shape after torch.cat: torch.Size([10, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.757276736199856e-05 | \n",
      "                    Train-Mae: 0.05521000549197197 |\n",
      "\n",
      "                    Average val loss: 0.0013654798967763782|\n",
      "                    Val-Mae: 0.029529251158237457\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 171\n",
      "Start training for stock: 7599\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.2205793540924785e-05 | \n",
      "                    Train-Mae: 0.036647506058216095 |\n",
      "\n",
      "                    Average val loss: 0.003577515948563814|\n",
      "                    Val-Mae: 0.04546159505844116\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 172\n",
      "Start training for stock: 2157\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.062277428805828e-05 | \n",
      "                    Train-Mae: 0.039497967809438705 |\n",
      "\n",
      "                    Average val loss: 0.003802001243457198|\n",
      "                    Val-Mae: 0.059580493718385696\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 173\n",
      "Start training for stock: 2413\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.124504815787077e-05 | \n",
      "                    Train-Mae: 0.03689590096473694 |\n",
      "\n",
      "                    Average val loss: 0.0011421607341617346|\n",
      "                    Val-Mae: 0.025593997910618782\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 174\n",
      "Start training for stock: 3966\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.130044184625149e-05 | \n",
      "                    Train-Mae: 0.04259292408823967 |\n",
      "\n",
      "                    Average val loss: 0.002736280206590891|\n",
      "                    Val-Mae: 0.04611757770180702\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 175\n",
      "Start training for stock: 9783\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.434350550174713e-05 | \n",
      "                    Train-Mae: 0.03646101430058479 |\n",
      "\n",
      "                    Average val loss: 0.000905947177670896|\n",
      "                    Val-Mae: 0.029341233894228935\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 176\n",
      "Start training for stock: 4552\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.8179686050862074e-05 | \n",
      "                    Train-Mae: 0.034146882593631744 |\n",
      "\n",
      "                    Average val loss: 0.00010853433923330158|\n",
      "                    Val-Mae: 0.00898537877947092\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 177\n",
      "Start training for stock: 3157\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.959330566227436e-05 | \n",
      "                    Train-Mae: 0.03472183272242546 |\n",
      "\n",
      "                    Average val loss: 0.002467472106218338|\n",
      "                    Val-Mae: 0.03799128159880638\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 178\n",
      "Start training for stock: 9044\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.1362327970564365e-05 | \n",
      "                    Train-Mae: 0.03452877327799797 |\n",
      "\n",
      "                    Average val loss: 0.0034808404743671417|\n",
      "                    Val-Mae: 0.04454526677727699\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 179\n",
      "Start training for stock: 2475\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.0649304389953615e-05 | \n",
      "                    Train-Mae: 0.036763664335012436 |\n",
      "\n",
      "                    Average val loss: 0.0008462838595733047|\n",
      "                    Val-Mae: 0.028219914063811302\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 180\n",
      "Start training for stock: 7621\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2279368238523603e-05 | \n",
      "                    Train-Mae: 0.026172183454036713 |\n",
      "\n",
      "                    Average val loss: 0.005499090068042278|\n",
      "                    Val-Mae: 0.061244696378707886\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 181\n",
      "Start training for stock: 7965\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.629755297675729e-05 | \n",
      "                    Train-Mae: 0.035139188170433044 |\n",
      "\n",
      "                    Average val loss: 0.000693635898642242|\n",
      "                    Val-Mae: 0.025216108188033104\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 182\n",
      "Start training for stock: 9828\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.0560938864946365e-05 | \n",
      "                    Train-Mae: 0.030663594603538513 |\n",
      "\n",
      "                    Average val loss: 0.004064565524458885|\n",
      "                    Val-Mae: 0.04222439229488373\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 183\n",
      "Start training for stock: 2681\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.6869886312633753e-05 | \n",
      "                    Train-Mae: 0.029022524133324623 |\n",
      "\n",
      "                    Average val loss: 0.003816829062998295|\n",
      "                    Val-Mae: 0.04654155671596527\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 184\n",
      "Start training for stock: 8249\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.2873892448842524e-05 | \n",
      "                    Train-Mae: 0.037564195692539215 |\n",
      "\n",
      "                    Average val loss: 0.008330374956130981|\n",
      "                    Val-Mae: 0.07377677410840988\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 185\n",
      "Start training for stock: 7280\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.751916414126754e-05 | \n",
      "                    Train-Mae: 0.03798903524875641 |\n",
      "\n",
      "                    Average val loss: 0.0017550550401210785|\n",
      "                    Val-Mae: 0.03834027424454689\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 186\n",
      "Start training for stock: 7157\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.326920093968511e-05 | \n",
      "                    Train-Mae: 0.043452560901641846 |\n",
      "\n",
      "                    Average val loss: 0.0005597639828920364|\n",
      "                    Val-Mae: 0.023218287155032158\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 187\n",
      "Start training for stock: 5481\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.4998974986374378e-05 | \n",
      "                    Train-Mae: 0.03884755074977875 |\n",
      "\n",
      "                    Average val loss: 0.0010205793660134077|\n",
      "                    Val-Mae: 0.028308473527431488\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 188\n",
      "Start training for stock: 2669\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0368539951741696e-05 | \n",
      "                    Train-Mae: 0.023113295435905457 |\n",
      "\n",
      "                    Average val loss: 0.005209944676607847|\n",
      "                    Val-Mae: 0.062132950872182846\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 189\n",
      "Start training for stock: 9602\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.178045455366373e-05 | \n",
      "                    Train-Mae: 0.03123798593878746 |\n",
      "\n",
      "                    Average val loss: 0.002320465398952365|\n",
      "                    Val-Mae: 0.04063352197408676\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 190\n",
      "Start training for stock: 7638\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.5179646909236907e-05 | \n",
      "                    Train-Mae: 0.047481268644332886 |\n",
      "\n",
      "                    Average val loss: 0.008234000764787197|\n",
      "                    Val-Mae: 0.05766145512461662\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 191\n",
      "Start training for stock: 4548\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.821508726105094e-05 | \n",
      "                    Train-Mae: 0.03614494577050209 |\n",
      "\n",
      "                    Average val loss: 0.00022349476057570428|\n",
      "                    Val-Mae: 0.012819036841392517\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 192\n",
      "Start training for stock: 3649\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.568757325410843e-05 | \n",
      "                    Train-Mae: 0.04034719616174698 |\n",
      "\n",
      "                    Average val loss: 0.0043585291132330894|\n",
      "                    Val-Mae: 0.044321730732917786\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 193\n",
      "Start training for stock: 4097\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.6580095291137695e-05 | \n",
      "                    Train-Mae: 0.03391651064157486 |\n",
      "\n",
      "                    Average val loss: 0.0001131150929722935|\n",
      "                    Val-Mae: 0.009852729737758636\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 194\n",
      "Start training for stock: 5706\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00031578518450260163 | \n",
      "                    Train-Mae: 0.03126097097992897 |\n",
      "\n",
      "                    Average val loss: 0.004269900266081095|\n",
      "                    Val-Mae: 0.0650000348687172\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 195\n",
      "Start training for stock: 7358\n",
      "continuos shape: (234, 11)  categorical shape: (234, 7)\n",
      "x_cat.shape before embedding: torch.Size([230, 7])\n",
      "x_cat after embedding: torch.Size([230, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([230, 7, 1])\n",
      "x.shape after fft2: torch.Size([230, 5])\n",
      "x.shape after pooling: torch.Size([230, 2])\n",
      "x.shape after first cont layer: torch.Size([230, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([230, 7])\n",
      "x_cat.squeeze().shape: torch.Size([230, 7])\n",
      "x.shape after torch.cat: torch.Size([230, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.466630820184946e-05 | \n",
      "                    Train-Mae: 0.046189580112695694 |\n",
      "\n",
      "                    Average val loss: 0.0017930709291249514|\n",
      "                    Val-Mae: 0.030950019136071205\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 196\n",
      "Start training for stock: 9946\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.049832906574011e-05 | \n",
      "                    Train-Mae: 0.025667615234851837 |\n",
      "\n",
      "                    Average val loss: 0.003997171297669411|\n",
      "                    Val-Mae: 0.04941010847687721\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 197\n",
      "Start training for stock: 3361\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2571095721796155e-05 | \n",
      "                    Train-Mae: 0.026479022577404976 |\n",
      "\n",
      "                    Average val loss: 0.002586288144811988|\n",
      "                    Val-Mae: 0.035461749881505966\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 198\n",
      "Start training for stock: 6668\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.2618210427463056e-05 | \n",
      "                    Train-Mae: 0.04856301471590996 |\n",
      "\n",
      "                    Average val loss: 0.004225790500640869|\n",
      "                    Val-Mae: 0.048278044909238815\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 199\n",
      "Start training for stock: 8182\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.093581086024642e-05 | \n",
      "                    Train-Mae: 0.025860639289021492 |\n",
      "\n",
      "                    Average val loss: 0.002367435023188591|\n",
      "                    Val-Mae: 0.03552288934588432\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 200\n",
      "Start training for stock: 6272\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.58103315718472e-05 | \n",
      "                    Train-Mae: 0.03314906358718872 |\n",
      "\n",
      "                    Average val loss: 0.0010841503972187638|\n",
      "                    Val-Mae: 0.027101680636405945\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 201\n",
      "Start training for stock: 4483\n",
      "continuos shape: (480, 11)  categorical shape: (480, 7)\n",
      "x_cat.shape before embedding: torch.Size([476, 7])\n",
      "x_cat after embedding: torch.Size([476, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([476, 7, 1])\n",
      "x.shape after fft2: torch.Size([476, 5])\n",
      "x.shape after pooling: torch.Size([476, 2])\n",
      "x.shape after first cont layer: torch.Size([476, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([476, 7])\n",
      "x_cat.squeeze().shape: torch.Size([476, 7])\n",
      "x.shape after torch.cat: torch.Size([476, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.181974567472935e-05 | \n",
      "                    Train-Mae: 0.05184653773903847 |\n",
      "\n",
      "                    Average val loss: 0.0016458705067634583|\n",
      "                    Val-Mae: 0.03776192292571068\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 202\n",
      "Start training for stock: 3632\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.4721527006477116e-05 | \n",
      "                    Train-Mae: 0.032711390405893326 |\n",
      "\n",
      "                    Average val loss: 0.0072543988935649395|\n",
      "                    Val-Mae: 0.05624678358435631\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 203\n",
      "Start training for stock: 5911\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.1697292104363442e-05 | \n",
      "                    Train-Mae: 0.029426058754324913 |\n",
      "\n",
      "                    Average val loss: 0.0006271027377806604|\n",
      "                    Val-Mae: 0.016047673299908638\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 204\n",
      "Start training for stock: 7272\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.8659908091649415e-05 | \n",
      "                    Train-Mae: 0.03288339823484421 |\n",
      "\n",
      "                    Average val loss: 0.0014543848810717463|\n",
      "                    Val-Mae: 0.030569573864340782\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 205\n",
      "Start training for stock: 9031\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.438618663698435e-05 | \n",
      "                    Train-Mae: 0.033477891236543655 |\n",
      "\n",
      "                    Average val loss: 0.00304149161092937|\n",
      "                    Val-Mae: 0.04306619241833687\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 206\n",
      "Start training for stock: 8203\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4323529321700334e-05 | \n",
      "                    Train-Mae: 0.02831689827144146 |\n",
      "\n",
      "                    Average val loss: 0.003349076025187969|\n",
      "                    Val-Mae: 0.043983712792396545\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 207\n",
      "Start training for stock: 8252\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.242440426722169e-05 | \n",
      "                    Train-Mae: 0.02960817888379097 |\n",
      "\n",
      "                    Average val loss: 0.005524205043911934|\n",
      "                    Val-Mae: 0.05560925975441933\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 208\n",
      "Start training for stock: 3678\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.4051716793328526e-05 | \n",
      "                    Train-Mae: 0.035479094833135605 |\n",
      "\n",
      "                    Average val loss: 0.0018007507314905524|\n",
      "                    Val-Mae: 0.031002862378954887\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 209\n",
      "Start training for stock: 7351\n",
      "continuos shape: (351, 11)  categorical shape: (351, 7)\n",
      "x_cat.shape before embedding: torch.Size([346, 7])\n",
      "x_cat after embedding: torch.Size([346, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([346, 7, 1])\n",
      "x.shape after fft2: torch.Size([346, 5])\n",
      "x.shape after pooling: torch.Size([346, 2])\n",
      "x.shape after first cont layer: torch.Size([346, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([346, 7])\n",
      "x_cat.squeeze().shape: torch.Size([346, 7])\n",
      "x.shape after torch.cat: torch.Size([346, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.2250677943229675e-05 | \n",
      "                    Train-Mae: 0.051738254725933075 |\n",
      "\n",
      "                    Average val loss: 0.003889976069331169|\n",
      "                    Val-Mae: 0.05696016550064087\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 210\n",
      "Start training for stock: 2929\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.153415629640222e-05 | \n",
      "                    Train-Mae: 0.03801266476511955 |\n",
      "\n",
      "                    Average val loss: 0.0012598955072462559|\n",
      "                    Val-Mae: 0.03239937499165535\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 211\n",
      "Start training for stock: 4716\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.0976488012820483e-05 | \n",
      "                    Train-Mae: 0.03148910030722618 |\n",
      "\n",
      "                    Average val loss: 0.0011204436887055635|\n",
      "                    Val-Mae: 0.024107007309794426\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 212\n",
      "Start training for stock: 3046\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2928441865369678e-05 | \n",
      "                    Train-Mae: 0.025962676852941513 |\n",
      "\n",
      "                    Average val loss: 0.0044840360060334206|\n",
      "                    Val-Mae: 0.05957770720124245\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 213\n",
      "Start training for stock: 6770\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.050874987617135e-05 | \n",
      "                    Train-Mae: 0.03188270330429077 |\n",
      "\n",
      "                    Average val loss: 0.0019040347542613745|\n",
      "                    Val-Mae: 0.043523866683244705\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 214\n",
      "Start training for stock: 2594\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1946136364713311e-05 | \n",
      "                    Train-Mae: 0.02716868184506893 |\n",
      "\n",
      "                    Average val loss: 0.0015657575568184257|\n",
      "                    Val-Mae: 0.0349242202937603\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 215\n",
      "Start training for stock: 3315\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.83976290281862e-05 | \n",
      "                    Train-Mae: 0.029875081032514572 |\n",
      "\n",
      "                    Average val loss: 0.0019195759668946266|\n",
      "                    Val-Mae: 0.04269510507583618\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 216\n",
      "Start training for stock: 7575\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.6078835837543014e-05 | \n",
      "                    Train-Mae: 0.028149934485554695 |\n",
      "\n",
      "                    Average val loss: 0.0032297372817993164|\n",
      "                    Val-Mae: 0.04194388911128044\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 217\n",
      "Start training for stock: 2780\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.7949009090662003e-05 | \n",
      "                    Train-Mae: 0.044073186814785004 |\n",
      "\n",
      "                    Average val loss: 0.011064673773944378|\n",
      "                    Val-Mae: 0.08602890372276306\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 218\n",
      "Start training for stock: 7276\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.9772083032876252e-05 | \n",
      "                    Train-Mae: 0.03437700867652893 |\n",
      "\n",
      "                    Average val loss: 0.0017080698162317276|\n",
      "                    Val-Mae: 0.03898239508271217\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 219\n",
      "Start training for stock: 4490\n",
      "continuos shape: (426, 11)  categorical shape: (426, 7)\n",
      "x_cat.shape before embedding: torch.Size([422, 7])\n",
      "x_cat after embedding: torch.Size([422, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([422, 7, 1])\n",
      "x.shape after fft2: torch.Size([422, 5])\n",
      "x.shape after pooling: torch.Size([422, 2])\n",
      "x.shape after first cont layer: torch.Size([422, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([422, 7])\n",
      "x_cat.squeeze().shape: torch.Size([422, 7])\n",
      "x.shape after torch.cat: torch.Size([422, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.533854778856039e-05 | \n",
      "                    Train-Mae: 0.053972914814949036 |\n",
      "\n",
      "                    Average val loss: 0.003006518352776766|\n",
      "                    Val-Mae: 0.04683458432555199\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 220\n",
      "Start training for stock: 3443\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.239213092252612e-05 | \n",
      "                    Train-Mae: 0.029670899733901024 |\n",
      "\n",
      "                    Average val loss: 0.0011746747186407447|\n",
      "                    Val-Mae: 0.0262898039072752\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 221\n",
      "Start training for stock: 4595\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.658362828195095e-05 | \n",
      "                    Train-Mae: 0.04501406103372574 |\n",
      "\n",
      "                    Average val loss: 0.001082707429304719|\n",
      "                    Val-Mae: 0.029567666351795197\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 222\n",
      "Start training for stock: 4978\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.6403532829135656e-05 | \n",
      "                    Train-Mae: 0.03675812855362892 |\n",
      "\n",
      "                    Average val loss: 0.0009361765696667135|\n",
      "                    Val-Mae: 0.027176668867468834\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 223\n",
      "Start training for stock: 4025\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.2094732634723186e-05 | \n",
      "                    Train-Mae: 0.03419902175664902 |\n",
      "\n",
      "                    Average val loss: 0.0008431204478256404|\n",
      "                    Val-Mae: 0.02216058038175106\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 224\n",
      "Start training for stock: 4114\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.0724355708807705e-05 | \n",
      "                    Train-Mae: 0.031643737107515335 |\n",
      "\n",
      "                    Average val loss: 0.00030067565967328846|\n",
      "                    Val-Mae: 0.013465452007949352\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 225\n",
      "Start training for stock: 8381\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.9226751755923034e-05 | \n",
      "                    Train-Mae: 0.033747732639312744 |\n",
      "\n",
      "                    Average val loss: 0.002110125496983528|\n",
      "                    Val-Mae: 0.03836044296622276\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 226\n",
      "Start training for stock: 9830\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.659953035414219e-05 | \n",
      "                    Train-Mae: 0.027901219204068184 |\n",
      "\n",
      "                    Average val loss: 0.00242799730040133|\n",
      "                    Val-Mae: 0.039133790880441666\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 227\n",
      "Start training for stock: 9068\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.0919248927384616e-05 | \n",
      "                    Train-Mae: 0.03253830596804619 |\n",
      "\n",
      "                    Average val loss: 0.0026928423903882504|\n",
      "                    Val-Mae: 0.042217571288347244\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 228\n",
      "Start training for stock: 3696\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.2437641639262435e-05 | \n",
      "                    Train-Mae: 0.042608603835105896 |\n",
      "\n",
      "                    Average val loss: 0.0040567731484770775|\n",
      "                    Val-Mae: 0.05767884850502014\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 229\n",
      "Start training for stock: 9310\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.5382913406938315e-05 | \n",
      "                    Train-Mae: 0.03525745868682861 |\n",
      "\n",
      "                    Average val loss: 0.003604073077440262|\n",
      "                    Val-Mae: 0.051384925842285156\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 230\n",
      "Start training for stock: 4482\n",
      "continuos shape: (479, 11)  categorical shape: (479, 7)\n",
      "x_cat.shape before embedding: torch.Size([474, 7])\n",
      "x_cat after embedding: torch.Size([474, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([474, 7, 1])\n",
      "x.shape after fft2: torch.Size([474, 5])\n",
      "x.shape after pooling: torch.Size([474, 2])\n",
      "x.shape after first cont layer: torch.Size([474, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([474, 7])\n",
      "x_cat.squeeze().shape: torch.Size([474, 7])\n",
      "x.shape after torch.cat: torch.Size([474, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.309743504971265e-05 | \n",
      "                    Train-Mae: 0.06365442276000977 |\n",
      "\n",
      "                    Average val loss: 0.0032117515802383423|\n",
      "                    Val-Mae: 0.0554477833211422\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 231\n",
      "Start training for stock: 1799\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.298699855804443e-05 | \n",
      "                    Train-Mae: 0.03877677395939827 |\n",
      "\n",
      "                    Average val loss: 0.0004965912085026503|\n",
      "                    Val-Mae: 0.020786819979548454\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 232\n",
      "Start training for stock: 9099\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.7912077018991114e-05 | \n",
      "                    Train-Mae: 0.03412291780114174 |\n",
      "\n",
      "                    Average val loss: 0.0016229944303631783|\n",
      "                    Val-Mae: 0.028693698346614838\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 233\n",
      "Start training for stock: 9960\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.927705598063767e-05 | \n",
      "                    Train-Mae: 0.02986929938197136 |\n",
      "\n",
      "                    Average val loss: 0.0009950156090781093|\n",
      "                    Val-Mae: 0.024648413062095642\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 234\n",
      "Start training for stock: 8707\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.649028854444623e-05 | \n",
      "                    Train-Mae: 0.02972325310111046 |\n",
      "\n",
      "                    Average val loss: 0.000732920307200402|\n",
      "                    Val-Mae: 0.024874359369277954\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 235\n",
      "Start training for stock: 6630\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 0.00015568891540169717 | \n",
      "                    Train-Mae: 0.03543500602245331 |\n",
      "\n",
      "                    Average val loss: 0.0021580192260444164|\n",
      "                    Val-Mae: 0.03786556422710419\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 236\n",
      "Start training for stock: 6564\n",
      "continuos shape: (715, 11)  categorical shape: (715, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([199, 7])\n",
      "x_cat after embedding: torch.Size([199, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([199, 7, 1])\n",
      "x.shape after fft2: torch.Size([199, 5])\n",
      "x.shape after pooling: torch.Size([199, 2])\n",
      "x.shape after first cont layer: torch.Size([199, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([199, 7])\n",
      "x_cat.squeeze().shape: torch.Size([199, 7])\n",
      "x.shape after torch.cat: torch.Size([199, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.9238384924829e-05 | \n",
      "                    Train-Mae: 0.04488444700837135 |\n",
      "\n",
      "                    Average val loss: 0.00623154966160655|\n",
      "                    Val-Mae: 0.061687737703323364\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 237\n",
      "Start training for stock: 8070\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.6989074647426606e-05 | \n",
      "                    Train-Mae: 0.02944980189204216 |\n",
      "\n",
      "                    Average val loss: 0.0033231645356863737|\n",
      "                    Val-Mae: 0.04155036434531212\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 238\n",
      "Start training for stock: 4403\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.527085831388831e-05 | \n",
      "                    Train-Mae: 0.030092310160398483 |\n",
      "\n",
      "                    Average val loss: 0.000412814028095454|\n",
      "                    Val-Mae: 0.018711278215050697\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 239\n",
      "Start training for stock: 7326\n",
      "continuos shape: (774, 11)  categorical shape: (774, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([258, 7])\n",
      "x_cat after embedding: torch.Size([258, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([258, 7, 1])\n",
      "x.shape after fft2: torch.Size([258, 5])\n",
      "x.shape after pooling: torch.Size([258, 2])\n",
      "x.shape after first cont layer: torch.Size([258, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([258, 7])\n",
      "x_cat.squeeze().shape: torch.Size([258, 7])\n",
      "x.shape after torch.cat: torch.Size([258, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.837445354089141e-05 | \n",
      "                    Train-Mae: 0.03761968016624451 |\n",
      "\n",
      "                    Average val loss: 0.00018620965420268476|\n",
      "                    Val-Mae: 0.01285704318434\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 240\n",
      "Start training for stock: 3946\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.7907379660755396e-05 | \n",
      "                    Train-Mae: 0.032487865537405014 |\n",
      "\n",
      "                    Average val loss: 0.0024299114011228085|\n",
      "                    Val-Mae: 0.03688974305987358\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 241\n",
      "Start training for stock: 4251\n",
      "continuos shape: (512, 11)  categorical shape: (512, 7)\n",
      "x_cat.shape before embedding: torch.Size([508, 7])\n",
      "x_cat after embedding: torch.Size([508, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([508, 7, 1])\n",
      "x.shape after fft2: torch.Size([508, 5])\n",
      "x.shape after pooling: torch.Size([508, 2])\n",
      "x.shape after first cont layer: torch.Size([508, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([508, 7])\n",
      "x_cat.squeeze().shape: torch.Size([508, 7])\n",
      "x.shape after torch.cat: torch.Size([508, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.8847713731229306e-05 | \n",
      "                    Train-Mae: 0.049539752304553986 |\n",
      "\n",
      "                    Average val loss: 0.002100085373967886|\n",
      "                    Val-Mae: 0.04026980698108673\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 242\n",
      "Start training for stock: 6200\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.148187417536974e-05 | \n",
      "                    Train-Mae: 0.03904970735311508 |\n",
      "\n",
      "                    Average val loss: 0.002281184308230877|\n",
      "                    Val-Mae: 0.047431085258722305\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 243\n",
      "Start training for stock: 6050\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.8251935727894306e-05 | \n",
      "                    Train-Mae: 0.03369737043976784 |\n",
      "\n",
      "                    Average val loss: 0.0026701658498495817|\n",
      "                    Val-Mae: 0.039778318256139755\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 244\n",
      "Start training for stock: 4323\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.407731022685766e-05 | \n",
      "                    Train-Mae: 0.035507939755916595 |\n",
      "\n",
      "                    Average val loss: 0.003074181964620948|\n",
      "                    Val-Mae: 0.03837401419878006\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 245\n",
      "Start training for stock: 7172\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.588307950645685e-05 | \n",
      "                    Train-Mae: 0.03172706812620163 |\n",
      "\n",
      "                    Average val loss: 0.0011891870526596904|\n",
      "                    Val-Mae: 0.03435603901743889\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 246\n",
      "Start training for stock: 3694\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.3541105911135673e-05 | \n",
      "                    Train-Mae: 0.029558885842561722 |\n",
      "\n",
      "                    Average val loss: 0.002787065226584673|\n",
      "                    Val-Mae: 0.0384189747273922\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 247\n",
      "Start training for stock: 3371\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.7782484646886588e-05 | \n",
      "                    Train-Mae: 0.027682391926646233 |\n",
      "\n",
      "                    Average val loss: 0.0025711196940392256|\n",
      "                    Val-Mae: 0.04194394871592522\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 248\n",
      "Start training for stock: 5602\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4770575799047947e-05 | \n",
      "                    Train-Mae: 0.027731260284781456 |\n",
      "\n",
      "                    Average val loss: 0.0011259247548878193|\n",
      "                    Val-Mae: 0.03140581026673317\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 249\n",
      "Start training for stock: 2353\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.377475098706782e-05 | \n",
      "                    Train-Mae: 0.030549420043826103 |\n",
      "\n",
      "                    Average val loss: 0.00176949892193079|\n",
      "                    Val-Mae: 0.03997072950005531\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 250\n",
      "Start training for stock: 9983\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.735685307532548e-06 | \n",
      "                    Train-Mae: 0.027762407436966896 |\n",
      "\n",
      "                    Average val loss: 0.0026008293498307467|\n",
      "                    Val-Mae: 0.04315446689724922\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 251\n",
      "Start training for stock: 8699\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.3798530455678702e-05 | \n",
      "                    Train-Mae: 0.03557116910815239 |\n",
      "\n",
      "                    Average val loss: 0.0007452574791386724|\n",
      "                    Val-Mae: 0.023070499300956726\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 252\n",
      "Start training for stock: 8917\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.3447251403704285e-05 | \n",
      "                    Train-Mae: 0.028086354956030846 |\n",
      "\n",
      "                    Average val loss: 0.001655785134062171|\n",
      "                    Val-Mae: 0.039211075752973557\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 253\n",
      "Start training for stock: 8566\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1872079921886325e-05 | \n",
      "                    Train-Mae: 0.024001745507121086 |\n",
      "\n",
      "                    Average val loss: 0.0010639075189828873|\n",
      "                    Val-Mae: 0.03169415146112442\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 254\n",
      "Start training for stock: 8864\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.3362024910748005e-05 | \n",
      "                    Train-Mae: 0.029218360781669617 |\n",
      "\n",
      "                    Average val loss: 0.0016843726625666022|\n",
      "                    Val-Mae: 0.0407288558781147\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 255\n",
      "Start training for stock: 1885\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.8888787599280478e-05 | \n",
      "                    Train-Mae: 0.02650427259504795 |\n",
      "\n",
      "                    Average val loss: 0.0007325623882934451|\n",
      "                    Val-Mae: 0.023632409051060677\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 256\n",
      "Start training for stock: 9474\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.951779471710324e-05 | \n",
      "                    Train-Mae: 0.026854466646909714 |\n",
      "\n",
      "                    Average val loss: 0.001362397219054401|\n",
      "                    Val-Mae: 0.031258899718523026\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 257\n",
      "Start training for stock: 8283\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4879272785037755e-05 | \n",
      "                    Train-Mae: 0.02639356628060341 |\n",
      "\n",
      "                    Average val loss: 0.0022437958978116512|\n",
      "                    Val-Mae: 0.03574555739760399\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 258\n",
      "Start training for stock: 7003\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.250030800700188e-05 | \n",
      "                    Train-Mae: 0.030977405607700348 |\n",
      "\n",
      "                    Average val loss: 0.0013479532208293676|\n",
      "                    Val-Mae: 0.03028603456914425\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 259\n",
      "Start training for stock: 2659\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.837140001356602e-06 | \n",
      "                    Train-Mae: 0.02299800142645836 |\n",
      "\n",
      "                    Average val loss: 0.0023135291412472725|\n",
      "                    Val-Mae: 0.03347543999552727\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 260\n",
      "Start training for stock: 6465\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4601412694901227e-05 | \n",
      "                    Train-Mae: 0.025948233902454376 |\n",
      "\n",
      "                    Average val loss: 0.001513890689238906|\n",
      "                    Val-Mae: 0.03260093554854393\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 261\n",
      "Start training for stock: 1431\n",
      "continuos shape: (602, 11)  categorical shape: (602, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([86, 7])\n",
      "x_cat after embedding: torch.Size([86, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([86, 7, 1])\n",
      "x.shape after fft2: torch.Size([86, 5])\n",
      "x.shape after pooling: torch.Size([86, 2])\n",
      "x.shape after first cont layer: torch.Size([86, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([86, 7])\n",
      "x_cat.squeeze().shape: torch.Size([86, 7])\n",
      "x.shape after torch.cat: torch.Size([86, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.97710083052516e-05 | \n",
      "                    Train-Mae: 0.03450297191739082 |\n",
      "\n",
      "                    Average val loss: 0.001607356476597488|\n",
      "                    Val-Mae: 0.03635258600115776\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 262\n",
      "Start training for stock: 3984\n",
      "continuos shape: (1143, 11)  categorical shape: (1143, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([114, 7])\n",
      "x_cat after embedding: torch.Size([114, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([114, 7, 1])\n",
      "x.shape after fft2: torch.Size([114, 5])\n",
      "x.shape after pooling: torch.Size([114, 2])\n",
      "x.shape after first cont layer: torch.Size([114, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([114, 7])\n",
      "x_cat.squeeze().shape: torch.Size([114, 7])\n",
      "x.shape after torch.cat: torch.Size([114, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.2880174219608308e-05 | \n",
      "                    Train-Mae: 0.03414788842201233 |\n",
      "\n",
      "                    Average val loss: 0.0021929272916167974|\n",
      "                    Val-Mae: 0.04592849686741829\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 263\n",
      "Start training for stock: 6638\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.1822229027748107e-05 | \n",
      "                    Train-Mae: 0.03148169070482254 |\n",
      "\n",
      "                    Average val loss: 0.0020506016444414854|\n",
      "                    Val-Mae: 0.0370645634829998\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 264\n",
      "Start training for stock: 3919\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.1510575897991658e-05 | \n",
      "                    Train-Mae: 0.030472708866000175 |\n",
      "\n",
      "                    Average val loss: 0.0014218990691006184|\n",
      "                    Val-Mae: 0.031912799924612045\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 265\n",
      "Start training for stock: 6815\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.61968157812953e-05 | \n",
      "                    Train-Mae: 0.03003031387925148 |\n",
      "\n",
      "                    Average val loss: 0.0003841779544018209|\n",
      "                    Val-Mae: 0.017026027664542198\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 266\n",
      "Start training for stock: 6941\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.37747048959136e-05 | \n",
      "                    Train-Mae: 0.02999020367860794 |\n",
      "\n",
      "                    Average val loss: 0.0011960796546190977|\n",
      "                    Val-Mae: 0.031225090846419334\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 267\n",
      "Start training for stock: 9832\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1756645981222392e-05 | \n",
      "                    Train-Mae: 0.024330319836735725 |\n",
      "\n",
      "                    Average val loss: 0.0024380506947636604|\n",
      "                    Val-Mae: 0.03745377063751221\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 268\n",
      "Start training for stock: 8242\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0052802972495556e-05 | \n",
      "                    Train-Mae: 0.02838311716914177 |\n",
      "\n",
      "                    Average val loss: 0.004683919250965118|\n",
      "                    Val-Mae: 0.05228964611887932\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 269\n",
      "Start training for stock: 9041\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4229031512513756e-05 | \n",
      "                    Train-Mae: 0.029123837128281593 |\n",
      "\n",
      "                    Average val loss: 0.0029186317697167397|\n",
      "                    Val-Mae: 0.03898771479725838\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 270\n",
      "Start training for stock: 4185\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.7963715363293888e-05 | \n",
      "                    Train-Mae: 0.028312571346759796 |\n",
      "\n",
      "                    Average val loss: 0.00030151617829687893|\n",
      "                    Val-Mae: 0.012273762375116348\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 271\n",
      "Start training for stock: 4719\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.646653632633388e-05 | \n",
      "                    Train-Mae: 0.030926227569580078 |\n",
      "\n",
      "                    Average val loss: 0.0013995789922773838|\n",
      "                    Val-Mae: 0.023157550022006035\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 272\n",
      "Start training for stock: 5185\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.686117611825466e-05 | \n",
      "                    Train-Mae: 0.03400920331478119 |\n",
      "\n",
      "                    Average val loss: 0.002949570072814822|\n",
      "                    Val-Mae: 0.04321159049868584\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 273\n",
      "Start training for stock: 4318\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.1515041589736937e-05 | \n",
      "                    Train-Mae: 0.031785231083631516 |\n",
      "\n",
      "                    Average val loss: 0.0023345667868852615|\n",
      "                    Val-Mae: 0.03972427174448967\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 274\n",
      "Start training for stock: 4061\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.273669932037592e-05 | \n",
      "                    Train-Mae: 0.027481630444526672 |\n",
      "\n",
      "                    Average val loss: 0.001182714942842722|\n",
      "                    Val-Mae: 0.030193492770195007\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 275\n",
      "Start training for stock: 6238\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.264541806653142e-05 | \n",
      "                    Train-Mae: 0.029822343960404396 |\n",
      "\n",
      "                    Average val loss: 0.001968654803931713|\n",
      "                    Val-Mae: 0.036381982266902924\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 276\n",
      "Start training for stock: 4718\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.6485261730849744e-05 | \n",
      "                    Train-Mae: 0.026972441002726555 |\n",
      "\n",
      "                    Average val loss: 0.0013447380624711514|\n",
      "                    Val-Mae: 0.034430358558893204\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 277\n",
      "Start training for stock: 2612\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1335250455886125e-05 | \n",
      "                    Train-Mae: 0.022376649081707 |\n",
      "\n",
      "                    Average val loss: 0.0014619012363255024|\n",
      "                    Val-Mae: 0.03208674117922783\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 278\n",
      "Start training for stock: 8411\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2421738356351852e-05 | \n",
      "                    Train-Mae: 0.028886636719107628 |\n",
      "\n",
      "                    Average val loss: 0.0015064787585288286|\n",
      "                    Val-Mae: 0.0310506671667099\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 279\n",
      "Start training for stock: 6330\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.272819634526968e-05 | \n",
      "                    Train-Mae: 0.033286940306425095 |\n",
      "\n",
      "                    Average val loss: 0.0005662140320055187|\n",
      "                    Val-Mae: 0.023157870396971703\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 280\n",
      "Start training for stock: 7914\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.6251036431640386e-05 | \n",
      "                    Train-Mae: 0.023090915754437447 |\n",
      "\n",
      "                    Average val loss: 0.0017029558075591922|\n",
      "                    Val-Mae: 0.034457847476005554\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 281\n",
      "Start training for stock: 6800\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.4428456090390682e-05 | \n",
      "                    Train-Mae: 0.028400247916579247 |\n",
      "\n",
      "                    Average val loss: 0.00045816783676855266|\n",
      "                    Val-Mae: 0.019588276743888855\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 282\n",
      "Start training for stock: 7955\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1393087916076184e-05 | \n",
      "                    Train-Mae: 0.023697441443800926 |\n",
      "\n",
      "                    Average val loss: 0.002057652920484543|\n",
      "                    Val-Mae: 0.03888770565390587\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 283\n",
      "Start training for stock: 2268\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.465977152809501e-06 | \n",
      "                    Train-Mae: 0.019433937966823578 |\n",
      "\n",
      "                    Average val loss: 0.0005483865388669074|\n",
      "                    Val-Mae: 0.020404355600476265\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 284\n",
      "Start training for stock: 6235\n",
      "continuos shape: (963, 11)  categorical shape: (963, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([447, 7])\n",
      "x_cat after embedding: torch.Size([447, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([447, 7, 1])\n",
      "x.shape after fft2: torch.Size([447, 5])\n",
      "x.shape after pooling: torch.Size([447, 2])\n",
      "x.shape after first cont layer: torch.Size([447, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([447, 7])\n",
      "x_cat.squeeze().shape: torch.Size([447, 7])\n",
      "x.shape after torch.cat: torch.Size([447, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.552696667611599e-05 | \n",
      "                    Train-Mae: 0.0320102721452713 |\n",
      "\n",
      "                    Average val loss: 0.00010803576151374727|\n",
      "                    Val-Mae: 0.009573918767273426\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 285\n",
      "Start training for stock: 4554\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.9258877728134394e-05 | \n",
      "                    Train-Mae: 0.023358268663287163 |\n",
      "\n",
      "                    Average val loss: 0.0002685957297217101|\n",
      "                    Val-Mae: 0.014348916709423065\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 286\n",
      "Start training for stock: 4819\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.8332251347601413e-05 | \n",
      "                    Train-Mae: 0.02541060373187065 |\n",
      "\n",
      "                    Average val loss: 0.002316719153895974|\n",
      "                    Val-Mae: 0.04052208736538887\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 287\n",
      "Start training for stock: 4540\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1537394020706416e-05 | \n",
      "                    Train-Mae: 0.02239363268017769 |\n",
      "\n",
      "                    Average val loss: 0.00019153530593030155|\n",
      "                    Val-Mae: 0.013777785003185272\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 288\n",
      "Start training for stock: 7419\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2532246764749288e-05 | \n",
      "                    Train-Mae: 0.02614586427807808 |\n",
      "\n",
      "                    Average val loss: 0.001765746041201055|\n",
      "                    Val-Mae: 0.026773056015372276\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 289\n",
      "Start training for stock: 1871\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.48102012462914e-05 | \n",
      "                    Train-Mae: 0.02425604872405529 |\n",
      "\n",
      "                    Average val loss: 0.00034891959512606263|\n",
      "                    Val-Mae: 0.014168589375913143\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 290\n",
      "Start training for stock: 2211\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4865052653476596e-05 | \n",
      "                    Train-Mae: 0.022498687729239464 |\n",
      "\n",
      "                    Average val loss: 0.0008959257393144071|\n",
      "                    Val-Mae: 0.02636147104203701\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 291\n",
      "Start training for stock: 8336\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4116038801148534e-05 | \n",
      "                    Train-Mae: 0.029284153133630753 |\n",
      "\n",
      "                    Average val loss: 0.0018350735772401094|\n",
      "                    Val-Mae: 0.035986389964818954\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 292\n",
      "Start training for stock: 6962\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.6884992364794017e-05 | \n",
      "                    Train-Mae: 0.04619000852108002 |\n",
      "\n",
      "                    Average val loss: 0.002768890466541052|\n",
      "                    Val-Mae: 0.0486358143389225\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 293\n",
      "Start training for stock: 2484\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.1362307965755465e-05 | \n",
      "                    Train-Mae: 0.03348715975880623 |\n",
      "\n",
      "                    Average val loss: 0.0026317813899368048|\n",
      "                    Val-Mae: 0.0465497188270092\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 294\n",
      "Start training for stock: 5951\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0540958028286695e-05 | \n",
      "                    Train-Mae: 0.02618502452969551 |\n",
      "\n",
      "                    Average val loss: 0.0014826117549091578|\n",
      "                    Val-Mae: 0.026040388271212578\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 295\n",
      "Start training for stock: 4186\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.6504551749676465e-05 | \n",
      "                    Train-Mae: 0.02727349102497101 |\n",
      "\n",
      "                    Average val loss: 0.00032376672606915236|\n",
      "                    Val-Mae: 0.016769172623753548\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 296\n",
      "Start training for stock: 6141\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4586143661290408e-05 | \n",
      "                    Train-Mae: 0.027194522321224213 |\n",
      "\n",
      "                    Average val loss: 0.000496895401738584|\n",
      "                    Val-Mae: 0.016301292926073074\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 297\n",
      "Start training for stock: 4659\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.9731377251446246e-05 | \n",
      "                    Train-Mae: 0.027443815022706985 |\n",
      "\n",
      "                    Average val loss: 0.0005217052530497313|\n",
      "                    Val-Mae: 0.021444974467158318\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 298\n",
      "Start training for stock: 7893\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2323774863034487e-05 | \n",
      "                    Train-Mae: 0.024029633030295372 |\n",
      "\n",
      "                    Average val loss: 0.0009601183119229972|\n",
      "                    Val-Mae: 0.022556787356734276\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 299\n",
      "Start training for stock: 4951\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.9214211497455836e-05 | \n",
      "                    Train-Mae: 0.02431696280837059 |\n",
      "\n",
      "                    Average val loss: 0.0002901875413954258|\n",
      "                    Val-Mae: 0.015348770655691624\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 300\n",
      "Start training for stock: 7744\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.1438742987811565e-05 | \n",
      "                    Train-Mae: 0.028132131323218346 |\n",
      "\n",
      "                    Average val loss: 0.006136480253189802|\n",
      "                    Val-Mae: 0.060237545520067215\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 301\n",
      "Start training for stock: 6340\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4389619464054704e-05 | \n",
      "                    Train-Mae: 0.025167925283312798 |\n",
      "\n",
      "                    Average val loss: 0.0006868973723612726|\n",
      "                    Val-Mae: 0.018806902691721916\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 302\n",
      "Start training for stock: 7047\n",
      "continuos shape: (715, 11)  categorical shape: (715, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([198, 7])\n",
      "x_cat after embedding: torch.Size([198, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([198, 7, 1])\n",
      "x.shape after fft2: torch.Size([198, 5])\n",
      "x.shape after pooling: torch.Size([198, 2])\n",
      "x.shape after first cont layer: torch.Size([198, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([198, 7])\n",
      "x_cat.squeeze().shape: torch.Size([198, 7])\n",
      "x.shape after torch.cat: torch.Size([198, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.883812576532364e-05 | \n",
      "                    Train-Mae: 0.038762569427490234 |\n",
      "\n",
      "                    Average val loss: 0.018399395048618317|\n",
      "                    Val-Mae: 0.08346555382013321\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 303\n",
      "Start training for stock: 4534\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.729251032695175e-06 | \n",
      "                    Train-Mae: 0.02168545313179493 |\n",
      "\n",
      "                    Average val loss: 0.00041936300112865865|\n",
      "                    Val-Mae: 0.018241196870803833\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 304\n",
      "Start training for stock: 6809\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.3446375960484148e-05 | \n",
      "                    Train-Mae: 0.024855975061655045 |\n",
      "\n",
      "                    Average val loss: 0.0007326290942728519|\n",
      "                    Val-Mae: 0.02244449220597744\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 305\n",
      "Start training for stock: 9622\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.143134431913495e-05 | \n",
      "                    Train-Mae: 0.025881942361593246 |\n",
      "\n",
      "                    Average val loss: 0.0014718150487169623|\n",
      "                    Val-Mae: 0.036012325435876846\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 306\n",
      "Start training for stock: 3401\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1503241257742047e-05 | \n",
      "                    Train-Mae: 0.02249445579946041 |\n",
      "\n",
      "                    Average val loss: 0.0005456765647977591|\n",
      "                    Val-Mae: 0.02150830067694187\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 307\n",
      "Start training for stock: 2588\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.803613595664501e-05 | \n",
      "                    Train-Mae: 0.03527321666479111 |\n",
      "\n",
      "                    Average val loss: 0.0024502649903297424|\n",
      "                    Val-Mae: 0.03767337650060654\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 308\n",
      "Start training for stock: 8601\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.979503229260445e-06 | \n",
      "                    Train-Mae: 0.02307857945561409 |\n",
      "\n",
      "                    Average val loss: 0.0009897778509184718|\n",
      "                    Val-Mae: 0.03029666282236576\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 309\n",
      "Start training for stock: 8697\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0004420764744283e-05 | \n",
      "                    Train-Mae: 0.023784318938851357 |\n",
      "\n",
      "                    Average val loss: 0.0005424562841653824|\n",
      "                    Val-Mae: 0.018253514543175697\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 310\n",
      "Start training for stock: 1808\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.954465785995127e-06 | \n",
      "                    Train-Mae: 0.023288536816835403 |\n",
      "\n",
      "                    Average val loss: 0.00045667000813409686|\n",
      "                    Val-Mae: 0.0190789345651865\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 311\n",
      "Start training for stock: 6459\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.964019991457462e-06 | \n",
      "                    Train-Mae: 0.022163599729537964 |\n",
      "\n",
      "                    Average val loss: 0.001371852122247219|\n",
      "                    Val-Mae: 0.033856820315122604\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 312\n",
      "Start training for stock: 1898\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.04076499864459e-06 | \n",
      "                    Train-Mae: 0.023617830127477646 |\n",
      "\n",
      "                    Average val loss: 0.0003197555197402835|\n",
      "                    Val-Mae: 0.012168963439762592\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 313\n",
      "Start training for stock: 6315\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.8614400178194047e-05 | \n",
      "                    Train-Mae: 0.03061569668352604 |\n",
      "\n",
      "                    Average val loss: 0.001991450320929289|\n",
      "                    Val-Mae: 0.03669273480772972\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 314\n",
      "Start training for stock: 8331\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.3407659716904163e-05 | \n",
      "                    Train-Mae: 0.027352886274456978 |\n",
      "\n",
      "                    Average val loss: 0.0020358660258352757|\n",
      "                    Val-Mae: 0.03917548432946205\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 315\n",
      "Start training for stock: 6140\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2423415901139379e-05 | \n",
      "                    Train-Mae: 0.02375240996479988 |\n",
      "\n",
      "                    Average val loss: 0.0005181000451557338|\n",
      "                    Val-Mae: 0.0203044805675745\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 316\n",
      "Start training for stock: 2497\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.6315699797123672e-05 | \n",
      "                    Train-Mae: 0.028542106971144676 |\n",
      "\n",
      "                    Average val loss: 0.0011938137467950583|\n",
      "                    Val-Mae: 0.031419213861227036\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 317\n",
      "Start training for stock: 3431\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.72374483756721e-05 | \n",
      "                    Train-Mae: 0.023862093687057495 |\n",
      "\n",
      "                    Average val loss: 0.00097230653045699|\n",
      "                    Val-Mae: 0.024650894105434418\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 318\n",
      "Start training for stock: 7820\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.5021328581497073e-05 | \n",
      "                    Train-Mae: 0.02220696210861206 |\n",
      "\n",
      "                    Average val loss: 0.001228900859132409|\n",
      "                    Val-Mae: 0.027751794084906578\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 319\n",
      "Start training for stock: 8570\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.059779695235192e-06 | \n",
      "                    Train-Mae: 0.02578813210129738 |\n",
      "\n",
      "                    Average val loss: 0.0021356428042054176|\n",
      "                    Val-Mae: 0.041243914514780045\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 320\n",
      "Start training for stock: 4479\n",
      "continuos shape: (483, 11)  categorical shape: (483, 7)\n",
      "x_cat.shape before embedding: torch.Size([479, 7])\n",
      "x_cat after embedding: torch.Size([479, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([479, 7, 1])\n",
      "x.shape after fft2: torch.Size([479, 5])\n",
      "x.shape after pooling: torch.Size([479, 2])\n",
      "x.shape after first cont layer: torch.Size([479, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([479, 7])\n",
      "x_cat.squeeze().shape: torch.Size([479, 7])\n",
      "x.shape after torch.cat: torch.Size([479, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.178923623636365e-05 | \n",
      "                    Train-Mae: 0.043955013155937195 |\n",
      "\n",
      "                    Average val loss: 0.0017559989355504513|\n",
      "                    Val-Mae: 0.03605208918452263\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 321\n",
      "Start training for stock: 7856\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1522516142576933e-05 | \n",
      "                    Train-Mae: 0.0194365493953228 |\n",
      "\n",
      "                    Average val loss: 0.001378368935547769|\n",
      "                    Val-Mae: 0.02727377973496914\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 322\n",
      "Start training for stock: 1377\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1460394598543643e-05 | \n",
      "                    Train-Mae: 0.02127256616950035 |\n",
      "\n",
      "                    Average val loss: 0.001327131176367402|\n",
      "                    Val-Mae: 0.03480576351284981\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 323\n",
      "Start training for stock: 1879\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.052830833941698e-05 | \n",
      "                    Train-Mae: 0.024453140795230865 |\n",
      "\n",
      "                    Average val loss: 0.0004615702200680971|\n",
      "                    Val-Mae: 0.02138855867087841\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 324\n",
      "Start training for stock: 9020\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.084284443408251e-06 | \n",
      "                    Train-Mae: 0.025726603344082832 |\n",
      "\n",
      "                    Average val loss: 0.0034253110643476248|\n",
      "                    Val-Mae: 0.04881373047828674\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 325\n",
      "Start training for stock: 8005\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2578665046021343e-05 | \n",
      "                    Train-Mae: 0.02734559401869774 |\n",
      "\n",
      "                    Average val loss: 0.003155852435156703|\n",
      "                    Val-Mae: 0.03986606001853943\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 326\n",
      "Start training for stock: 5703\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.275301126763224e-06 | \n",
      "                    Train-Mae: 0.024075454100966454 |\n",
      "\n",
      "                    Average val loss: 0.0017436523921787739|\n",
      "                    Val-Mae: 0.04140264168381691\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 327\n",
      "Start training for stock: 7725\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.382260514423251e-05 | \n",
      "                    Train-Mae: 0.02614523656666279 |\n",
      "\n",
      "                    Average val loss: 0.0023010042496025562|\n",
      "                    Val-Mae: 0.039785291999578476\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 328\n",
      "Start training for stock: 7337\n",
      "continuos shape: (289, 11)  categorical shape: (289, 7)\n",
      "x_cat.shape before embedding: torch.Size([284, 7])\n",
      "x_cat after embedding: torch.Size([284, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([284, 7, 1])\n",
      "x.shape after fft2: torch.Size([284, 5])\n",
      "x.shape after pooling: torch.Size([284, 2])\n",
      "x.shape after first cont layer: torch.Size([284, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([284, 7])\n",
      "x_cat.squeeze().shape: torch.Size([284, 7])\n",
      "x.shape after torch.cat: torch.Size([284, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0722537990659475e-05 | \n",
      "                    Train-Mae: 0.026104021817445755 |\n",
      "\n",
      "                    Average val loss: 4.5676362788071856e-05|\n",
      "                    Val-Mae: 0.006612821016460657\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 329\n",
      "Start training for stock: 1965\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.143254805356264e-05 | \n",
      "                    Train-Mae: 0.025501342490315437 |\n",
      "\n",
      "                    Average val loss: 0.0009160467307083309|\n",
      "                    Val-Mae: 0.023846397176384926\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 330\n",
      "Start training for stock: 6856\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.303607365116477e-05 | \n",
      "                    Train-Mae: 0.024300619959831238 |\n",
      "\n",
      "                    Average val loss: 0.0009792521595954895|\n",
      "                    Val-Mae: 0.030240172520279884\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 331\n",
      "Start training for stock: 8848\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2612275313585996e-05 | \n",
      "                    Train-Mae: 0.02969277650117874 |\n",
      "\n",
      "                    Average val loss: 0.0017887604190036654|\n",
      "                    Val-Mae: 0.04096011817455292\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 332\n",
      "Start training for stock: 8358\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.988014206290245e-05 | \n",
      "                    Train-Mae: 0.02589435689151287 |\n",
      "\n",
      "                    Average val loss: 0.0015782832633703947|\n",
      "                    Val-Mae: 0.035863786935806274\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 333\n",
      "Start training for stock: 6363\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.592709782533348e-06 | \n",
      "                    Train-Mae: 0.020603278651833534 |\n",
      "\n",
      "                    Average val loss: 0.00026376405730843544|\n",
      "                    Val-Mae: 0.011272672563791275\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 334\n",
      "Start training for stock: 5105\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4837111812084914e-05 | \n",
      "                    Train-Mae: 0.024720540270209312 |\n",
      "\n",
      "                    Average val loss: 0.0018828734755516052|\n",
      "                    Val-Mae: 0.03320011869072914\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 335\n",
      "Start training for stock: 2871\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.268270805478096e-06 | \n",
      "                    Train-Mae: 0.020342763513326645 |\n",
      "\n",
      "                    Average val loss: 0.0004975027986802161|\n",
      "                    Val-Mae: 0.016733378171920776\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 336\n",
      "Start training for stock: 4921\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.600961433723569e-05 | \n",
      "                    Train-Mae: 0.02427055686712265 |\n",
      "\n",
      "                    Average val loss: 0.0001100023728213273|\n",
      "                    Val-Mae: 0.006594676524400711\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 337\n",
      "Start training for stock: 2193\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4283977216109633e-05 | \n",
      "                    Train-Mae: 0.028314026072621346 |\n",
      "\n",
      "                    Average val loss: 0.003043952863663435|\n",
      "                    Val-Mae: 0.04849003255367279\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 338\n",
      "Start training for stock: 7839\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2293850304558873e-05 | \n",
      "                    Train-Mae: 0.02271566540002823 |\n",
      "\n",
      "                    Average val loss: 0.0015699220821261406|\n",
      "                    Val-Mae: 0.02913679927587509\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 339\n",
      "Start training for stock: 2220\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.545873522758484e-06 | \n",
      "                    Train-Mae: 0.01922350376844406 |\n",
      "\n",
      "                    Average val loss: 0.0009166417876258492|\n",
      "                    Val-Mae: 0.024792419746518135\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 340\n",
      "Start training for stock: 9632\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.762956380844116e-05 | \n",
      "                    Train-Mae: 0.020589889958500862 |\n",
      "\n",
      "                    Average val loss: 0.005797781981527805|\n",
      "                    Val-Mae: 0.05742054060101509\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 341\n",
      "Start training for stock: 7483\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.310846799053252e-06 | \n",
      "                    Train-Mae: 0.02039262093603611 |\n",
      "\n",
      "                    Average val loss: 0.001124036149121821|\n",
      "                    Val-Mae: 0.023693695664405823\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 342\n",
      "Start training for stock: 4763\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.7590074567124247e-05 | \n",
      "                    Train-Mae: 0.02591954730451107 |\n",
      "\n",
      "                    Average val loss: 0.0019229534082114697|\n",
      "                    Val-Mae: 0.035446517169475555\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 343\n",
      "Start training for stock: 6569\n",
      "continuos shape: (906, 11)  categorical shape: (906, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([390, 7])\n",
      "x_cat after embedding: torch.Size([390, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([390, 7, 1])\n",
      "x.shape after fft2: torch.Size([390, 5])\n",
      "x.shape after pooling: torch.Size([390, 2])\n",
      "x.shape after first cont layer: torch.Size([390, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([390, 7])\n",
      "x_cat.squeeze().shape: torch.Size([390, 7])\n",
      "x.shape after torch.cat: torch.Size([390, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.3185645006597044e-05 | \n",
      "                    Train-Mae: 0.03175375610589981 |\n",
      "\n",
      "                    Average val loss: 0.0010485232342034578|\n",
      "                    Val-Mae: 0.02738751471042633\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 344\n",
      "Start training for stock: 4923\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1568220797926187e-05 | \n",
      "                    Train-Mae: 0.02082332968711853 |\n",
      "\n",
      "                    Average val loss: 0.0003328222665004432|\n",
      "                    Val-Mae: 0.013898911885917187\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 345\n",
      "Start training for stock: 9719\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0137120261788368e-05 | \n",
      "                    Train-Mae: 0.03304619714617729 |\n",
      "\n",
      "                    Average val loss: 0.0006247457349672914|\n",
      "                    Val-Mae: 0.022134194150567055\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 346\n",
      "Start training for stock: 8750\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.364752913825215e-06 | \n",
      "                    Train-Mae: 0.022579999640583992 |\n",
      "\n",
      "                    Average val loss: 0.0009794824291020632|\n",
      "                    Val-Mae: 0.028487706556916237\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 347\n",
      "Start training for stock: 1662\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1110026389360427e-05 | \n",
      "                    Train-Mae: 0.02430001087486744 |\n",
      "\n",
      "                    Average val loss: 0.0006485559861175716|\n",
      "                    Val-Mae: 0.02377806045114994\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 348\n",
      "Start training for stock: 7944\n",
      "continuos shape: (237, 11)  categorical shape: (237, 7)\n",
      "x_cat.shape before embedding: torch.Size([233, 7])\n",
      "x_cat after embedding: torch.Size([233, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([233, 7, 1])\n",
      "x.shape after fft2: torch.Size([233, 5])\n",
      "x.shape after pooling: torch.Size([233, 2])\n",
      "x.shape after first cont layer: torch.Size([233, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([233, 7])\n",
      "x_cat.squeeze().shape: torch.Size([233, 7])\n",
      "x.shape after torch.cat: torch.Size([233, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1381952790543437e-05 | \n",
      "                    Train-Mae: 0.02567317895591259 |\n",
      "\n",
      "                    Average val loss: 0.0007573692128062248|\n",
      "                    Val-Mae: 0.023112386465072632\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 349\n",
      "Start training for stock: 5444\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0016422020271421e-05 | \n",
      "                    Train-Mae: 0.026042785495519638 |\n",
      "\n",
      "                    Average val loss: 0.001065800548531115|\n",
      "                    Val-Mae: 0.02712877094745636\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 350\n",
      "Start training for stock: 1870\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.662566542625428e-06 | \n",
      "                    Train-Mae: 0.022575585171580315 |\n",
      "\n",
      "                    Average val loss: 0.0003443881287239492|\n",
      "                    Val-Mae: 0.01725033111870289\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 351\n",
      "Start training for stock: 9601\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1342526413500309e-05 | \n",
      "                    Train-Mae: 0.02312799170613289 |\n",
      "\n",
      "                    Average val loss: 0.0018142176559194922|\n",
      "                    Val-Mae: 0.04113525152206421\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 352\n",
      "Start training for stock: 7718\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.232868906110526e-06 | \n",
      "                    Train-Mae: 0.021377529948949814 |\n",
      "\n",
      "                    Average val loss: 0.0005598845891654491|\n",
      "                    Val-Mae: 0.022979849949479103\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 353\n",
      "Start training for stock: 4912\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0547891724854707e-05 | \n",
      "                    Train-Mae: 0.020611915737390518 |\n",
      "\n",
      "                    Average val loss: 0.0008901582332327962|\n",
      "                    Val-Mae: 0.029004983603954315\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 354\n",
      "Start training for stock: 5991\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.042128804139793e-06 | \n",
      "                    Train-Mae: 0.02351229451596737 |\n",
      "\n",
      "                    Average val loss: 0.0009816121309995651|\n",
      "                    Val-Mae: 0.029058480635285378\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 355\n",
      "Start training for stock: 6777\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.8168382123112678e-05 | \n",
      "                    Train-Mae: 0.0280689038336277 |\n",
      "\n",
      "                    Average val loss: 0.003004202153533697|\n",
      "                    Val-Mae: 0.04380081966519356\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 356\n",
      "Start training for stock: 7060\n",
      "continuos shape: (659, 11)  categorical shape: (659, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([143, 7])\n",
      "x_cat after embedding: torch.Size([143, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([143, 7, 1])\n",
      "x.shape after fft2: torch.Size([143, 5])\n",
      "x.shape after pooling: torch.Size([143, 2])\n",
      "x.shape after first cont layer: torch.Size([143, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([143, 7])\n",
      "x_cat.squeeze().shape: torch.Size([143, 7])\n",
      "x.shape after torch.cat: torch.Size([143, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.1632399186491965e-05 | \n",
      "                    Train-Mae: 0.035486336797475815 |\n",
      "\n",
      "                    Average val loss: 0.0030479254201054573|\n",
      "                    Val-Mae: 0.04402117058634758\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 357\n",
      "Start training for stock: 9728\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.898771436884998e-06 | \n",
      "                    Train-Mae: 0.020540254190564156 |\n",
      "\n",
      "                    Average val loss: 0.0006028083153069019|\n",
      "                    Val-Mae: 0.022154435515403748\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 358\n",
      "Start training for stock: 1332\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.744254639372229e-06 | \n",
      "                    Train-Mae: 0.02324524149298668 |\n",
      "\n",
      "                    Average val loss: 0.0009388438193127513|\n",
      "                    Val-Mae: 0.03020932711660862\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 359\n",
      "Start training for stock: 4771\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.8082769820466637e-05 | \n",
      "                    Train-Mae: 0.030214479193091393 |\n",
      "\n",
      "                    Average val loss: 0.005320771597325802|\n",
      "                    Val-Mae: 0.07149266451597214\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 360\n",
      "Start training for stock: 7775\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.535251788794994e-06 | \n",
      "                    Train-Mae: 0.023241864517331123 |\n",
      "\n",
      "                    Average val loss: 0.0014576746616512537|\n",
      "                    Val-Mae: 0.03071460872888565\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 361\n",
      "Start training for stock: 2117\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.895718932151794e-06 | \n",
      "                    Train-Mae: 0.0174916572868824 |\n",
      "\n",
      "                    Average val loss: 0.0003263461112510413|\n",
      "                    Val-Mae: 0.012568320147693157\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 362\n",
      "Start training for stock: 6869\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.76720475591719e-06 | \n",
      "                    Train-Mae: 0.02455238439142704 |\n",
      "\n",
      "                    Average val loss: 0.00046010309597477317|\n",
      "                    Val-Mae: 0.017453541979193687\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 363\n",
      "Start training for stock: 6364\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.741469984874129e-06 | \n",
      "                    Train-Mae: 0.020690148696303368 |\n",
      "\n",
      "                    Average val loss: 0.0004453027795534581|\n",
      "                    Val-Mae: 0.020629584789276123\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 364\n",
      "Start training for stock: 3636\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1665632482618094e-05 | \n",
      "                    Train-Mae: 0.021137038245797157 |\n",
      "\n",
      "                    Average val loss: 0.00137252954300493|\n",
      "                    Val-Mae: 0.03026256524026394\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 365\n",
      "Start training for stock: 7844\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.933393448591232e-06 | \n",
      "                    Train-Mae: 0.021189726889133453 |\n",
      "\n",
      "                    Average val loss: 0.0008641579188406467|\n",
      "                    Val-Mae: 0.022382674738764763\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 366\n",
      "Start training for stock: 1945\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.04735061340034e-06 | \n",
      "                    Train-Mae: 0.02153329737484455 |\n",
      "\n",
      "                    Average val loss: 0.0002896646619774401|\n",
      "                    Val-Mae: 0.015323826111853123\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 367\n",
      "Start training for stock: 3333\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.226619007065892e-06 | \n",
      "                    Train-Mae: 0.018803605809807777 |\n",
      "\n",
      "                    Average val loss: 0.001292153261601925|\n",
      "                    Val-Mae: 0.031200921162962914\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 368\n",
      "Start training for stock: 6845\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.988678455352784e-06 | \n",
      "                    Train-Mae: 0.02409490756690502 |\n",
      "\n",
      "                    Average val loss: 0.0005956816021353006|\n",
      "                    Val-Mae: 0.02401009202003479\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 369\n",
      "Start training for stock: 7733\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.99047707207501e-06 | \n",
      "                    Train-Mae: 0.02319990284740925 |\n",
      "\n",
      "                    Average val loss: 0.0020900131203234196|\n",
      "                    Val-Mae: 0.03429180011153221\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 370\n",
      "Start training for stock: 3104\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0108962887898088e-05 | \n",
      "                    Train-Mae: 0.022331764921545982 |\n",
      "\n",
      "                    Average val loss: 0.0004872410499956459|\n",
      "                    Val-Mae: 0.01892779767513275\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 371\n",
      "Start training for stock: 4973\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.70154409110546e-06 | \n",
      "                    Train-Mae: 0.023447129875421524 |\n",
      "\n",
      "                    Average val loss: 0.0001480177161283791|\n",
      "                    Val-Mae: 0.010087736882269382\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 372\n",
      "Start training for stock: 3097\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2452853843569755e-05 | \n",
      "                    Train-Mae: 0.025136535987257957 |\n",
      "\n",
      "                    Average val loss: 0.0024028404150158167|\n",
      "                    Val-Mae: 0.03599120303988457\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 373\n",
      "Start training for stock: 4628\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.4574479795992377e-05 | \n",
      "                    Train-Mae: 0.02499883435666561 |\n",
      "\n",
      "                    Average val loss: 0.0004766755737364292|\n",
      "                    Val-Mae: 0.020924752578139305\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 374\n",
      "Start training for stock: 7322\n",
      "continuos shape: (896, 11)  categorical shape: (896, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([380, 7])\n",
      "x_cat after embedding: torch.Size([380, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([380, 7, 1])\n",
      "x.shape after fft2: torch.Size([380, 5])\n",
      "x.shape after pooling: torch.Size([380, 2])\n",
      "x.shape after first cont layer: torch.Size([380, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([380, 7])\n",
      "x_cat.squeeze().shape: torch.Size([380, 7])\n",
      "x.shape after torch.cat: torch.Size([380, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4509117463603615e-05 | \n",
      "                    Train-Mae: 0.026226947084069252 |\n",
      "\n",
      "                    Average val loss: 0.0005547363543882966|\n",
      "                    Val-Mae: 0.02044607885181904\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 375\n",
      "Start training for stock: 8129\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.303373422473669e-06 | \n",
      "                    Train-Mae: 0.018437102437019348 |\n",
      "\n",
      "                    Average val loss: 0.0008585594478063285|\n",
      "                    Val-Mae: 0.02096826769411564\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 376\n",
      "Start training for stock: 3708\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.093353874050081e-06 | \n",
      "                    Train-Mae: 0.024423029273748398 |\n",
      "\n",
      "                    Average val loss: 0.0007409037789329886|\n",
      "                    Val-Mae: 0.026200704276561737\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 377\n",
      "Start training for stock: 3561\n",
      "continuos shape: (1150, 11)  categorical shape: (1150, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([121, 7])\n",
      "x_cat after embedding: torch.Size([121, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([121, 7, 1])\n",
      "x.shape after fft2: torch.Size([121, 5])\n",
      "x.shape after pooling: torch.Size([121, 2])\n",
      "x.shape after first cont layer: torch.Size([121, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([121, 7])\n",
      "x_cat.squeeze().shape: torch.Size([121, 7])\n",
      "x.shape after torch.cat: torch.Size([121, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.7176143592223526e-05 | \n",
      "                    Train-Mae: 0.01985767111182213 |\n",
      "\n",
      "                    Average val loss: 0.0019504628144204617|\n",
      "                    Val-Mae: 0.04029763117432594\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 378\n",
      "Start training for stock: 6481\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.51136345975101e-06 | \n",
      "                    Train-Mae: 0.025312097743153572 |\n",
      "\n",
      "                    Average val loss: 0.0013781603192910552|\n",
      "                    Val-Mae: 0.034130048006772995\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 379\n",
      "Start training for stock: 2153\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.398568654432892e-06 | \n",
      "                    Train-Mae: 0.022512273862957954 |\n",
      "\n",
      "                    Average val loss: 0.00038736796705052257|\n",
      "                    Val-Mae: 0.016726266592741013\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 380\n",
      "Start training for stock: 5463\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.1659759376198055e-06 | \n",
      "                    Train-Mae: 0.019972948357462883 |\n",
      "\n",
      "                    Average val loss: 0.0007348563522100449|\n",
      "                    Val-Mae: 0.026910027489066124\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 381\n",
      "Start training for stock: 3663\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.3102399427443743e-05 | \n",
      "                    Train-Mae: 0.043065182864665985 |\n",
      "\n",
      "                    Average val loss: 0.0034346957691013813|\n",
      "                    Val-Mae: 0.05237298831343651\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 382\n",
      "Start training for stock: 1822\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0397379519417882e-05 | \n",
      "                    Train-Mae: 0.018725164234638214 |\n",
      "\n",
      "                    Average val loss: 0.00013059511547908187|\n",
      "                    Val-Mae: 0.01104447990655899\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 383\n",
      "Start training for stock: 6804\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.3480724301189183e-05 | \n",
      "                    Train-Mae: 0.022070182487368584 |\n",
      "\n",
      "                    Average val loss: 0.0011237453436478972|\n",
      "                    Val-Mae: 0.0301241222769022\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 384\n",
      "Start training for stock: 5344\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.502068480476737e-05 | \n",
      "                    Train-Mae: 0.021463904529809952 |\n",
      "\n",
      "                    Average val loss: 0.0004650456830859184|\n",
      "                    Val-Mae: 0.015520977787673473\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 385\n",
      "Start training for stock: 7451\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.785323446616531e-06 | \n",
      "                    Train-Mae: 0.01785608008503914 |\n",
      "\n",
      "                    Average val loss: 0.0007020875345915556|\n",
      "                    Val-Mae: 0.019308606162667274\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 386\n",
      "Start training for stock: 6981\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.838037610985338e-06 | \n",
      "                    Train-Mae: 0.02075766772031784 |\n",
      "\n",
      "                    Average val loss: 0.0007958388305269182|\n",
      "                    Val-Mae: 0.02568402886390686\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 387\n",
      "Start training for stock: 3922\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.3180542048066854e-05 | \n",
      "                    Train-Mae: 0.026985105127096176 |\n",
      "\n",
      "                    Average val loss: 0.0014651301316916943|\n",
      "                    Val-Mae: 0.034914057701826096\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 388\n",
      "Start training for stock: 7414\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.762290722690523e-06 | \n",
      "                    Train-Mae: 0.018442604690790176 |\n",
      "\n",
      "                    Average val loss: 0.0005637663416564465|\n",
      "                    Val-Mae: 0.01577589474618435\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 389\n",
      "Start training for stock: 8897\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.755167316645384e-06 | \n",
      "                    Train-Mae: 0.018963800743222237 |\n",
      "\n",
      "                    Average val loss: 0.00134373945184052|\n",
      "                    Val-Mae: 0.036539364606142044\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 390\n",
      "Start training for stock: 2975\n",
      "continuos shape: (613, 11)  categorical shape: (613, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([97, 7])\n",
      "x_cat after embedding: torch.Size([97, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([97, 7, 1])\n",
      "x.shape after fft2: torch.Size([97, 5])\n",
      "x.shape after pooling: torch.Size([97, 2])\n",
      "x.shape after first cont layer: torch.Size([97, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([97, 7])\n",
      "x_cat.squeeze().shape: torch.Size([97, 7])\n",
      "x.shape after torch.cat: torch.Size([97, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.3166454154998064e-05 | \n",
      "                    Train-Mae: 0.02177492342889309 |\n",
      "\n",
      "                    Average val loss: 0.0007356225978583097|\n",
      "                    Val-Mae: 0.02108200453221798\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 391\n",
      "Start training for stock: 1860\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.831511855125427e-06 | \n",
      "                    Train-Mae: 0.021208535879850388 |\n",
      "\n",
      "                    Average val loss: 0.00045005069114267826|\n",
      "                    Val-Mae: 0.020766329020261765\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 392\n",
      "Start training for stock: 9436\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.3690711930394173e-05 | \n",
      "                    Train-Mae: 0.02198166586458683 |\n",
      "\n",
      "                    Average val loss: 0.0003876239061355591|\n",
      "                    Val-Mae: 0.01910061575472355\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 393\n",
      "Start training for stock: 7267\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.593126406893134e-06 | \n",
      "                    Train-Mae: 0.020876722410321236 |\n",
      "\n",
      "                    Average val loss: 0.0005998379783704877|\n",
      "                    Val-Mae: 0.02227845974266529\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 394\n",
      "Start training for stock: 7261\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.181432352401316e-06 | \n",
      "                    Train-Mae: 0.024270249530673027 |\n",
      "\n",
      "                    Average val loss: 0.0013576069613918662|\n",
      "                    Val-Mae: 0.03292200341820717\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 395\n",
      "Start training for stock: 4443\n",
      "continuos shape: (601, 11)  categorical shape: (601, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([85, 7])\n",
      "x_cat after embedding: torch.Size([85, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([85, 7, 1])\n",
      "x.shape after fft2: torch.Size([85, 5])\n",
      "x.shape after pooling: torch.Size([85, 2])\n",
      "x.shape after first cont layer: torch.Size([85, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([85, 7])\n",
      "x_cat.squeeze().shape: torch.Size([85, 7])\n",
      "x.shape after torch.cat: torch.Size([85, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.1441886201500892e-05 | \n",
      "                    Train-Mae: 0.040579743683338165 |\n",
      "\n",
      "                    Average val loss: 0.0017839305801317096|\n",
      "                    Val-Mae: 0.038229599595069885\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 396\n",
      "Start training for stock: 3221\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1794610181823372e-05 | \n",
      "                    Train-Mae: 0.023730726912617683 |\n",
      "\n",
      "                    Average val loss: 0.002177862450480461|\n",
      "                    Val-Mae: 0.0389755554497242\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 397\n",
      "Start training for stock: 9104\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.2851134203374386e-05 | \n",
      "                    Train-Mae: 0.032130878418684006 |\n",
      "\n",
      "                    Average val loss: 0.0005296397721394897|\n",
      "                    Val-Mae: 0.017544731497764587\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 398\n",
      "Start training for stock: 8860\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.74440641887486e-06 | \n",
      "                    Train-Mae: 0.019996769726276398 |\n",
      "\n",
      "                    Average val loss: 0.0015154741704463959|\n",
      "                    Val-Mae: 0.037940576672554016\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 399\n",
      "Start training for stock: 4113\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.8625403065234422e-05 | \n",
      "                    Train-Mae: 0.04518124833703041 |\n",
      "\n",
      "                    Average val loss: 0.0010042707435786724|\n",
      "                    Val-Mae: 0.021226180717349052\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 400\n",
      "Start training for stock: 4205\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.854075033217669e-06 | \n",
      "                    Train-Mae: 0.022213341668248177 |\n",
      "\n",
      "                    Average val loss: 0.0009085693745873868|\n",
      "                    Val-Mae: 0.02671056054532528\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 401\n",
      "Start training for stock: 2294\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.071235937066376e-06 | \n",
      "                    Train-Mae: 0.01622576266527176 |\n",
      "\n",
      "                    Average val loss: 0.0005582262529060245|\n",
      "                    Val-Mae: 0.020978465676307678\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 402\n",
      "Start training for stock: 9279\n",
      "continuos shape: (759, 11)  categorical shape: (759, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([243, 7])\n",
      "x_cat after embedding: torch.Size([243, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([243, 7, 1])\n",
      "x.shape after fft2: torch.Size([243, 5])\n",
      "x.shape after pooling: torch.Size([243, 2])\n",
      "x.shape after first cont layer: torch.Size([243, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([243, 7])\n",
      "x_cat.squeeze().shape: torch.Size([243, 7])\n",
      "x.shape after torch.cat: torch.Size([243, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.3232523817569016e-05 | \n",
      "                    Train-Mae: 0.025065172463655472 |\n",
      "\n",
      "                    Average val loss: 0.0013063631486147642|\n",
      "                    Val-Mae: 0.028562160208821297\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 403\n",
      "Start training for stock: 3087\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.323286120779813e-06 | \n",
      "                    Train-Mae: 0.018312957137823105 |\n",
      "\n",
      "                    Average val loss: 0.0014338481705635786|\n",
      "                    Val-Mae: 0.033866554498672485\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 404\n",
      "Start training for stock: 4611\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.742217323742807e-06 | \n",
      "                    Train-Mae: 0.018535302951931953 |\n",
      "\n",
      "                    Average val loss: 0.0006261194939725101|\n",
      "                    Val-Mae: 0.01710893213748932\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 405\n",
      "Start training for stock: 4027\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.67726946529001e-05 | \n",
      "                    Train-Mae: 0.022880852222442627 |\n",
      "\n",
      "                    Average val loss: 0.00026021047960966825|\n",
      "                    Val-Mae: 0.011735389940440655\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 406\n",
      "Start training for stock: 3854\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.9384619081392883e-05 | \n",
      "                    Train-Mae: 0.02192346192896366 |\n",
      "\n",
      "                    Average val loss: 0.0032953235786408186|\n",
      "                    Val-Mae: 0.04747169092297554\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 407\n",
      "Start training for stock: 2751\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.93429620191455e-06 | \n",
      "                    Train-Mae: 0.01902720332145691 |\n",
      "\n",
      "                    Average val loss: 0.0005926681915298104|\n",
      "                    Val-Mae: 0.02028963342308998\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 408\n",
      "Start training for stock: 7911\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.161108708009124e-06 | \n",
      "                    Train-Mae: 0.018237780779600143 |\n",
      "\n",
      "                    Average val loss: 0.0012783220736309886|\n",
      "                    Val-Mae: 0.02891053818166256\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 409\n",
      "Start training for stock: 7705\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.9077984616160394e-05 | \n",
      "                    Train-Mae: 0.02495151199400425 |\n",
      "\n",
      "                    Average val loss: 0.0041245995089411736|\n",
      "                    Val-Mae: 0.05772748216986656\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 410\n",
      "Start training for stock: 2429\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.166490837931633e-05 | \n",
      "                    Train-Mae: 0.02337736450135708 |\n",
      "\n",
      "                    Average val loss: 0.0013989171711727977|\n",
      "                    Val-Mae: 0.0336158461868763\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 411\n",
      "Start training for stock: 9364\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.334137240424752e-06 | \n",
      "                    Train-Mae: 0.01984141208231449 |\n",
      "\n",
      "                    Average val loss: 0.0008352383738383651|\n",
      "                    Val-Mae: 0.024177828803658485\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 412\n",
      "Start training for stock: 4344\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.897315261885524e-05 | \n",
      "                    Train-Mae: 0.02190382592380047 |\n",
      "\n",
      "                    Average val loss: 0.004659369587898254|\n",
      "                    Val-Mae: 0.060655880719423294\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 413\n",
      "Start training for stock: 7868\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.529654053039848e-06 | \n",
      "                    Train-Mae: 0.02803134359419346 |\n",
      "\n",
      "                    Average val loss: 0.0017287118826061487|\n",
      "                    Val-Mae: 0.03825284168124199\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 414\n",
      "Start training for stock: 4189\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.044620294123888e-05 | \n",
      "                    Train-Mae: 0.023493213579058647 |\n",
      "\n",
      "                    Average val loss: 0.0010955168399959803|\n",
      "                    Val-Mae: 0.028398960828781128\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 415\n",
      "Start training for stock: 3183\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.97129939403385e-06 | \n",
      "                    Train-Mae: 0.01619957759976387 |\n",
      "\n",
      "                    Average val loss: 0.0014046784490346909|\n",
      "                    Val-Mae: 0.026154182851314545\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 416\n",
      "Start training for stock: 6857\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.951743995770812e-06 | \n",
      "                    Train-Mae: 0.02350342459976673 |\n",
      "\n",
      "                    Average val loss: 0.000584206311032176|\n",
      "                    Val-Mae: 0.02110622078180313\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 417\n",
      "Start training for stock: 7628\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.783590022474528e-06 | \n",
      "                    Train-Mae: 0.01952040009200573 |\n",
      "\n",
      "                    Average val loss: 0.0011218603467568755|\n",
      "                    Val-Mae: 0.03338735178112984\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 418\n",
      "Start training for stock: 7354\n",
      "continuos shape: (287, 11)  categorical shape: (287, 7)\n",
      "x_cat.shape before embedding: torch.Size([283, 7])\n",
      "x_cat after embedding: torch.Size([283, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([283, 7, 1])\n",
      "x.shape after fft2: torch.Size([283, 5])\n",
      "x.shape after pooling: torch.Size([283, 2])\n",
      "x.shape after first cont layer: torch.Size([283, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([283, 7])\n",
      "x_cat.squeeze().shape: torch.Size([283, 7])\n",
      "x.shape after torch.cat: torch.Size([283, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2045912444591523e-05 | \n",
      "                    Train-Mae: 0.027933932840824127 |\n",
      "\n",
      "                    Average val loss: 0.0018961112946271896|\n",
      "                    Val-Mae: 0.04309818148612976\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 419\n",
      "Start training for stock: 5989\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2976485304534436e-05 | \n",
      "                    Train-Mae: 0.021133093163371086 |\n",
      "\n",
      "                    Average val loss: 0.00035697774728760123|\n",
      "                    Val-Mae: 0.018780440092086792\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 420\n",
      "Start training for stock: 8217\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.336147103458643e-06 | \n",
      "                    Train-Mae: 0.018674127757549286 |\n",
      "\n",
      "                    Average val loss: 0.000646335887722671|\n",
      "                    Val-Mae: 0.0233344454318285\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 421\n",
      "Start training for stock: 2753\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.47486701887101e-06 | \n",
      "                    Train-Mae: 0.015178803354501724 |\n",
      "\n",
      "                    Average val loss: 0.0009229198913089931|\n",
      "                    Val-Mae: 0.026326799765229225\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 422\n",
      "Start training for stock: 6368\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.712715934962035e-06 | \n",
      "                    Train-Mae: 0.018254820257425308 |\n",
      "\n",
      "                    Average val loss: 0.0004927808186039329|\n",
      "                    Val-Mae: 0.02018526755273342\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 423\n",
      "Start training for stock: 2980\n",
      "continuos shape: (477, 11)  categorical shape: (477, 7)\n",
      "x_cat.shape before embedding: torch.Size([473, 7])\n",
      "x_cat after embedding: torch.Size([473, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([473, 7, 1])\n",
      "x.shape after fft2: torch.Size([473, 5])\n",
      "x.shape after pooling: torch.Size([473, 2])\n",
      "x.shape after first cont layer: torch.Size([473, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([473, 7])\n",
      "x_cat.squeeze().shape: torch.Size([473, 7])\n",
      "x.shape after torch.cat: torch.Size([473, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.8217767355963587e-05 | \n",
      "                    Train-Mae: 0.03309895843267441 |\n",
      "\n",
      "                    Average val loss: 0.0015653603477403522|\n",
      "                    Val-Mae: 0.033706437796354294\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 424\n",
      "Start training for stock: 8155\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.308532414957881e-06 | \n",
      "                    Train-Mae: 0.021962154656648636 |\n",
      "\n",
      "                    Average val loss: 0.00016947169206105173|\n",
      "                    Val-Mae: 0.0115797845646739\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 425\n",
      "Start training for stock: 4165\n",
      "continuos shape: (236, 11)  categorical shape: (236, 7)\n",
      "x_cat.shape before embedding: torch.Size([231, 7])\n",
      "x_cat after embedding: torch.Size([231, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([231, 7, 1])\n",
      "x.shape after fft2: torch.Size([231, 5])\n",
      "x.shape after pooling: torch.Size([231, 2])\n",
      "x.shape after first cont layer: torch.Size([231, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([231, 7])\n",
      "x_cat.squeeze().shape: torch.Size([231, 7])\n",
      "x.shape after torch.cat: torch.Size([231, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.8509655967354773e-05 | \n",
      "                    Train-Mae: 0.03918775916099548 |\n",
      "\n",
      "                    Average val loss: 0.0018803449347615242|\n",
      "                    Val-Mae: 0.03763006255030632\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 426\n",
      "Start training for stock: 5186\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.3078309651464225e-06 | \n",
      "                    Train-Mae: 0.017741382122039795 |\n",
      "\n",
      "                    Average val loss: 0.0005092804203741252|\n",
      "                    Val-Mae: 0.022516662254929543\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 427\n",
      "Start training for stock: 4568\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.866982439532876e-06 | \n",
      "                    Train-Mae: 0.020408540964126587 |\n",
      "\n",
      "                    Average val loss: 0.0008185426122508943|\n",
      "                    Val-Mae: 0.02114550955593586\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 428\n",
      "Start training for stock: 8001\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.507168243639171e-06 | \n",
      "                    Train-Mae: 0.018017012625932693 |\n",
      "\n",
      "                    Average val loss: 0.00029254675609990954|\n",
      "                    Val-Mae: 0.015790240839123726\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 429\n",
      "Start training for stock: 4116\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0032847058027983e-05 | \n",
      "                    Train-Mae: 0.020797690376639366 |\n",
      "\n",
      "                    Average val loss: 0.00013454293366521597|\n",
      "                    Val-Mae: 0.011302996426820755\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 430\n",
      "Start training for stock: 8098\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.906431470066309e-06 | \n",
      "                    Train-Mae: 0.01740255206823349 |\n",
      "\n",
      "                    Average val loss: 0.0005189772346056998|\n",
      "                    Val-Mae: 0.01926417089998722\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 431\n",
      "Start training for stock: 2810\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.997819196432829e-06 | \n",
      "                    Train-Mae: 0.01596900448203087 |\n",
      "\n",
      "                    Average val loss: 0.00034165504621341825|\n",
      "                    Val-Mae: 0.017426833510398865\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 432\n",
      "Start training for stock: 2573\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.202119402587414e-06 | \n",
      "                    Train-Mae: 0.015767784789204597 |\n",
      "\n",
      "                    Average val loss: 0.0005445482674986124|\n",
      "                    Val-Mae: 0.015980346128344536\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 433\n",
      "Start training for stock: 7806\n",
      "continuos shape: (828, 11)  categorical shape: (828, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([312, 7])\n",
      "x_cat after embedding: torch.Size([312, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([312, 7, 1])\n",
      "x.shape after fft2: torch.Size([312, 5])\n",
      "x.shape after pooling: torch.Size([312, 2])\n",
      "x.shape after first cont layer: torch.Size([312, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([312, 7])\n",
      "x_cat.squeeze().shape: torch.Size([312, 7])\n",
      "x.shape after torch.cat: torch.Size([312, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.1302057430148125e-05 | \n",
      "                    Train-Mae: 0.03383016958832741 |\n",
      "\n",
      "                    Average val loss: 0.001231000293046236|\n",
      "                    Val-Mae: 0.028384963050484657\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 434\n",
      "Start training for stock: 6752\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.812059971503913e-06 | \n",
      "                    Train-Mae: 0.019675912335515022 |\n",
      "\n",
      "                    Average val loss: 0.0003455003025010228|\n",
      "                    Val-Mae: 0.016274141147732735\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 435\n",
      "Start training for stock: 4043\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1226977221667767e-05 | \n",
      "                    Train-Mae: 0.018844064325094223 |\n",
      "\n",
      "                    Average val loss: 0.00022610648011323065|\n",
      "                    Val-Mae: 0.012593276798725128\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 436\n",
      "Start training for stock: 7456\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.520994309335947e-06 | \n",
      "                    Train-Mae: 0.02419932745397091 |\n",
      "\n",
      "                    Average val loss: 0.0011562934378162026|\n",
      "                    Val-Mae: 0.029775084927678108\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 437\n",
      "Start training for stock: 4348\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.10854534432292e-05 | \n",
      "                    Train-Mae: 0.02256951667368412 |\n",
      "\n",
      "                    Average val loss: 0.0009157590102404356|\n",
      "                    Val-Mae: 0.025662096217274666\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 438\n",
      "Start training for stock: 4519\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.002396694384515e-06 | \n",
      "                    Train-Mae: 0.01786993443965912 |\n",
      "\n",
      "                    Average val loss: 0.00029749295208603144|\n",
      "                    Val-Mae: 0.015126901678740978\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 439\n",
      "Start training for stock: 4005\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.9956353399902586e-06 | \n",
      "                    Train-Mae: 0.01967519521713257 |\n",
      "\n",
      "                    Average val loss: 0.001061955001205206|\n",
      "                    Val-Mae: 0.031108202412724495\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 440\n",
      "Start training for stock: 6788\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.483720012009145e-06 | \n",
      "                    Train-Mae: 0.017365360632538795 |\n",
      "\n",
      "                    Average val loss: 0.0002492099883966148|\n",
      "                    Val-Mae: 0.01247252244502306\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 441\n",
      "Start training for stock: 9386\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.318572511896491e-06 | \n",
      "                    Train-Mae: 0.030360784381628036 |\n",
      "\n",
      "                    Average val loss: 0.0019379736622795463|\n",
      "                    Val-Mae: 0.03796405717730522\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 442\n",
      "Start training for stock: 7420\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.2992080822587015e-06 | \n",
      "                    Train-Mae: 0.018000923097133636 |\n",
      "\n",
      "                    Average val loss: 0.0008895035134628415|\n",
      "                    Val-Mae: 0.022661244496703148\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 443\n",
      "Start training for stock: 4528\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.737266037613153e-06 | \n",
      "                    Train-Mae: 0.016902100294828415 |\n",
      "\n",
      "                    Average val loss: 0.0008855540072545409|\n",
      "                    Val-Mae: 0.02598213218152523\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 444\n",
      "Start training for stock: 1376\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.001490935683251e-06 | \n",
      "                    Train-Mae: 0.015829360112547874 |\n",
      "\n",
      "                    Average val loss: 0.0010019545443356037|\n",
      "                    Val-Mae: 0.028401218354701996\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 445\n",
      "Start training for stock: 2491\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.0379624329507353e-05 | \n",
      "                    Train-Mae: 0.021679310128092766 |\n",
      "\n",
      "                    Average val loss: 0.0006427011685445905|\n",
      "                    Val-Mae: 0.02206873707473278\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 446\n",
      "Start training for stock: 6062\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.5663980739191175e-05 | \n",
      "                    Train-Mae: 0.02305440977215767 |\n",
      "\n",
      "                    Average val loss: 0.00137766869738698|\n",
      "                    Val-Mae: 0.03449467569589615\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 447\n",
      "Start training for stock: 3475\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4647562056779862e-05 | \n",
      "                    Train-Mae: 0.02245377190411091 |\n",
      "\n",
      "                    Average val loss: 0.0019797522109001875|\n",
      "                    Val-Mae: 0.0362367182970047\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 448\n",
      "Start training for stock: 3626\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.940657622180879e-06 | \n",
      "                    Train-Mae: 0.019663309678435326 |\n",
      "\n",
      "                    Average val loss: 0.0004693652444984764|\n",
      "                    Val-Mae: 0.01673002541065216\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 449\n",
      "Start training for stock: 4776\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.628484210930765e-06 | \n",
      "                    Train-Mae: 0.021388757973909378 |\n",
      "\n",
      "                    Average val loss: 0.0021327610593289137|\n",
      "                    Val-Mae: 0.043699514120817184\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 450\n",
      "Start training for stock: 2593\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.4524032273329795e-06 | \n",
      "                    Train-Mae: 0.017752259969711304 |\n",
      "\n",
      "                    Average val loss: 0.0036594474222511053|\n",
      "                    Val-Mae: 0.04537665843963623\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 451\n",
      "Start training for stock: 6099\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.5076275449246168e-05 | \n",
      "                    Train-Mae: 0.022206438705325127 |\n",
      "\n",
      "                    Average val loss: 0.0006869229837320745|\n",
      "                    Val-Mae: 0.024272754788398743\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 452\n",
      "Start training for stock: 3688\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.5156702138483525e-05 | \n",
      "                    Train-Mae: 0.023488281294703484 |\n",
      "\n",
      "                    Average val loss: 0.002662388142198324|\n",
      "                    Val-Mae: 0.045076679438352585\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 453\n",
      "Start training for stock: 7814\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.1056067887693643e-05 | \n",
      "                    Train-Mae: 0.02027757652103901 |\n",
      "\n",
      "                    Average val loss: 0.0004440110642462969|\n",
      "                    Val-Mae: 0.020556069910526276\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 454\n",
      "Start training for stock: 9384\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.830724305473268e-06 | \n",
      "                    Train-Mae: 0.02846192754805088 |\n",
      "\n",
      "                    Average val loss: 0.0012910955119878054|\n",
      "                    Val-Mae: 0.029909035190939903\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 455\n",
      "Start training for stock: 6676\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.1053520767018196e-06 | \n",
      "                    Train-Mae: 0.020455777645111084 |\n",
      "\n",
      "                    Average val loss: 0.0008121302234940231|\n",
      "                    Val-Mae: 0.02400561235845089\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 456\n",
      "Start training for stock: 8150\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.80304947309196e-06 | \n",
      "                    Train-Mae: 0.01872006431221962 |\n",
      "\n",
      "                    Average val loss: 0.0006285694544203579|\n",
      "                    Val-Mae: 0.02020828239619732\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 457\n",
      "Start training for stock: 6098\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.67735017184168e-06 | \n",
      "                    Train-Mae: 0.021134449169039726 |\n",
      "\n",
      "                    Average val loss: 0.00021950648806523532|\n",
      "                    Val-Mae: 0.01225697249174118\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 458\n",
      "Start training for stock: 9115\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.412620423361659e-06 | \n",
      "                    Train-Mae: 0.03362296149134636 |\n",
      "\n",
      "                    Average val loss: 0.0014541186392307281|\n",
      "                    Val-Mae: 0.03157809376716614\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 459\n",
      "Start training for stock: 6629\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.3244749754667283e-05 | \n",
      "                    Train-Mae: 0.03475717082619667 |\n",
      "\n",
      "                    Average val loss: 0.0008522177813574672|\n",
      "                    Val-Mae: 0.02320997416973114\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 460\n",
      "Start training for stock: 4343\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.3112222077324987e-05 | \n",
      "                    Train-Mae: 0.023926975205540657 |\n",
      "\n",
      "                    Average val loss: 0.001453649252653122|\n",
      "                    Val-Mae: 0.03586433455348015\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 461\n",
      "Start training for stock: 7747\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.016995318233966e-06 | \n",
      "                    Train-Mae: 0.01819896511733532 |\n",
      "\n",
      "                    Average val loss: 0.0017870854353532195|\n",
      "                    Val-Mae: 0.03393156826496124\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 462\n",
      "Start training for stock: 3837\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.644523961469531e-06 | \n",
      "                    Train-Mae: 0.018631400540471077 |\n",
      "\n",
      "                    Average val loss: 0.0009447708143852651|\n",
      "                    Val-Mae: 0.025957966223359108\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 463\n",
      "Start training for stock: 2805\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.0296276304870843e-05 | \n",
      "                    Train-Mae: 0.015472417697310448 |\n",
      "\n",
      "                    Average val loss: 0.0020032990723848343|\n",
      "                    Val-Mae: 0.030433975160121918\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 464\n",
      "Start training for stock: 4619\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.607705822214485e-06 | \n",
      "                    Train-Mae: 0.020432060584425926 |\n",
      "\n",
      "                    Average val loss: 0.0013207439333200455|\n",
      "                    Val-Mae: 0.03293466940522194\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 465\n",
      "Start training for stock: 1780\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.446179955266416e-06 | \n",
      "                    Train-Mae: 0.013973277062177658 |\n",
      "\n",
      "                    Average val loss: 0.00015512696700170636|\n",
      "                    Val-Mae: 0.010201473720371723\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 466\n",
      "Start training for stock: 8157\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.626111619174481e-06 | \n",
      "                    Train-Mae: 0.01988237537443638 |\n",
      "\n",
      "                    Average val loss: 0.00038390367990359664|\n",
      "                    Val-Mae: 0.01828165352344513\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 467\n",
      "Start training for stock: 9902\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.503993645310402e-06 | \n",
      "                    Train-Mae: 0.017218098044395447 |\n",
      "\n",
      "                    Average val loss: 0.0008394091273657978|\n",
      "                    Val-Mae: 0.02690451592206955\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 468\n",
      "Start training for stock: 9987\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.678656696341932e-06 | \n",
      "                    Train-Mae: 0.015444544143974781 |\n",
      "\n",
      "                    Average val loss: 0.00029889788129366934|\n",
      "                    Val-Mae: 0.01569247990846634\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 469\n",
      "Start training for stock: 3834\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.3447304051369425e-06 | \n",
      "                    Train-Mae: 0.016156936064362526 |\n",
      "\n",
      "                    Average val loss: 0.001416875864379108|\n",
      "                    Val-Mae: 0.034124359488487244\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 470\n",
      "Start training for stock: 7780\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.987873977981508e-06 | \n",
      "                    Train-Mae: 0.020546110346913338 |\n",
      "\n",
      "                    Average val loss: 0.0009575436706654727|\n",
      "                    Val-Mae: 0.020468996837735176\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 471\n",
      "Start training for stock: 6183\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.954862667247653e-06 | \n",
      "                    Train-Mae: 0.018673833459615707 |\n",
      "\n",
      "                    Average val loss: 0.0003670529113151133|\n",
      "                    Val-Mae: 0.01755152828991413\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 472\n",
      "Start training for stock: 7600\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.294059127569199e-06 | \n",
      "                    Train-Mae: 0.017541633918881416 |\n",
      "\n",
      "                    Average val loss: 0.0007367720827460289|\n",
      "                    Val-Mae: 0.018760045990347862\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 473\n",
      "Start training for stock: 5015\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.673686344176531e-06 | \n",
      "                    Train-Mae: 0.012492064386606216 |\n",
      "\n",
      "                    Average val loss: 0.00018018086848314852|\n",
      "                    Val-Mae: 0.011194154620170593\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 474\n",
      "Start training for stock: 7823\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.459439660422504e-06 | \n",
      "                    Train-Mae: 0.015283200889825821 |\n",
      "\n",
      "                    Average val loss: 6.861107249278575e-05|\n",
      "                    Val-Mae: 0.007062343414872885\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 475\n",
      "Start training for stock: 3178\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.754266024567187e-06 | \n",
      "                    Train-Mae: 0.019427264109253883 |\n",
      "\n",
      "                    Average val loss: 0.0016424702480435371|\n",
      "                    Val-Mae: 0.034051936119794846\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 476\n",
      "Start training for stock: 9368\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 3.188511764165014e-06 | \n",
      "                    Train-Mae: 0.01721719279885292 |\n",
      "\n",
      "                    Average val loss: 0.0013760619331151247|\n",
      "                    Val-Mae: 0.03632938116788864\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 477\n",
      "Start training for stock: 9010\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.887975614517927e-06 | \n",
      "                    Train-Mae: 0.023901253938674927 |\n",
      "\n",
      "                    Average val loss: 0.00310231139883399|\n",
      "                    Val-Mae: 0.0471893846988678\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 478\n",
      "Start training for stock: 7012\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.337841670960188e-05 | \n",
      "                    Train-Mae: 0.022648492828011513 |\n",
      "\n",
      "                    Average val loss: 0.00044262874871492386|\n",
      "                    Val-Mae: 0.013460482470691204\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 479\n",
      "Start training for stock: 5726\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2797539820894599e-05 | \n",
      "                    Train-Mae: 0.02196354605257511 |\n",
      "\n",
      "                    Average val loss: 0.0018444580491632223|\n",
      "                    Val-Mae: 0.039275046437978745\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 480\n",
      "Start training for stock: 9984\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.22721335478127e-06 | \n",
      "                    Train-Mae: 0.0227085892111063 |\n",
      "\n",
      "                    Average val loss: 0.0037853694520890713|\n",
      "                    Val-Mae: 0.051389098167419434\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 481\n",
      "Start training for stock: 6502\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.5149607788771391e-05 | \n",
      "                    Train-Mae: 0.018146004527807236 |\n",
      "\n",
      "                    Average val loss: 0.00039737633778713644|\n",
      "                    Val-Mae: 0.012586855329573154\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 482\n",
      "Start training for stock: 6264\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 2.141498029232025e-05 | \n",
      "                    Train-Mae: 0.027954934164881706 |\n",
      "\n",
      "                    Average val loss: 0.0017104194266721606|\n",
      "                    Val-Mae: 0.03453991562128067\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 483\n",
      "Start training for stock: 8524\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.7240261705592274e-06 | \n",
      "                    Train-Mae: 0.02139914035797119 |\n",
      "\n",
      "                    Average val loss: 0.00012775693903677166|\n",
      "                    Val-Mae: 0.010506768710911274\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 484\n",
      "Start training for stock: 3863\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 4.288576892577112e-06 | \n",
      "                    Train-Mae: 0.016756825149059296 |\n",
      "\n",
      "                    Average val loss: 0.00026832357980310917|\n",
      "                    Val-Mae: 0.012490921653807163\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 485\n",
      "Start training for stock: 1959\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.0724280774593356e-06 | \n",
      "                    Train-Mae: 0.015828261151909828 |\n",
      "\n",
      "                    Average val loss: 0.00032183757866732776|\n",
      "                    Val-Mae: 0.01765110343694687\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 486\n",
      "Start training for stock: 6088\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.4118074905127287e-05 | \n",
      "                    Train-Mae: 0.023711536079645157 |\n",
      "\n",
      "                    Average val loss: 0.0013486648676916957|\n",
      "                    Val-Mae: 0.02747236378490925\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 487\n",
      "Start training for stock: 4809\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.531319766305387e-06 | \n",
      "                    Train-Mae: 0.015044540166854858 |\n",
      "\n",
      "                    Average val loss: 0.0009703155374154449|\n",
      "                    Val-Mae: 0.029590487480163574\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 488\n",
      "Start training for stock: 7774\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 8.835452026687563e-06 | \n",
      "                    Train-Mae: 0.018654752522706985 |\n",
      "\n",
      "                    Average val loss: 0.0005963878938928246|\n",
      "                    Val-Mae: 0.020691171288490295\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 489\n",
      "Start training for stock: 7777\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.598759670741856e-05 | \n",
      "                    Train-Mae: 0.03137869015336037 |\n",
      "\n",
      "                    Average val loss: 0.0020468353759497404|\n",
      "                    Val-Mae: 0.03822807967662811\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 490\n",
      "Start training for stock: 2371\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.99504923634231e-06 | \n",
      "                    Train-Mae: 0.02069701813161373 |\n",
      "\n",
      "                    Average val loss: 0.0014496272196993232|\n",
      "                    Val-Mae: 0.032631766051054\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 491\n",
      "Start training for stock: 3048\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.342768272385001e-06 | \n",
      "                    Train-Mae: 0.014655587263405323 |\n",
      "\n",
      "                    Average val loss: 0.0005415677442215383|\n",
      "                    Val-Mae: 0.020759401842951775\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 492\n",
      "Start training for stock: 6157\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.816891884431243e-06 | \n",
      "                    Train-Mae: 0.01989002153277397 |\n",
      "\n",
      "                    Average val loss: 6.080115417717025e-05|\n",
      "                    Val-Mae: 0.00675491988658905\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 493\n",
      "Start training for stock: 4362\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.429277709685266e-06 | \n",
      "                    Train-Mae: 0.019977249205112457 |\n",
      "\n",
      "                    Average val loss: 0.0006244858377613127|\n",
      "                    Val-Mae: 0.016590891405940056\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 494\n",
      "Start training for stock: 6727\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 1.2483326718211175e-05 | \n",
      "                    Train-Mae: 0.020208057016134262 |\n",
      "\n",
      "                    Average val loss: 0.00027307221898809075|\n",
      "                    Val-Mae: 0.01623239554464817\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 495\n",
      "Start training for stock: 5970\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.152477046474814e-06 | \n",
      "                    Train-Mae: 0.019553348422050476 |\n",
      "\n",
      "                    Average val loss: 0.000244944472797215|\n",
      "                    Val-Mae: 0.013301650993525982\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 496\n",
      "Start training for stock: 7994\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 5.9882528148591515e-06 | \n",
      "                    Train-Mae: 0.017584705725312233 |\n",
      "\n",
      "                    Average val loss: 0.0004211697669234127|\n",
      "                    Val-Mae: 0.0204811729490757\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 497\n",
      "Start training for stock: 7607\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 6.1884662136435505e-06 | \n",
      "                    Train-Mae: 0.014598049223423004 |\n",
      "\n",
      "                    Average val loss: 0.0002626465284265578|\n",
      "                    Val-Mae: 0.01517826970666647\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 498\n",
      "Start training for stock: 6871\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 9.56186675466597e-06 | \n",
      "                    Train-Mae: 0.024384895339608192 |\n",
      "\n",
      "                    Average val loss: 0.0005412455066107213|\n",
      "                    Val-Mae: 0.0166526660323143\n",
      "                    \n",
      "####################\n",
      "\n",
      "Stock-iteratation: 499\n",
      "Start training for stock: 8056\n",
      "continuos shape: (1202, 11)  categorical shape: (1202, 7)\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([512, 7])\n",
      "x_cat after embedding: torch.Size([512, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([512, 7, 1])\n",
      "x.shape after fft2: torch.Size([512, 5])\n",
      "x.shape after pooling: torch.Size([512, 2])\n",
      "x.shape after first cont layer: torch.Size([512, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([512, 7])\n",
      "x_cat.squeeze().shape: torch.Size([512, 7])\n",
      "x.shape after torch.cat: torch.Size([512, 135])\n",
      "x_cat.shape before embedding: torch.Size([174, 7])\n",
      "x_cat after embedding: torch.Size([174, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([174, 7, 1])\n",
      "x.shape after fft2: torch.Size([174, 5])\n",
      "x.shape after pooling: torch.Size([174, 2])\n",
      "x.shape after first cont layer: torch.Size([174, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([174, 7])\n",
      "x_cat.squeeze().shape: torch.Size([174, 7])\n",
      "x.shape after torch.cat: torch.Size([174, 135])\n",
      "x_cat.shape before embedding: torch.Size([3, 7])\n",
      "x_cat after embedding: torch.Size([3, 7, 300])\n",
      "Shape of x_cat before cat: torch.Size([3, 7, 1])\n",
      "x.shape after fft2: torch.Size([3, 5])\n",
      "x.shape after pooling: torch.Size([3, 2])\n",
      "x.shape after first cont layer: torch.Size([3, 128])\n",
      "x_cat.view((x_cat.size(0), -1)).shape: torch.Size([3, 7])\n",
      "x_cat.squeeze().shape: torch.Size([3, 7])\n",
      "x.shape after torch.cat: torch.Size([3, 135])\n",
      "\n",
      "                    Average train loss: 7.177878869697452e-06 | \n",
      "                    Train-Mae: 0.017550615593791008 |\n",
      "\n",
      "                    Average val loss: 0.0008006758289411664|\n",
      "                    Val-Mae: 0.02536546252667904\n",
      "                    \n",
      "####################\n",
      "\n",
      "CPU times: total: 1h 13min 23s\n",
      "Wall time: 1h 13min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stocks = train_df['SecuritiesCode'].unique()\n",
    "count = 0\n",
    "BATCH_SIZE = 512\n",
    "weight_decay = 0.1\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "scaler_dict = {}\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer_name='rmsprop', \n",
    "    lr=1.3333e-5, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_mae_list = []\n",
    "valid_loss_list = []\n",
    "valid_mae_list = []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SHUFFLE STOCKS WHEN TRAINING\n",
    "\"\"\"\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DO RANDOM CHOICE OF STOCKS.\n",
    "\"\"\"\n",
    "# STOCK_EPOCH = 3\n",
    "# for s_epoch in range(STOCK_EPOCH):\n",
    "#     print('$' * 80)\n",
    "#     print('stock-epoch:', s_epoch)\n",
    "np.random.shuffle(stocks) # SHUFFLE STOCKS IN PLACE\n",
    "print(stocks)\n",
    "for no_stock, stock in enumerate(stocks[: 500]):\n",
    "    try:\n",
    "        train_loader, val_dataloader = None, None\n",
    "        print(f'Stock-iteratation: {no_stock}')\n",
    "        print(f'Start training for stock: {stock}')\n",
    "\n",
    "        train_dataloader, val_dataloader = dataloader_by_stock(\n",
    "            train_df, \n",
    "            stock, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            continous_cols=CONT_COLS,\n",
    "            # return_scaler=True\n",
    "        )\n",
    "        # if count > 1:\n",
    "        #     EPOCHS = 15\n",
    "        train_loss, train_mae, val_loss, val_mae = trainer.fit_epochs(\n",
    "            train_dataloader, \n",
    "            val_dataloader, \n",
    "            use_cyclic_lr=True, \n",
    "            x_cat=True, \n",
    "            epochs=EPOCHS\n",
    "        )\n",
    "\n",
    "        train_loss_list.extend(train_loss)\n",
    "        train_mae_list.extend(train_mae)\n",
    "        valid_loss_list.extend(val_loss)\n",
    "        valid_mae_list.extend(val_mae)\n",
    "        # scaler_dict[stock] = scaler\n",
    "        print('#' * 20)\n",
    "        print()\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(f'Training loop: {e}')\n",
    "\n",
    "trainer.save_model(model)\n",
    "# with open('scaler_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(scaler_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.title('Train loss');\n",
    "plt.show()\n",
    "\n",
    "plt.plot(valid_loss_list)\n",
    "plt.title('Valid loss')\n",
    "plt.show()\n",
    "\n",
    "plot_loss(train_loss_list, valid_loss_list, 'Train-loss', 'Valid-loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_mae_list)\n",
    "plt.title('Train MAE');\n",
    "plt.show()\n",
    "\n",
    "plt.plot(valid_mae_list)\n",
    "plt.title('Valid MAE')\n",
    "plt.show()\n",
    "\n",
    "plot_loss(train_loss_list, valid_loss_list, 'Train-MAE', 'Valid-MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.002099018692970276,\n",
       " 0.0007113485038280487,\n",
       " 0.00035366296768188474,\n",
       " 0.0001983572170138359,\n",
       " 0.0001166369952261448]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
