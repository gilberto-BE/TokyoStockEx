{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-13T07:12:14.976581Z","iopub.execute_input":"2022-04-13T07:12:14.977208Z","iopub.status.idle":"2022-04-13T07:12:14.995579Z","shell.execute_reply.started":"2022-04-13T07:12:14.977163Z","shell.execute_reply":"2022-04-13T07:12:14.994779Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\n\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchmetrics as TM\npl.utilities.seed.seed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T07:12:15.013697Z","iopub.execute_input":"2022-04-13T07:12:15.014095Z","iopub.status.idle":"2022-04-13T07:12:15.017361Z","shell.execute_reply.started":"2022-04-13T07:12:15.014054Z","shell.execute_reply":"2022-04-13T07:12:15.016755Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utility functions and classes","metadata":{}},{"cell_type":"code","source":"\n\ndef show_df(\n    df, \n    show_info=True, \n    show_head=True, \n    show_tail=True, \n    dataframe_name='financials'\n):\n    print(f'<<< {dataframe_name} >>>')\n    print(df.shape)\n    if show_info:\n        display(df.info())\n    if show_head:\n        display(df.head())\n    if show_tail:\n        display(df.tail())\n    print('-.' * 80)\n    \n    \ndef date_features(df, date_col=None):\n    # Check if index is datetime.\n#     if isinstance(df, pd.core.series.Series):\n#         df = pd.DataFrame(df, index=df.index)\n    if date_col:\n        df[date_col] = pd.to_datetime(df[date_col])\n        df.set_index(date_col, inplace=True)\n\n    df.loc[:, 'day_of_year'] = df.index.dayofyear\n    df.loc[:, 'month'] = df.index.month\n    df.loc[:, 'day_of_week'] = df.index.day\n#     df.loc[:, 'hour'] = df.index.hour\n    return  df\n\n\ndef preprocess(df, diff_cols=['Open', 'Close', 'High', 'Low', 'Volume']):\n    if diff_cols:\n        df[diff_cols] = df[diff_cols].pct_change()\n    df = df.select_dtypes(include=[int, float])\n    return df\n\n\n\nclass ToTorch(Dataset):\n\n    def __init__(\n            self,\n            features,\n            target\n            ):\n        self.features = features\n        self.target = target\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        features = self.features[idx]\n        target = self.target[idx]\n        return {\n            'features': torch.from_numpy(np.array(features)).float(), \n            'target': torch.from_numpy(np.array(target)).float()\n            }\n    \n\ndef get_loader(x, y, batch_size):\n    # Return dict with {'features', 'targets'}\n    return DataLoader(ToTorch(x, y), batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T07:16:07.563457Z","iopub.execute_input":"2022-04-13T07:16:07.565052Z","iopub.status.idle":"2022-04-13T07:16:07.579070Z","shell.execute_reply.started":"2022-04-13T07:16:07.564944Z","shell.execute_reply":"2022-04-13T07:16:07.578380Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{"execution":{"iopub.status.busy":"2022-04-13T05:54:38.473593Z","iopub.execute_input":"2022-04-13T05:54:38.473935Z","iopub.status.idle":"2022-04-13T05:54:38.478780Z","shell.execute_reply.started":"2022-04-13T05:54:38.473903Z","shell.execute_reply":"2022-04-13T05:54:38.477579Z"}}},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(NeuralNetwork, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n#         self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n#         x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n    \n\n\n\n\nclass Trainer:\n    def __init__(\n        self, \n        model, \n        optimizer_name='rmsprop', \n        lr=0.003, \n        loss_fn_name='mse'\n        ):\n\n        self.model = model\n        self.lr = lr\n        self.optimizer_name=optimizer_name\n        self.loss_fn_name = loss_fn_name\n        self.train_loss = []\n        self.valid_loss = []\n        self.test_loss = []\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"Using {self.device}-device\")\n\n        if self.loss_fn_name == 'mse':\n            self.loss_fn = nn.MSELoss()\n\n        if self.optimizer_name.lower() == 'rmsprop': \n            self.optimizer = torch.optim.RMSprop(self.model.parameters(), self.lr)\n\n        elif self.optimizer_name.lower() == 'adam':\n            pass\n\n    def _set_optimizer(self):\n        try:\n            pass\n        except:\n            pass\n\n    def _set_loss(self):\n        try:\n            pass\n        except:\n            pass\n\n    def check_optimizer_loss_args(self):\n        print(f'Allowed opmimizer names are:')\n        print(f'Allowed loss function names are:')\n\n    def fit_epochs(self, train_loader, valid_loader=None, epochs=5):\n        for epoch in epochs:\n            self.fit_one_epoch(train_loader, valid_loader)\n\n\n    def fit_one_epoch(self, train_loader, valid_loader=None):\n        size = len(train_loader)\n        self.model.to(self.device).train()\n        for batch, data in enumerate(train_loader):\n            x = data['features']\n            y = data['target']\n            print('x.shap1e:', x.shape)\n            x, y = x.to(self.device), y.to(self.device)\n            self._run_train_step(x, y, batch, size)\n\n        if valid_loader is not None:\n            with torch.no_grad():\n                pass\n\n    def evaluate(self, test_loader, batch, size):\n        self.model.eval()\n        for x, y in range(len(test_loader)):\n            x, y = x.to(self.device), y.to(self.device)\n            pred = self.model(x)\n            loss += self.loss_fn(pred, y).item()\n            print(f'val-loss: {loss.item()} [{batch * len(x)/{size}}]')\n\n\n    def _run_train_step(self, x, y, batch, size):\n        pred = self.model(x)\n        loss = self.loss_fn(pred, y)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.train_loss.append(loss.item())\n        # if batch % 100 == 0:\n        print(f'loss: {loss.item()} [{batch * len(x)}/{size}]')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T07:22:57.118437Z","iopub.execute_input":"2022-04-13T07:22:57.119183Z","iopub.status.idle":"2022-04-13T07:22:57.135335Z","shell.execute_reply.started":"2022-04-13T07:22:57.119146Z","shell.execute_reply":"2022-04-13T07:22:57.134626Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"ROOT_PATH = '/kaggle/input/jpx-tokyo-stock-exchange-prediction'","metadata":{"execution":{"iopub.status.busy":"2022-04-13T07:12:15.152322Z","iopub.execute_input":"2022-04-13T07:12:15.153217Z","iopub.status.idle":"2022-04-13T07:12:15.156762Z","shell.execute_reply.started":"2022-04-13T07:12:15.153169Z","shell.execute_reply":"2022-04-13T07:12:15.156159Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Start data analysis ","metadata":{}},{"cell_type":"markdown","source":"### API data","metadata":{}},{"cell_type":"raw","source":"dataset_dir=\"../input/jpx-tokyo-stock-exchange-prediction/example_test_files/\"\n# 読み込むファイルを定義します。\ninputs = {\n    \"financials\": f\"{dataset_dir}/financials.csv\",\n    \"options\": f\"{dataset_dir}/options.csv\",\n    \"secondary_stock_prices\": f\"{dataset_dir}secondary_stock_prices.csv\",\n    \"stock_prices\": f\"{dataset_dir}/stock_prices.csv\",\n    \"trades\": f\"{dataset_dir}/trades.csv\",\n}\n\n# ファイルを読み込みます\nexample_test_files = {}\nfor k, v in inputs.items():\n    print(k)\n    example_test_files[k] = pd.read_csv(v)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:44:54.293559Z","iopub.execute_input":"2022-04-12T12:44:54.294018Z","iopub.status.idle":"2022-04-12T12:44:54.408209Z","shell.execute_reply.started":"2022-04-12T12:44:54.293975Z","shell.execute_reply":"2022-04-12T12:44:54.407409Z"}}},{"cell_type":"code","source":"'/kaggle/input/jpx-tokyo-stock-exchange-prediction/train_files/financials.csv'\n'/train_files/trades.csv'\n\ntrain_df = pd.read_csv(f'{ROOT_PATH}/train_files/stock_prices.csv')\ntrain_df['Date'] = pd.to_datetime(train_df['Date']) \ntrain_df.set_index('Date', inplace=True)\n# train_df = date_features(train_df)\n\ntrain_options = pd.read_csv(f'{ROOT_PATH}/train_files/options.csv', low_memory=False)\ntrain_financials = pd.read_csv(f'{ROOT_PATH}/train_files/financials.csv', low_memory=False)\ntrain_trades = pd.read_csv(f'{ROOT_PATH}/train_files/trades.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T07:12:15.179149Z","iopub.execute_input":"2022-04-13T07:12:15.179698Z","iopub.status.idle":"2022-04-13T07:12:46.112933Z","shell.execute_reply.started":"2022-04-13T07:12:15.179664Z","shell.execute_reply":"2022-04-13T07:12:46.112276Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\nprint()\nprint('Unique values for Adjustment factor:')\nprint(train_df.AdjustmentFactor.unique())\nprint()\nprint('Number of Unique Securities code:')\nprint(train_df.SecuritiesCode.nunique())\nprint()\nprint('Number of Unique Expected dividends:')\nprint(train_df.ExpectedDividend.nunique())\n# print(train_df.ExpectedDividend.unique())\nshow_df(train_df, dataframe_name='stock_data')\n\nshow_df(train_options, dataframe_name='options_data')\n\nshow_df(train_financials, dataframe_name='financials')\n\nshow_df(train_trades, dataframe_name='trades')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T07:12:46.114391Z","iopub.execute_input":"2022-04-13T07:12:46.114579Z","iopub.status.idle":"2022-04-13T07:12:46.562272Z","shell.execute_reply.started":"2022-04-13T07:12:46.114556Z","shell.execute_reply":"2022-04-13T07:12:46.561491Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndf_1301 = train_df[train_df['SecuritiesCode'] == 1301].drop('SecuritiesCode', axis=1)\n# df_1301.set_index('Date', inplace=True)\n# df_1301[['Open', 'Close', 'High', 'Low', 'Volume']] = df_1301[['Open', 'Close', 'High', 'Low', 'Volume']].pct_change()\ndf_1301 = preprocess(df_1301)\ndf_1301.plot(figsize=(15, 20), subplots=True);\nplt.show();\ndf_1301.Target.hist(bins=50);\nplt.show();\n\n\n# pd.plotting.scatter_matrix(df_1301);\n# plt.show();\nprint(df_1301.info())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T07:16:13.694418Z","iopub.execute_input":"2022-04-13T07:16:13.695132Z","iopub.status.idle":"2022-04-13T07:16:15.340513Z","shell.execute_reply.started":"2022-04-13T07:16:13.695064Z","shell.execute_reply":"2022-04-13T07:16:15.339723Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Models\nTodo:\n\n    1) Preprocesses data and use correct features (date features, etc)\n    2) Deep learning for prediction of stock-returns\n    3) Ranking with XGBoost or other methods","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nytrain_stock = df_1301['Target'].to_numpy()\nxtrain_stock = df_1301.drop('Target', axis=1).to_numpy()\nprint(ytrain_stock)\ntrain_dataloader = get_loader(xtrain_stock, ytrain_stock, 64)\n\n\n# train_set = TimeSeriesDataset(x_num, y_num)\n# train_loader = DataLoader(train_set, batch_size=64)\nmodel = NeuralNetwork(in_features=xtrain_stock.shape[1], out_features=1)\ntrainer = Trainer(model)\n\n\n# # Display image and label.\n# train_features, train_labels = next(iter(train_dataloader))\n# print(train_features['features'])\n# print(f\"Feature batch shape: {train_features.size()}\")\n# print(f\"Labels batch shape: {train_labels.size()}\")\n# # img = train_features[0].squeeze()\n# # label = train_labels[0]\n# # plt.imshow(img, cmap=\"gray\")\n# # plt.show()\n# # print(f\"Label: {label}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-13T07:24:01.011578Z","iopub.execute_input":"2022-04-13T07:24:01.012478Z","iopub.status.idle":"2022-04-13T07:24:01.034440Z","shell.execute_reply.started":"2022-04-13T07:24:01.012421Z","shell.execute_reply":"2022-04-13T07:24:01.033651Z"},"trusted":true},"execution_count":27,"outputs":[]}]}